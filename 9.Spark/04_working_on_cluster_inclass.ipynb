{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "04-working_on_cluster.inclass.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufoQYvVDV576"
      },
      "source": [
        "# Working with computing clusters\n",
        "\n",
        "  <a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msCzEAqdV578"
      },
      "source": [
        "The idea here is to transmit the idea of how a Data Scientist works remotely.\n",
        "\n",
        "We do it all the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZCRs4UV579"
      },
      "source": [
        "## Amazon Web Services\n",
        "\n",
        "![AWS](https://d7umqicpi7263.cloudfront.net/img/product/0f7858eb-0831-4a33-9af9-8e78db6b23d8/c7939ac3-d352-4bee-bf8a-7ee4a2dd2bff.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KAHFxIkV57-"
      },
      "source": [
        "AWS is widely used\n",
        "\n",
        "You can use an introductory offer to get a taste of it for free.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UDwcpMxV57-"
      },
      "source": [
        "### Signing up for aws\n",
        "\n",
        "https://aws.amazon.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLPb-pcXV57-"
      },
      "source": [
        "### AWS services of interest:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85LwX-ZV57_"
      },
      "source": [
        "- EC2: Elastic Cloud Compute. Allows us to rent virtual machines, computing power, fit to our needs and under several pricing models.\n",
        "\n",
        "- S3: Simple Storage Service: Allows us to rent storage capacity to plug into our virtual servers.\n",
        "\n",
        "- EMR: Elastic MapReduce. Simplifies the creation and management of Hadoop/Spark clusters. \n",
        "\n",
        "- Lambda: Serverless computing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toAIc1YEV57_"
      },
      "source": [
        "### Creating a single instance: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajz-mxaDV58A"
      },
      "source": [
        "- choosing an operating system\n",
        "\n",
        "- choosing an instance type - spot prices\n",
        "\n",
        "- auto scaling groups\n",
        "\n",
        "- creating a new keypair and storing the private key in .ssh/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afb0QN8GV58A"
      },
      "source": [
        "## Google Cloud Platform\n",
        "\n",
        "![Google Cloud](https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000%C3%971000.png)\n",
        "\n",
        "\n",
        "Rival to AWS\n",
        "\n",
        "300$ introductory credit!\n",
        "\n",
        "We will look into it in a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwlT6PLwV58A"
      },
      "source": [
        "## Accessing remote computers\n",
        "\n",
        "ssh is your basic tool. You should always use public/private key pair authentication rather than passwords, especially if the ssh port (usually 22) is open to the internet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88l2hUqqV58B"
      },
      "source": [
        "### public-private keys\n",
        "\n",
        "Generating ssh keys:\n",
        "\n",
        "- [ssh-keygen](https://www.ssh.com/ssh/keygen/)\n",
        "\n",
        "\n",
        "```shell\n",
        "ssh-keygen -y -f $PRIVATE_KEY\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRN7LgHqV58B"
      },
      "source": [
        "### `ssh`\n",
        "\n",
        "We need to keep the private key (not the `.pub` file) somewhere where we will not lose it. The standard place is the `~/.ssh/` folder.\n",
        "\n",
        "```shell\n",
        "mkdir -p ~/.ssh\n",
        "mv $key_file ~/.ssh\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10yDWzTOV58B"
      },
      "source": [
        "`ssh` will let you control a remote machine as if you were typing at its terminal\n",
        "\n",
        "Let's connect to the instance we just created. \n",
        "\n",
        "We need to use the \"identity file\" (private key) to authenticate ourselves:\n",
        "\n",
        "```shell\n",
        "ssh -i $PRIVATE_KEY $REMOTE_USER@$REMOTE_MACHINE\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_Cv0ZvV58C"
      },
      "source": [
        "\n",
        "### scp\n",
        "\n",
        "Sending our data to a remote computer\n",
        "\n",
        "Let's send coupon150720.csv to the recently created instance.\n",
        "\n",
        "```shell\n",
        "scp -i $PRIVATE_KEY $LOCAL_PATH $REMOTE_USER@$REMOTE_MACHINE:$REMOTE_PATH\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzG-sYxpV58C"
      },
      "source": [
        "### SSH config file\n",
        "\n",
        "\n",
        "An SSH config file saves us from typing those long connections every time. It needs to be in `~/.ssh/config` and looks like this:\n",
        "\n",
        "```\n",
        "Host mygcpcluster\n",
        "    User remoteuser\n",
        "    HostName masternodename\n",
        "    IdentityFile ~/.ssh/my-private-key\n",
        "```\n",
        "\n",
        "Once it's there, we can just type\n",
        "\n",
        "```\n",
        "ssh kschoolcluster\n",
        "```\n",
        "and we'll be connected.\n",
        "\n",
        "There are lots and lots of options to ssh we can configure like this. More details [here](https://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KKW6bYiV58D"
      },
      "source": [
        "## Google Cloud Platform\n",
        "\n",
        "![Google Cloud](https://cloud.google.com/images/velostrata/cloud-lockup-logo.png)\n",
        "\n",
        "\n",
        "Three ways to interact with GCP:\n",
        "\n",
        "* The Google Cloud Platform console (the GUI)\n",
        "\n",
        "* `gcloud` command-line tool\n",
        "\n",
        "* Cloud Dataproc REST API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tyuJHkvV58D"
      },
      "source": [
        "## Google dataproc\n",
        "\n",
        "[Cloud Dataproc FAQ](https://cloud.google.com/dataproc/docs/resources/faq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ipeLSIV58D"
      },
      "source": [
        "### Creating a cluster in Google dataproc\n",
        "\n",
        "* Creating a cluster\n",
        "\n",
        "* Installing gcloud SDK \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6WEgd-jV58E"
      },
      "source": [
        "### Installing the `gcloud` SDK\n",
        "\n",
        "[On Ubuntu/Debian](https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu):\n",
        "\n",
        "```bash\n",
        "# Add the Cloud SDK distribution URI as a package source\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "\n",
        "# Import the Google Cloud Platform public key\n",
        "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n",
        "\n",
        "# Update the package list and install the Cloud SDK\n",
        "sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYm1ro9sV58E"
      },
      "source": [
        "### Configuring the `gcloud` SDK\n",
        "\n",
        "```bash\n",
        "gcloud init\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAY3N_QbV58E"
      },
      "source": [
        "### Adding users to a project\n",
        "\n",
        "[Identity and access management](https://cloud.google.com/iam/docs/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LA5yxX5V58F"
      },
      "source": [
        "### Creating a cluster\n",
        "\n",
        "With GUI: Google Cloud Console -> dataproc -> Clusters -> create cluster\n",
        "\n",
        "With SDK: \n",
        "\n",
        "```bash\n",
        "gcloud dataproc clusters create $CLUSTERNAME --region $REGION\n",
        "```\n",
        "\n",
        "Many more options available. You can explore them within the SDK or through the GUI.\n",
        "\n",
        "[Creating a Cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67P7JImWV58F"
      },
      "source": [
        "### Uploading data a GCP cluster\n",
        "\n",
        "2 Options:\n",
        "\n",
        "* scp to the master node\n",
        "\n",
        "* Upload the data to Google Cloud Storage, then use `gs://` as a path prefic on your script\n",
        "\n",
        "    * First, you'll need to [create a storage bucket].\n",
        "    \n",
        "[create a storage bucket]: https://cloud.google.com/storage/docs/creating-buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGLxkwj9V58F"
      },
      "source": [
        "### Creating a storage bucket\n",
        "\n",
        "```bash\n",
        "gsutil mb -p kschool-spark  gs://bucket-name\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDX1xdOV58G"
      },
      "source": [
        "### Uploading your data\n",
        "\n",
        "```bash\n",
        "gsutil cp [LOCAL_OBJECT_LOCATION] gs://[DESTINATION_BUCKET_NAME]/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31ka7dPV58G"
      },
      "source": [
        "### Creating a cluster from Gcloud SDK\n",
        "\n",
        "```bash\n",
        "gcloud dataproc clusters create [CLUSTER_NAME] --region=region\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vATvmUUV58G"
      },
      "source": [
        "### Submitting a job to Google dataproc\n",
        "\n",
        "To submit a PySpark job, run:\n",
        "\n",
        "```bash\n",
        "  $ gcloud dataproc jobs submit pyspark --cluster my_cluster \\\n",
        "      my_script.py -- arg1 arg2\n",
        "      \n",
        "```\n",
        "\n",
        "https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHf-USv0V58G"
      },
      "source": [
        "## Storage: HDFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH2AZtNXV58H"
      },
      "source": [
        "Assumptions in [HDFS design]:\n",
        "\n",
        "* The system is built from many inexpensive commodity components that often fail. \n",
        "\n",
        "* The system stores a modest number of large files. \n",
        "\n",
        "* The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. \n",
        "\n",
        "* The workloads also have many large, sequential writes that append data to files.\n",
        "\n",
        "* The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.\n",
        "\n",
        "* High sustained bandwidth is more important than low latency. \n",
        "\n",
        "[HDFS design]: https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxwZay9V58H"
      },
      "source": [
        "\n",
        "![HDFS](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQDvpXoAV58I"
      },
      "source": [
        "### hdfs dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I9B04_3V58J"
      },
      "source": [
        "Mimics the shell, but with a few differences:\n",
        "\n",
        "* We call shell commands as options to a module named hdfs dfs\n",
        "\n",
        "* There is no concept of a current working directory (therefore, no cd command)\n",
        "\n",
        "* It has some annoying inconsistencies with regular bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlArM9XuV58J"
      },
      "source": [
        "```shell\n",
        "\n",
        "[hadoop@masternode ~]$ hdfs dfs \n",
        "\n",
        "Usage: hadoop fs [generic options]\n",
        "\t[-appendToFile <localsrc> ... <dst>]\n",
        "\t[-cat [-ignoreCrc] <src> ...]\n",
        "\t[-checksum <src> ...]\n",
        "\t[-chgrp [-R] GROUP PATH...]\n",
        "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
        "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
        "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
        "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
        "\t[-count [-q] [-h] <path> ...]\n",
        "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
        "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
        "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
        "\t[-df [-h] [<path> ...]]\n",
        "\t[-du [-s] [-h] <path> ...]\n",
        "\t[-expunge]\n",
        "\t[-find <path> ... <expression> ...]\n",
        "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
        "\t[-getfacl [-R] <path>]\n",
        "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
        "\t[-getmerge [-nl] <src> <localdst>]\n",
        "\t[-help [cmd ...]]\n",
        "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
        "\t[-mkdir [-p] <path> ...]\n",
        "\t[-moveFromLocal <localsrc> ... <dst>]\n",
        "\t[-moveToLocal <src> <localdst>]\n",
        "\t[-mv <src> ... <dst>]\n",
        "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
        "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
        "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
        "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
        "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
        "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
        "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
        "\t[-stat [format] <path> ...]\n",
        "\t[-tail [-f] <file>]\n",
        "\t[-test -[defsz] <path>]\n",
        "\t[-text [-ignoreCrc] <src> ...]\n",
        "\t[-touchz <path> ...]\n",
        "\t[-truncate [-w] <length> <path> ...]\n",
        "\t[-usage [cmd ...]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGvgnAh3V58K"
      },
      "source": [
        "Try:\n",
        "\n",
        "```shell\n",
        "user@gateway$ hdfs dfs -ls\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3dauwseV58L"
      },
      "source": [
        "Why does it return nothing?\n",
        "\n",
        "Now try:\n",
        "\n",
        "```shell\n",
        "user@gateway$ hdfs dfs -ls /\n",
        "user@gateway$ hdfs dfs -ls /user\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjWMr4FV58L"
      },
      "source": [
        "#### `hdfs dfs -mkdir`\n",
        "\n",
        "Create a folder inside your hdfs home folder that is called \"data\", on your own\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dRie_J7V58L"
      },
      "source": [
        "#### `hdfs dfs -put`\n",
        "\n",
        "By analogy with ls, can you guess where the\n",
        "`$LOCAL_FILE` will be put if I do this? (don't do it)\n",
        "                                       \n",
        "```shell\n",
        "user@gateway$ hdfs dfs -put $LOCAL_FILE\n",
        "\n",
        "```\n",
        "                                       \n",
        "                                       \n",
        "Now, put the file in hdfs, inside your \"data\" folder:\n",
        "```shell\n",
        "user@gateway$ hdfs dfs -put $LOCAL_FILE $HDFS_FOLDER\n",
        "```\n",
        " \n",
        "                                       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsQKg3m-V58M"
      },
      "source": [
        "#### `hdfs dfs -get` / `hdfs dfs -cat`\n",
        "\n",
        "If you do any kind of work in HDFS, eventually you'll need to get something out of it!\n",
        "\n",
        "```shell\n",
        "user@gateway$ hdfs dfs -cat $HDFS_FILE\n",
        "```\n",
        "\n",
        "However, you might only need take a peek into the contents of a file:\n",
        "\n",
        "```shell\n",
        "user@gateway$ hdfs dfs -get $HDFS_FILE\n",
        "```\n",
        "\n",
        "The neat thing about hdfs dfs -cat is that it outputs to stdout, so you can chain it to all your favorite shell pipelines!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68XlXcXV58N"
      },
      "source": [
        "Other useful hadoop filesystem commands:\n",
        "    \n",
        "```shell\n",
        "user@gateway$ hdfs dfs -getmerge $HDFS_GLOB $LOCAL_FILE\n",
        "user@gateway$ hdfs dfs -stat $HDFS_FILE\n",
        "user@gateway$ hdfs dfs -tail $HDFS_FILE\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTxFwsdqV58N"
      },
      "source": [
        "Much more at:\n",
        "https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSwYWhJkV58N"
      },
      "source": [
        "## spark-submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY0sRCm4V58O"
      },
      "source": [
        "#### ```mysparkjob.py```\n",
        "\n",
        "\n",
        "```python\n",
        "from __future__ import print_function\n",
        "from pyspark import SparkContext\n",
        "import sys\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: mysparkjob arg1 arg2 \", file=sys.stderr)\n",
        "        exit(-1)\n",
        "    sc = SparkContext(appName=\"MyTestJob\")\n",
        "    dataTextAll = sc.textFile(sys.argv[1])\n",
        "    dataRDD = dataTextAll.filter(lambda line: line.startswith('79065'))\n",
        "    dataRDD.saveAsTextFile(sys.argv[2])\n",
        "    sc.stop()\n",
        "```\n",
        "\n",
        "Just a simple Spark job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFiT_8T9V58O"
      },
      "source": [
        "### Runnning our Spark app\n",
        "\n",
        "```shell\n",
        "./bin/spark-submit \\\n",
        "    mysparkjob.py \\\n",
        "    data/coupon150720.csv \\\n",
        "    test.csv\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNlmObEpV58P"
      },
      "source": [
        "Once it runs, what is test.csv? How would you get it back on the local file system?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7uONYuV58P"
      },
      "source": [
        "#### Exercise \n",
        "\n",
        "Adapt our exercise from notebook 02 to run in the cluster. Remember:\n",
        "\n",
        "Get stats for all tickets with destination MAD from `coupons150720.csv`. You will need to extract ticket amounts with destination MAD, and then calculate:\n",
        "\n",
        "* Total ticket amounts per origin\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RqV10GNgV58P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6yIhNAFV58P"
      },
      "source": [
        "### Running on cluster versus client mode\n",
        "\n",
        "This setting controls where the driver runs.\n",
        "\n",
        "The default deployment mode is `client`, that is, the driver runs on the machine that is running the spark-submit script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLyE4S3aV58Q"
      },
      "source": [
        "\n",
        "```shell\n",
        "./bin/spark-submit \\\n",
        "    mysparkjob.py \\\n",
        "    data/coupon150720.csv \\\n",
        "    --deploy-mode client\n",
        "```\n",
        "\n",
        "\n",
        "```shell\n",
        "./bin/spark-submit \\\n",
        "    mysparkjob.py \\\n",
        "    data/coupon150720.csv \\\n",
        "    --deploy-mode cluster\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuXfv5vtV58Q"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "\n",
        "\n",
        "[hadoop fs](https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
        "\n",
        "[standalone Spark versus yarn versus Mesos](http://www.agildata.com/apache-spark-cluster-managers-yarn-mesos-or-standalone/)\n",
        "\n",
        "[How Spark runs on clusters](https://spark.apache.org/docs/2.2.0/cluster-overview.html)\n",
        "\n",
        "[spark-submit](https://aws.amazon.com/es/blogs/big-data/submitting-user-applications-with-spark-submit/)\n",
        "\n",
        "[Cluster versus Client deployment modes](https://stackoverflow.com/questions/28807490/what-conditions-should-cluster-deploy-mode-be-used-instead-of-client)\n",
        "\n",
        "[Tunnelling web connections through ssh to view the Spark management web views](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html)\n",
        "\n",
        "[Findings on running Google Dataproc](https://www.inovex.de/blog/findings-in-running-google-dataproc/)\n",
        "\n",
        "[Dataproc - Spark cluster in minutes](https://medium.com/google-cloud/dataproc-spark-cluster-on-gcp-in-minutes-3843b8d8c5f8)\n",
        "\n",
        "[Using the `gcloud` command line tool](https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu)"
      ]
    }
  ]
}