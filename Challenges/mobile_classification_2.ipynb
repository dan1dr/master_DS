{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civil-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bored-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acknowledged-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0               842     0          2.2         0   1       0           7   \n",
       "1              1021     1          0.5         1   0       1          53   \n",
       "2               563     1          0.5         1   2       1          41   \n",
       "3               615     1          2.5         0   0       0          10   \n",
       "4              1821     1          1.2         0  13       1          44   \n",
       "...             ...   ...          ...       ...  ..     ...         ...   \n",
       "1995            794     1          0.5         1   0       1           2   \n",
       "1996           1965     1          2.6         1   0       0          39   \n",
       "1997           1911     0          0.9         1   1       1          36   \n",
       "1998           1512     0          0.9         0   4       1          46   \n",
       "1999            510     1          2.0         1   5       1          45   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
       "0       0.6        188        2  ...         20       756  2549     9     7   \n",
       "1       0.7        136        3  ...        905      1988  2631    17     3   \n",
       "2       0.9        145        5  ...       1263      1716  2603    11     2   \n",
       "3       0.8        131        6  ...       1216      1786  2769    16     8   \n",
       "4       0.6        141        2  ...       1208      1212  1411     8     2   \n",
       "...     ...        ...      ...  ...        ...       ...   ...   ...   ...   \n",
       "1995    0.8        106        6  ...       1222      1890   668    13     4   \n",
       "1996    0.2        187        4  ...        915      1965  2032    11    10   \n",
       "1997    0.7        108        8  ...        868      1632  3057     9     1   \n",
       "1998    0.1        145        5  ...        336       670   869    18    10   \n",
       "1999    0.9        168        6  ...        483       754  3919    19     4   \n",
       "\n",
       "      talk_time  three_g  touch_screen  wifi  price_range  \n",
       "0            19        0             0     1            1  \n",
       "1             7        1             1     0            2  \n",
       "2             9        1             1     0            2  \n",
       "3            11        1             0     0            2  \n",
       "4            15        1             1     0            1  \n",
       "...         ...      ...           ...   ...          ...  \n",
       "1995         19        1             1     0            0  \n",
       "1996         16        1             1     1            2  \n",
       "1997          5        1             1     0            3  \n",
       "1998         19        1             1     1            0  \n",
       "1999          2        1             1     1            3  \n",
       "\n",
       "[2000 rows x 21 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "configured-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>...</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1043</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>226</td>\n",
       "      <td>1412</td>\n",
       "      <td>3476</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>841</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0.8</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>746</td>\n",
       "      <td>857</td>\n",
       "      <td>3895</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1807</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1270</td>\n",
       "      <td>1366</td>\n",
       "      <td>2396</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1546</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>295</td>\n",
       "      <td>1752</td>\n",
       "      <td>3893</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1434</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.5</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>749</td>\n",
       "      <td>810</td>\n",
       "      <td>1773</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>1700</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>644</td>\n",
       "      <td>913</td>\n",
       "      <td>2121</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>609</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1152</td>\n",
       "      <td>1632</td>\n",
       "      <td>1933</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>477</td>\n",
       "      <td>825</td>\n",
       "      <td>1223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>1533</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>832</td>\n",
       "      <td>2509</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>1270</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.1</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>457</td>\n",
       "      <td>608</td>\n",
       "      <td>2828</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0       1           1043     1          1.8         1  14       0           5   \n",
       "1       2            841     1          0.5         1   4       1          61   \n",
       "2       3           1807     1          2.8         0   1       0          27   \n",
       "3       4           1546     0          0.5         1  18       1          25   \n",
       "4       5           1434     0          1.4         0  11       1          49   \n",
       "..    ...            ...   ...          ...       ...  ..     ...         ...   \n",
       "995   996           1700     1          1.9         0   0       1          54   \n",
       "996   997            609     0          1.8         1   0       0          13   \n",
       "997   998           1185     0          1.4         0   1       1           8   \n",
       "998   999           1533     1          0.5         1   0       0          50   \n",
       "999  1000           1270     1          0.5         0   4       1          35   \n",
       "\n",
       "     m_dep  mobile_wt  ...  pc  px_height  px_width   ram  sc_h  sc_w  \\\n",
       "0      0.1        193  ...  16        226      1412  3476    12     7   \n",
       "1      0.8        191  ...  12        746       857  3895     6     0   \n",
       "2      0.9        186  ...   4       1270      1366  2396    17    10   \n",
       "3      0.5         96  ...  20        295      1752  3893    10     0   \n",
       "4      0.5        108  ...  18        749       810  1773    15     8   \n",
       "..     ...        ...  ...  ..        ...       ...   ...   ...   ...   \n",
       "995    0.5        170  ...  17        644       913  2121    14     8   \n",
       "996    0.9        186  ...   2       1152      1632  1933     8     1   \n",
       "997    0.5         80  ...  12        477       825  1223     5     0   \n",
       "998    0.4        171  ...  12         38       832  2509    15    11   \n",
       "999    0.1        140  ...  19        457       608  2828     9     2   \n",
       "\n",
       "     talk_time  three_g  touch_screen  wifi  \n",
       "0            2        0             1     0  \n",
       "1            7        1             0     0  \n",
       "2           10        0             1     1  \n",
       "3            7        1             1     0  \n",
       "4            7        1             0     1  \n",
       "..         ...      ...           ...   ...  \n",
       "995         15        1             1     0  \n",
       "996         19        0             1     1  \n",
       "997         14        1             0     0  \n",
       "998          6        0             1     0  \n",
       "999          3        1             0     1  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "trying-testing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "fig,ax = plt.subplots(figsize = (20,10))\n",
    "sns.heatmap(train.corr(), annot = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intelligent-problem",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "dimensions": [
          {
           "axis": {
            "matches": true
           },
           "label": "battery_power",
           "values": [
            842,
            1021,
            563,
            615,
            1821,
            1859,
            1821,
            1954,
            1445,
            509,
            769,
            1520,
            1815,
            803,
            1866,
            775,
            838,
            595,
            1131,
            682,
            772,
            1709,
            1949,
            1602,
            503,
            961,
            519,
            956,
            1453,
            851,
            1579,
            1568,
            1319,
            1310,
            644,
            725,
            589,
            1725,
            790,
            560,
            1347,
            1646,
            1253,
            1656,
            1195,
            1514,
            1723,
            1054,
            578,
            596,
            1547,
            1760,
            1654,
            1457,
            1073,
            1936,
            823,
            987,
            1757,
            1063,
            1484,
            799,
            1156,
            1720,
            702,
            616,
            1358,
            1866,
            1242,
            1166,
            1448,
            1407,
            605,
            1038,
            797,
            819,
            1114,
            1234,
            1199,
            1103,
            1589,
            999,
            1510,
            1008,
            1127,
            1412,
            1496,
            1083,
            668,
            1309,
            1724,
            1977,
            885,
            879,
            1322,
            1137,
            1355,
            1665,
            657,
            593,
            1883,
            1732,
            543,
            1939,
            553,
            832,
            1661,
            1657,
            1135,
            1775,
            783,
            617,
            867,
            1310,
            1804,
            1900,
            909,
            1084,
            1308,
            1778,
            1701,
            772,
            825,
            1379,
            1166,
            1659,
            826,
            1564,
            1957,
            1571,
            1414,
            1790,
            645,
            668,
            1652,
            1213,
            1272,
            866,
            536,
            523,
            1753,
            1218,
            1537,
            786,
            1678,
            1814,
            1101,
            1461,
            1216,
            506,
            843,
            742,
            1692,
            1485,
            1547,
            1692,
            637,
            1224,
            1356,
            1840,
            1481,
            961,
            1296,
            1193,
            1441,
            517,
            748,
            1126,
            1572,
            1569,
            1270,
            1854,
            625,
            1577,
            534,
            946,
            685,
            1949,
            947,
            801,
            703,
            1165,
            1082,
            959,
            1502,
            1380,
            1266,
            1934,
            1905,
            1831,
            596,
            1562,
            1490,
            1523,
            640,
            1526,
            1989,
            1308,
            609,
            1905,
            1703,
            1445,
            1087,
            671,
            1472,
            765,
            1642,
            1265,
            664,
            1277,
            1395,
            1539,
            1049,
            1827,
            903,
            1987,
            1154,
            1336,
            1886,
            1551,
            850,
            660,
            1225,
            1452,
            1686,
            1634,
            1708,
            1151,
            1578,
            1689,
            1488,
            1313,
            1715,
            1442,
            528,
            1523,
            1133,
            1718,
            1330,
            1799,
            633,
            724,
            822,
            1918,
            1891,
            1373,
            862,
            1273,
            957,
            1617,
            893,
            1210,
            708,
            835,
            1450,
            973,
            601,
            507,
            564,
            1559,
            754,
            728,
            1431,
            767,
            1722,
            1992,
            1876,
            1054,
            1283,
            1281,
            1066,
            730,
            1053,
            1611,
            793,
            1220,
            1662,
            1281,
            538,
            823,
            1733,
            1429,
            1839,
            659,
            1953,
            1172,
            612,
            1563,
            1191,
            615,
            1893,
            1563,
            1995,
            1517,
            832,
            1162,
            1595,
            1329,
            928,
            1656,
            1937,
            977,
            768,
            1464,
            1177,
            1348,
            1956,
            1751,
            1530,
            1997,
            1414,
            1707,
            894,
            645,
            1163,
            1126,
            1648,
            1170,
            508,
            1097,
            728,
            1980,
            1504,
            1379,
            1698,
            920,
            1144,
            1995,
            1430,
            972,
            662,
            1867,
            1730,
            1723,
            1882,
            803,
            580,
            668,
            1391,
            1560,
            821,
            811,
            1989,
            1034,
            618,
            654,
            576,
            667,
            869,
            635,
            609,
            1557,
            1604,
            1182,
            848,
            1610,
            1828,
            880,
            1394,
            1960,
            809,
            899,
            1976,
            879,
            916,
            763,
            508,
            825,
            1864,
            1725,
            1108,
            1011,
            1703,
            1067,
            1334,
            775,
            1899,
            930,
            1058,
            1187,
            1874,
            1482,
            1809,
            864,
            625,
            1880,
            1138,
            560,
            1117,
            1712,
            1836,
            1036,
            1860,
            1375,
            1945,
            1896,
            788,
            880,
            1323,
            1868,
            1266,
            1042,
            1479,
            1476,
            1552,
            1454,
            1007,
            652,
            1504,
            1726,
            1029,
            1582,
            1478,
            1178,
            707,
            755,
            912,
            651,
            1483,
            914,
            1456,
            1178,
            1503,
            1681,
            715,
            1876,
            1190,
            1755,
            1197,
            1048,
            1887,
            1772,
            1421,
            1464,
            728,
            954,
            685,
            1472,
            853,
            1469,
            1310,
            1654,
            902,
            1448,
            1631,
            1608,
            1991,
            1349,
            1589,
            1844,
            712,
            972,
            1406,
            1289,
            932,
            1747,
            1172,
            1128,
            664,
            513,
            1742,
            1512,
            1986,
            965,
            1067,
            1583,
            1653,
            1433,
            765,
            1845,
            752,
            1948,
            1077,
            932,
            1968,
            1122,
            1588,
            601,
            1615,
            1589,
            1417,
            1697,
            1330,
            1476,
            1579,
            1277,
            1089,
            1663,
            1949,
            1138,
            685,
            1889,
            857,
            1902,
            1225,
            1066,
            1554,
            1337,
            1926,
            869,
            1278,
            1773,
            1661,
            1438,
            1661,
            1846,
            1260,
            512,
            581,
            1872,
            687,
            1062,
            1678,
            1417,
            1074,
            1832,
            1039,
            1059,
            1606,
            1928,
            1875,
            1128,
            1748,
            1413,
            825,
            1589,
            535,
            1780,
            1671,
            1821,
            1076,
            532,
            777,
            1217,
            1656,
            1185,
            577,
            737,
            704,
            525,
            504,
            793,
            569,
            1590,
            707,
            767,
            1350,
            1117,
            641,
            1002,
            1408,
            1900,
            726,
            1544,
            1454,
            1230,
            1552,
            1519,
            723,
            1191,
            1168,
            1229,
            1758,
            1290,
            574,
            1271,
            1170,
            1269,
            1288,
            1366,
            1572,
            1627,
            1701,
            1900,
            1974,
            1197,
            587,
            934,
            1195,
            1853,
            1164,
            1512,
            1232,
            1813,
            946,
            739,
            704,
            1663,
            1966,
            1219,
            612,
            1658,
            1263,
            1395,
            621,
            652,
            1175,
            561,
            1137,
            1835,
            1170,
            1595,
            1719,
            1770,
            1312,
            1878,
            1871,
            922,
            1975,
            1212,
            1430,
            1958,
            1836,
            557,
            539,
            880,
            1369,
            1109,
            843,
            598,
            972,
            1944,
            1225,
            644,
            1919,
            501,
            1620,
            1227,
            1359,
            1914,
            1645,
            1063,
            946,
            1231,
            1397,
            701,
            570,
            1993,
            955,
            924,
            600,
            518,
            1063,
            920,
            1715,
            1841,
            543,
            1762,
            1112,
            709,
            1315,
            1762,
            1462,
            571,
            666,
            1994,
            1583,
            1778,
            1926,
            966,
            568,
            1897,
            1695,
            1623,
            914,
            1721,
            890,
            600,
            1677,
            723,
            638,
            852,
            1979,
            710,
            1034,
            600,
            1027,
            1260,
            1793,
            675,
            658,
            1694,
            804,
            1713,
            706,
            1362,
            1527,
            768,
            1314,
            705,
            1403,
            1486,
            781,
            986,
            1310,
            560,
            1348,
            1567,
            1940,
            1979,
            561,
            1717,
            535,
            1413,
            1358,
            1519,
            1254,
            1591,
            977,
            1640,
            663,
            1744,
            1624,
            1108,
            1188,
            817,
            863,
            1397,
            1108,
            1068,
            1702,
            808,
            1156,
            1271,
            696,
            1981,
            614,
            1590,
            1945,
            1135,
            1049,
            1807,
            984,
            720,
            1536,
            771,
            503,
            675,
            1936,
            1303,
            1004,
            1972,
            1822,
            1159,
            1782,
            894,
            1884,
            1648,
            798,
            802,
            1276,
            1331,
            1620,
            1996,
            1092,
            1018,
            545,
            554,
            1264,
            1030,
            1432,
            558,
            829,
            1741,
            1849,
            733,
            1872,
            1853,
            1149,
            891,
            1911,
            550,
            576,
            1760,
            969,
            1160,
            827,
            1786,
            774,
            819,
            1042,
            1368,
            1592,
            1067,
            1892,
            915,
            774,
            1848,
            1501,
            1614,
            911,
            1520,
            1647,
            1347,
            967,
            1442,
            1204,
            1320,
            1800,
            1567,
            1439,
            1422,
            1591,
            880,
            1929,
            1642,
            1511,
            1312,
            1852,
            972,
            691,
            807,
            1113,
            721,
            1188,
            1512,
            805,
            918,
            1320,
            1236,
            1387,
            1883,
            839,
            1593,
            1722,
            1954,
            1788,
            1628,
            1965,
            833,
            571,
            1808,
            1860,
            1368,
            1161,
            1224,
            902,
            787,
            854,
            1184,
            1973,
            510,
            966,
            1438,
            1986,
            1907,
            1489,
            1843,
            825,
            1286,
            840,
            757,
            814,
            1195,
            767,
            1068,
            994,
            1550,
            1878,
            623,
            1829,
            525,
            1065,
            1425,
            1296,
            642,
            1664,
            1498,
            981,
            1236,
            1673,
            712,
            569,
            603,
            1332,
            1413,
            1064,
            1444,
            1366,
            906,
            1554,
            1187,
            1156,
            1918,
            841,
            730,
            878,
            1059,
            1578,
            1829,
            1132,
            1652,
            912,
            1181,
            1497,
            1790,
            1742,
            1549,
            1372,
            1112,
            1005,
            783,
            1698,
            899,
            1062,
            989,
            1983,
            1800,
            1254,
            868,
            1205,
            1284,
            626,
            1763,
            694,
            1265,
            816,
            904,
            979,
            1531,
            934,
            1624,
            874,
            1027,
            1966,
            1048,
            1496,
            959,
            1349,
            1689,
            1558,
            582,
            1485,
            1991,
            553,
            1231,
            805,
            764,
            894,
            1589,
            1456,
            743,
            1061,
            1428,
            674,
            832,
            1625,
            907,
            1631,
            1584,
            1382,
            633,
            1444,
            852,
            922,
            1745,
            862,
            1754,
            1008,
            1569,
            1596,
            1807,
            1660,
            1861,
            648,
            1379,
            1910,
            1807,
            1923,
            1345,
            904,
            1239,
            1330,
            516,
            672,
            892,
            1778,
            1130,
            1359,
            1866,
            1597,
            1046,
            1035,
            1175,
            1261,
            1068,
            1713,
            1688,
            1413,
            559,
            1483,
            860,
            1090,
            686,
            818,
            1456,
            774,
            1068,
            1373,
            1777,
            594,
            1524,
            511,
            1402,
            965,
            1270,
            919,
            1963,
            1977,
            1881,
            683,
            772,
            536,
            1694,
            1633,
            1371,
            551,
            1221,
            1000,
            1986,
            1119,
            1699,
            1099,
            1576,
            950,
            1081,
            606,
            502,
            957,
            1658,
            1681,
            1983,
            1753,
            1632,
            1949,
            1696,
            688,
            1104,
            1122,
            504,
            1154,
            1727,
            1674,
            1948,
            1218,
            531,
            1057,
            576,
            1960,
            673,
            718,
            864,
            547,
            623,
            1793,
            1720,
            812,
            980,
            643,
            1925,
            1408,
            1522,
            587,
            1959,
            1872,
            1043,
            1809,
            1703,
            1095,
            1414,
            1659,
            1540,
            1342,
            1189,
            518,
            1449,
            516,
            1569,
            1312,
            1893,
            1076,
            530,
            1044,
            1330,
            1836,
            1741,
            1869,
            1331,
            1655,
            605,
            1245,
            1709,
            846,
            1403,
            688,
            775,
            1154,
            1375,
            1827,
            1697,
            741,
            1672,
            514,
            1375,
            989,
            1510,
            1266,
            1396,
            808,
            1542,
            745,
            1735,
            586,
            1341,
            781,
            501,
            1224,
            1970,
            618,
            1537,
            1018,
            1545,
            1871,
            1729,
            680,
            1106,
            635,
            618,
            1671,
            1562,
            1975,
            831,
            1524,
            1528,
            1447,
            1344,
            564,
            921,
            1413,
            831,
            802,
            1923,
            1193,
            904,
            1285,
            1635,
            1923,
            1851,
            727,
            1396,
            877,
            696,
            1009,
            1526,
            987,
            1414,
            1732,
            1587,
            1097,
            504,
            1159,
            1910,
            865,
            1930,
            860,
            1268,
            999,
            622,
            1250,
            763,
            1100,
            815,
            618,
            553,
            673,
            563,
            1590,
            1031,
            909,
            1880,
            1479,
            1271,
            709,
            1744,
            1314,
            1136,
            1039,
            1300,
            732,
            511,
            1092,
            921,
            1030,
            1130,
            1299,
            1009,
            1735,
            1129,
            708,
            894,
            530,
            1481,
            1068,
            1487,
            1524,
            1093,
            1816,
            627,
            989,
            1895,
            805,
            713,
            864,
            1177,
            582,
            1898,
            1158,
            1451,
            1820,
            984,
            580,
            1263,
            1237,
            514,
            1515,
            721,
            603,
            1426,
            1820,
            1215,
            697,
            717,
            1348,
            1164,
            578,
            1971,
            1794,
            1558,
            1597,
            1240,
            1994,
            1221,
            989,
            1180,
            718,
            1670,
            534,
            1566,
            1935,
            504,
            913,
            1317,
            917,
            712,
            1083,
            1039,
            1747,
            1449,
            1872,
            1796,
            1097,
            1562,
            1433,
            740,
            676,
            503,
            1020,
            896,
            1824,
            1512,
            1053,
            1944,
            1174,
            627,
            1602,
            1733,
            586,
            1528,
            875,
            1602,
            1426,
            1370,
            609,
            840,
            991,
            1724,
            584,
            860,
            1541,
            615,
            912,
            1278,
            1365,
            1702,
            589,
            1347,
            1644,
            956,
            1089,
            871,
            664,
            1874,
            1928,
            888,
            1077,
            1023,
            1426,
            831,
            1496,
            1433,
            1095,
            643,
            1142,
            730,
            1901,
            1510,
            1924,
            1825,
            1275,
            987,
            1538,
            1965,
            808,
            712,
            1507,
            912,
            769,
            948,
            507,
            1384,
            1766,
            1407,
            614,
            1972,
            1039,
            511,
            1811,
            1159,
            1848,
            1988,
            1469,
            1423,
            1974,
            835,
            1429,
            947,
            1446,
            1332,
            1498,
            1576,
            1398,
            1885,
            798,
            1436,
            1998,
            1321,
            1021,
            1339,
            1210,
            1949,
            882,
            1549,
            1991,
            796,
            1012,
            1318,
            1708,
            598,
            541,
            818,
            1414,
            601,
            561,
            1616,
            1263,
            1604,
            539,
            1071,
            826,
            771,
            1811,
            1842,
            1420,
            1763,
            1163,
            1805,
            610,
            1533,
            1924,
            1801,
            1726,
            794,
            1686,
            1444,
            1004,
            1242,
            539,
            717,
            1540,
            672,
            1325,
            950,
            948,
            1328,
            1447,
            1973,
            1731,
            1617,
            1791,
            851,
            856,
            714,
            951,
            1303,
            1550,
            1759,
            1448,
            1987,
            908,
            820,
            904,
            667,
            1109,
            1333,
            1352,
            1600,
            1454,
            1489,
            1823,
            1581,
            1672,
            1283,
            630,
            659,
            1811,
            688,
            514,
            1933,
            915,
            1006,
            1134,
            599,
            973,
            1180,
            1237,
            659,
            1142,
            1002,
            1392,
            602,
            1249,
            531,
            1450,
            1342,
            832,
            867,
            1404,
            840,
            1368,
            1927,
            1714,
            1201,
            1796,
            1147,
            776,
            510,
            1045,
            1497,
            1425,
            726,
            1576,
            714,
            1595,
            541,
            1617,
            1154,
            1638,
            1150,
            1254,
            1806,
            603,
            1834,
            520,
            565,
            1689,
            742,
            1143,
            761,
            959,
            772,
            1015,
            1824,
            1130,
            1183,
            574,
            1472,
            1868,
            1375,
            881,
            1742,
            1225,
            1970,
            1186,
            1762,
            1731,
            852,
            848,
            1575,
            1554,
            1972,
            827,
            1063,
            1695,
            1343,
            834,
            595,
            1081,
            911,
            844,
            1335,
            1883,
            1128,
            826,
            1650,
            1162,
            1517,
            963,
            1837,
            1028,
            1831,
            571,
            1770,
            970,
            642,
            622,
            600,
            1412,
            733,
            1070,
            875,
            1994,
            823,
            1908,
            790,
            1330,
            1660,
            1776,
            1611,
            1410,
            1772,
            1280,
            1712,
            1562,
            891,
            1957,
            1110,
            875,
            1211,
            769,
            671,
            1872,
            1076,
            1325,
            911,
            1273,
            1062,
            1317,
            940,
            729,
            1494,
            1546,
            1253,
            895,
            793,
            1628,
            625,
            1110,
            999,
            1856,
            1715,
            1897,
            1202,
            1171,
            964,
            1973,
            1992,
            546,
            1093,
            1880,
            1765,
            1640,
            1830,
            826,
            1864,
            586,
            1206,
            832,
            848,
            1851,
            1166,
            1776,
            1254,
            1494,
            984,
            1179,
            513,
            557,
            744,
            1129,
            1148,
            1793,
            1310,
            1604,
            1676,
            1001,
            1086,
            1986,
            634,
            502,
            1641,
            623,
            962,
            1057,
            1862,
            555,
            1000,
            841,
            865,
            999,
            1232,
            1194,
            612,
            1362,
            1469,
            986,
            1843,
            1561,
            1695,
            667,
            1768,
            1269,
            1109,
            936,
            1713,
            1175,
            1940,
            855,
            771,
            645,
            1307,
            1948,
            1509,
            626,
            1190,
            1727,
            1670,
            1150,
            1208,
            1812,
            770,
            1559,
            1902,
            1751,
            1416,
            1288,
            1003,
            1715,
            1967,
            962,
            1487,
            1569,
            1146,
            1379,
            1504,
            1945,
            1630,
            1125,
            614,
            854,
            1470,
            1826,
            807,
            1996,
            1083,
            1035,
            1521,
            1314,
            535,
            673,
            1219,
            1606,
            1603,
            1958,
            695,
            1441,
            1470,
            1890,
            1152,
            1619,
            1002,
            742,
            1306,
            1424,
            942,
            1934,
            1290,
            930,
            1699,
            1849,
            1922,
            548,
            916,
            1783,
            1083,
            1698,
            1969,
            869,
            1337,
            637,
            833,
            1248,
            1174,
            1831,
            1002,
            1010,
            1117,
            990,
            1564,
            1065,
            1982,
            1607,
            1066,
            681,
            1163,
            1285,
            753,
            1779,
            987,
            511,
            1044,
            1855,
            946,
            657,
            1673,
            731,
            926,
            1261,
            618,
            936,
            1485,
            1339,
            508,
            1938,
            995,
            1086,
            1322,
            1864,
            1944,
            581,
            1020,
            1936,
            1329,
            808,
            994,
            1811,
            765,
            993,
            1136,
            719,
            733,
            1946,
            1230,
            1577,
            1514,
            579,
            574,
            1072,
            843,
            1492,
            1807,
            909,
            1876,
            1436,
            1251,
            636,
            1354,
            930,
            1830,
            1424,
            583,
            648,
            1203,
            1777,
            1299,
            1494,
            527,
            793,
            873,
            722,
            1250,
            1358,
            1035,
            1289,
            1445,
            1702,
            534,
            628,
            713,
            1207,
            683,
            594,
            537,
            1565,
            1052,
            713,
            1766,
            584,
            1541,
            837,
            935,
            665,
            680,
            877,
            1493,
            1762,
            842,
            925,
            1692,
            1576,
            1065,
            958,
            623,
            897,
            1110,
            616,
            1680,
            1715,
            983,
            720,
            1660,
            1564,
            592,
            969,
            1356,
            504,
            1546,
            689,
            1685,
            1792,
            1786,
            1944,
            1077,
            648,
            1702,
            1571,
            856,
            1786,
            914,
            1615,
            649,
            1646,
            1189,
            1043,
            1920,
            1220,
            1748,
            907,
            1193,
            1766,
            1278,
            1234,
            1961,
            1745,
            1442,
            1083,
            868,
            864,
            1142,
            608,
            983,
            1720,
            1802,
            591,
            1205,
            1369,
            713,
            1082,
            1719,
            1544,
            1090,
            1991,
            1367,
            1179,
            721,
            1549,
            1349,
            1799,
            1075,
            684,
            1837,
            1619,
            965,
            730,
            1361,
            1494,
            1027,
            1807,
            709,
            689,
            955,
            1872,
            1259,
            695,
            1969,
            759,
            936,
            1176,
            1802,
            772,
            1318,
            591,
            569,
            764,
            1056,
            1982,
            1830,
            1122,
            1023,
            1283,
            1602,
            732,
            1854,
            718,
            897,
            1405,
            798,
            1035,
            1396,
            1749,
            1588,
            1600,
            1522,
            1279,
            719,
            544,
            1007,
            590,
            1788,
            748,
            757,
            1561,
            1327,
            797,
            727,
            686,
            1515,
            555,
            1589,
            1976,
            1884,
            1063,
            672,
            635,
            1906,
            1753,
            659,
            1583,
            1783,
            1667,
            640,
            1913,
            538,
            1191,
            816,
            915,
            1157,
            1201,
            1379,
            1483,
            1614,
            930,
            1454,
            1784,
            1262,
            797,
            1829,
            1139,
            618,
            1547,
            586,
            1617,
            1882,
            674,
            1467,
            858,
            794,
            1965,
            1911,
            1512,
            510
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "blue",
           "values": [
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "clock_speed",
           "values": [
            2.2,
            0.5,
            0.5,
            2.5,
            1.2,
            0.5,
            1.7,
            0.5,
            0.5,
            0.6,
            2.9,
            2.2,
            2.8,
            2.1,
            0.5,
            1,
            0.5,
            0.9,
            0.5,
            0.5,
            1.1,
            2.1,
            2.6,
            2.8,
            1.2,
            1.4,
            1.6,
            0.5,
            1.6,
            0.5,
            0.5,
            0.5,
            0.9,
            2.2,
            2.7,
            1.3,
            2.3,
            1.6,
            2,
            0.5,
            2.9,
            2.3,
            0.5,
            1,
            2.8,
            2.9,
            1.1,
            1.8,
            2.6,
            2.1,
            3,
            1.4,
            1.5,
            1.9,
            0.5,
            2.1,
            2.7,
            1.3,
            0.5,
            1.4,
            3,
            2.3,
            1.2,
            2,
            2.6,
            1.9,
            0.5,
            1.4,
            1.1,
            1.5,
            0.5,
            2.4,
            1,
            1.2,
            2.9,
            0.6,
            2.8,
            1.6,
            2.5,
            1,
            0.6,
            2.9,
            0.9,
            0.8,
            2.9,
            2.4,
            2,
            2.9,
            0.5,
            1.1,
            2,
            2,
            2.3,
            2.5,
            1.7,
            1,
            2.3,
            0.5,
            2.5,
            0.5,
            0.5,
            1.1,
            0.5,
            2.8,
            2.4,
            0.7,
            1.9,
            2.4,
            2,
            1.5,
            1.8,
            2.3,
            1.4,
            1.1,
            0.5,
            2.3,
            1.4,
            2.6,
            1.3,
            0.5,
            1.6,
            2.4,
            0.5,
            1.1,
            2,
            2.8,
            2.4,
            2.3,
            0.9,
            1.4,
            2,
            2.5,
            0.5,
            2.9,
            1.1,
            0.5,
            2.5,
            0.5,
            2.4,
            2.6,
            0.5,
            1.8,
            2.5,
            2.2,
            2.1,
            1.4,
            1.8,
            2.4,
            3,
            1.6,
            0.5,
            2.2,
            2.1,
            1,
            2.9,
            2.3,
            2.3,
            0.5,
            2.8,
            0.5,
            0.7,
            1.5,
            0.9,
            0.9,
            2.1,
            1.4,
            1.7,
            2.4,
            0.6,
            2.8,
            0.6,
            3,
            1.2,
            0.5,
            0.5,
            1.7,
            0.5,
            1.3,
            0.8,
            2.2,
            2.7,
            2.4,
            0.8,
            2.6,
            1.2,
            2.1,
            0.5,
            2.6,
            0.5,
            1.4,
            1.4,
            1.3,
            0.5,
            2.4,
            0.6,
            2.1,
            2.5,
            1.9,
            0.5,
            0.6,
            0.5,
            2.4,
            1.3,
            0.9,
            2.3,
            0.5,
            0.5,
            1.5,
            1.1,
            0.5,
            1.8,
            2.6,
            2.2,
            1.7,
            1,
            0.5,
            1.2,
            0.9,
            0.5,
            1.1,
            1.6,
            0.5,
            2.1,
            0.5,
            2.8,
            1.4,
            2.4,
            2.9,
            1.9,
            1.8,
            0.5,
            1.8,
            1,
            1,
            1.7,
            1.8,
            1.4,
            2.5,
            0.5,
            0.5,
            2.2,
            2.7,
            0.5,
            1.9,
            2.8,
            1,
            0.8,
            1,
            2.5,
            1.3,
            2.5,
            0.5,
            2.7,
            1.1,
            2.7,
            1.5,
            1.4,
            0.5,
            1.8,
            1.6,
            0.5,
            2.7,
            0.5,
            1.5,
            2.6,
            1.6,
            0.5,
            1.1,
            1.6,
            2.3,
            0.6,
            2.1,
            0.8,
            0.5,
            1,
            2.9,
            0.5,
            2.8,
            0.8,
            0.5,
            2.8,
            1.5,
            1.2,
            2.7,
            2.1,
            1.9,
            1.9,
            2.4,
            1.3,
            0.5,
            2.1,
            1.7,
            1.9,
            0.5,
            1.6,
            2,
            2.6,
            1,
            0.5,
            0.5,
            1.7,
            1.1,
            0.9,
            0.5,
            0.9,
            2,
            2.2,
            0.5,
            2,
            1.8,
            1.5,
            1.4,
            0.5,
            1.2,
            2.8,
            0.7,
            0.7,
            0.9,
            0.8,
            2.1,
            1.7,
            0.5,
            2.3,
            2.1,
            2.1,
            2.1,
            1.5,
            0.5,
            0.5,
            0.5,
            0.5,
            2.3,
            2,
            1,
            1.3,
            0.8,
            1.2,
            2.3,
            1.4,
            2.3,
            1.8,
            2.4,
            2.9,
            2.7,
            2.1,
            1.3,
            2.8,
            0.6,
            1,
            0.5,
            3,
            2.8,
            1.7,
            0.5,
            2.1,
            2.9,
            2.1,
            2,
            0.9,
            2.3,
            0.8,
            0.5,
            0.7,
            1.5,
            2.6,
            1.7,
            1.6,
            2.9,
            1.4,
            0.5,
            2.9,
            1.1,
            1.5,
            1.7,
            2.4,
            2.6,
            0.6,
            1.3,
            1.5,
            2.2,
            2.7,
            0.5,
            1.7,
            3,
            1.9,
            1.8,
            1.4,
            0.5,
            0.5,
            1.4,
            1.7,
            2.4,
            2.3,
            1.1,
            1.6,
            0.5,
            2.5,
            1.7,
            2.5,
            2,
            1.9,
            2.7,
            0.5,
            1.2,
            2.2,
            0.5,
            2,
            2.4,
            1.5,
            0.9,
            1.9,
            2.8,
            0.8,
            1.3,
            2.1,
            2.8,
            2,
            0.8,
            2.1,
            0.9,
            0.5,
            2.2,
            0.7,
            2.5,
            2.3,
            2.8,
            2.2,
            1.3,
            1.3,
            0.9,
            3,
            0.5,
            2.8,
            0.5,
            2.3,
            0.5,
            0.5,
            2.8,
            2,
            2.1,
            2.3,
            1.8,
            0.7,
            1.4,
            0.9,
            1.8,
            2.7,
            2,
            2.5,
            2.3,
            0.5,
            1.7,
            0.5,
            0.9,
            1.3,
            0.9,
            2.1,
            1.4,
            1.2,
            0.5,
            2.3,
            1.7,
            0.5,
            1.5,
            2.4,
            1.6,
            0.5,
            1.6,
            2.9,
            0.5,
            0.5,
            2.8,
            2.5,
            0.5,
            0.9,
            1.2,
            0.5,
            2.1,
            0.5,
            2.2,
            2.7,
            0.7,
            1.3,
            0.5,
            1.5,
            3,
            0.9,
            2.5,
            1.8,
            1.1,
            2.7,
            2.8,
            2,
            2.8,
            2.4,
            1.6,
            0.5,
            0.5,
            1.7,
            1.3,
            0.5,
            2.5,
            0.8,
            2.2,
            1.4,
            0.6,
            2,
            0.5,
            2.2,
            2.3,
            1.3,
            2.5,
            2.3,
            0.5,
            2.4,
            0.7,
            1.3,
            2.4,
            1.4,
            1.8,
            0.5,
            1.1,
            1.7,
            0.5,
            0.5,
            1.4,
            0.5,
            0.5,
            0.6,
            0.9,
            2.6,
            0.8,
            2.6,
            0.5,
            1.6,
            1.9,
            2.4,
            2.4,
            2.2,
            0.5,
            0.5,
            1.2,
            0.6,
            1.9,
            0.5,
            1.9,
            0.5,
            0.6,
            1.1,
            0.8,
            2.1,
            2,
            2.9,
            2.3,
            1.6,
            2.2,
            1.2,
            2.1,
            1.9,
            2.4,
            1.3,
            0.5,
            0.5,
            0.5,
            2.8,
            0.5,
            1.2,
            1.3,
            2.8,
            0.7,
            0.8,
            0.9,
            0.5,
            0.5,
            1,
            0.5,
            2.3,
            1.1,
            2,
            1,
            1,
            0.5,
            2.9,
            0.6,
            2.6,
            2.3,
            2.7,
            0.5,
            1.4,
            1,
            0.7,
            1.4,
            0.5,
            2.8,
            2.7,
            0.5,
            0.6,
            1,
            2.2,
            2.3,
            2.9,
            0.8,
            2,
            2.3,
            0.5,
            1.5,
            2,
            1.5,
            1.5,
            2.6,
            0.9,
            1.9,
            2.1,
            1.6,
            0.6,
            0.5,
            1.8,
            2.7,
            1.8,
            2.3,
            2.7,
            0.5,
            0.7,
            1.3,
            1.5,
            2.3,
            2.2,
            1.1,
            0.5,
            0.7,
            1.8,
            1.1,
            2.1,
            1.2,
            2.9,
            0.7,
            0.5,
            0.5,
            0.5,
            0.5,
            0.5,
            0.8,
            2.5,
            0.5,
            1.9,
            1.3,
            0.5,
            2.7,
            0.5,
            2,
            2.6,
            2.5,
            1.7,
            1.6,
            0.5,
            1.5,
            2.1,
            1.4,
            1.1,
            0.6,
            2,
            3,
            0.5,
            1.2,
            0.5,
            0.5,
            2.2,
            0.5,
            1.8,
            0.5,
            1.7,
            1,
            1.6,
            0.5,
            2.6,
            2.5,
            0.5,
            0.6,
            1.8,
            2.3,
            2.7,
            0.6,
            0.8,
            2.5,
            0.5,
            0.5,
            2.7,
            0.5,
            2.9,
            0.5,
            2.7,
            2.6,
            1.1,
            1.5,
            1.4,
            2,
            1.3,
            0.5,
            0.9,
            2.9,
            2.1,
            1.3,
            1.5,
            0.5,
            2.5,
            2.9,
            1.2,
            2.2,
            2.8,
            0.7,
            0.8,
            1,
            1.6,
            1.3,
            1.4,
            1.3,
            1.6,
            2.4,
            1,
            2,
            2.3,
            0.5,
            0.5,
            0.5,
            0.5,
            1.9,
            1.7,
            0.6,
            1.6,
            2.8,
            0.6,
            2.1,
            0.5,
            1.4,
            0.8,
            0.5,
            2.5,
            2.4,
            1.9,
            1,
            2.9,
            1.2,
            1.2,
            2.8,
            2.8,
            2.1,
            2.1,
            0.5,
            2.4,
            2.4,
            2.9,
            1.6,
            2.6,
            2.5,
            2.7,
            0.7,
            0.5,
            1.3,
            2.9,
            1.6,
            1.7,
            2.8,
            0.5,
            1.8,
            2.3,
            2.1,
            0.9,
            0.5,
            2.2,
            0.5,
            2,
            0.5,
            0.8,
            1.9,
            2.4,
            1.1,
            1.1,
            0.5,
            0.6,
            2.5,
            2.2,
            3,
            2.1,
            1.8,
            0.7,
            1.1,
            2.8,
            1.4,
            1.1,
            1.9,
            2.9,
            0.5,
            1.4,
            0.9,
            1.6,
            2.2,
            2.6,
            2.1,
            0.7,
            2.1,
            0.9,
            1,
            2.2,
            1.7,
            0.5,
            1.6,
            2.5,
            1.3,
            1.4,
            1.3,
            2.7,
            0.5,
            0.6,
            1.3,
            1.5,
            0.5,
            0.7,
            0.9,
            0.5,
            0.9,
            0.5,
            2.7,
            2,
            1,
            1,
            0.6,
            0.5,
            0.5,
            0.5,
            2.2,
            0.6,
            0.5,
            0.5,
            1.6,
            1.3,
            1.6,
            0.6,
            0.5,
            0.5,
            1.5,
            1.6,
            1.7,
            2,
            1.8,
            0.5,
            2,
            2.4,
            1.8,
            1.5,
            2.3,
            0.6,
            2.3,
            1.1,
            1.1,
            0.8,
            1.3,
            2.8,
            2.5,
            0.5,
            2,
            0.5,
            2.4,
            0.5,
            1.2,
            1.4,
            2.8,
            2.9,
            0.7,
            1.9,
            2.8,
            2.8,
            1.6,
            0.5,
            0.7,
            0.8,
            0.6,
            2.9,
            0.6,
            1.7,
            2.1,
            2.7,
            2.3,
            2.7,
            2.8,
            0.5,
            1.3,
            1.3,
            0.5,
            2.9,
            1.7,
            1,
            0.6,
            0.5,
            2.3,
            0.7,
            2.3,
            1.6,
            0.5,
            2.7,
            0.5,
            0.5,
            1.7,
            2.5,
            2.7,
            1.5,
            2,
            1.2,
            3,
            1.7,
            1.7,
            0.5,
            2.8,
            0.7,
            2.9,
            1.8,
            0.6,
            0.5,
            1.8,
            0.5,
            2,
            2,
            2.6,
            1.3,
            2.2,
            0.8,
            1.5,
            1.9,
            1.7,
            2.2,
            2.4,
            2.4,
            0.5,
            2.8,
            1.8,
            0.5,
            0.7,
            0.8,
            1.2,
            0.9,
            2.6,
            1.6,
            0.5,
            2.1,
            1.6,
            2.8,
            1.1,
            0.5,
            1.4,
            0.5,
            1.3,
            0.5,
            1.4,
            2.1,
            1.6,
            0.7,
            1.5,
            2.3,
            2.1,
            2.5,
            0.5,
            2.9,
            0.5,
            2.8,
            0.9,
            1.9,
            0.5,
            1.7,
            2,
            0.5,
            3,
            1.6,
            1.2,
            0.9,
            0.7,
            0.5,
            0.5,
            0.5,
            2.5,
            0.6,
            2.5,
            2.3,
            2.8,
            0.6,
            1.3,
            0.5,
            0.7,
            3,
            2.5,
            0.5,
            0.5,
            2.6,
            1.6,
            1.3,
            0.5,
            1.3,
            1.6,
            0.5,
            0.5,
            1.9,
            3,
            0.8,
            2.6,
            0.6,
            0.6,
            0.5,
            2.3,
            0.5,
            1,
            1.3,
            2.2,
            0.7,
            2.6,
            1.4,
            1.8,
            1.7,
            0.5,
            2.8,
            0.5,
            0.5,
            2.7,
            0.5,
            0.5,
            0.5,
            2.6,
            0.5,
            2.5,
            2.5,
            1.5,
            0.7,
            1.4,
            2.5,
            0.5,
            1.2,
            1.4,
            1.4,
            1.7,
            2.5,
            1.7,
            2.8,
            0.5,
            0.5,
            2.1,
            2.8,
            2.8,
            2.8,
            1.1,
            2.1,
            1.2,
            2.1,
            1.7,
            0.7,
            1.7,
            1.9,
            0.9,
            0.7,
            0.7,
            0.5,
            0.5,
            1,
            3,
            2.5,
            0.7,
            2.5,
            0.5,
            0.5,
            0.5,
            0.5,
            0.5,
            0.5,
            2.6,
            2.5,
            0.7,
            1,
            2.3,
            2.4,
            0.8,
            1.1,
            0.8,
            0.8,
            0.5,
            2.5,
            2.6,
            2.8,
            1.1,
            1.1,
            0.5,
            2.6,
            0.5,
            0.5,
            1.5,
            0.6,
            0.8,
            2.6,
            2.5,
            1.5,
            2.5,
            2,
            0.5,
            0.8,
            0.5,
            1.5,
            0.5,
            0.7,
            1.3,
            1.8,
            2.5,
            2.6,
            2.4,
            2.3,
            2.3,
            0.6,
            1.9,
            0.5,
            2.1,
            1.3,
            0.5,
            1.8,
            1.6,
            2.2,
            2.9,
            2.2,
            1.3,
            2.1,
            2.1,
            2.1,
            1.9,
            0.6,
            0.6,
            0.5,
            1.2,
            1.9,
            0.7,
            1.8,
            1.8,
            2.5,
            1.5,
            3,
            0.6,
            1.7,
            2.4,
            0.5,
            1.3,
            3,
            1.2,
            2.4,
            2.6,
            1,
            1.9,
            1.5,
            1.5,
            1.3,
            2.4,
            1,
            1.1,
            2,
            2.2,
            0.8,
            0.5,
            0.8,
            2.8,
            2.7,
            1.4,
            1.2,
            2,
            1.3,
            2.9,
            1.8,
            0.7,
            1.4,
            0.5,
            1.6,
            2,
            0.5,
            2.2,
            2.2,
            2,
            1.5,
            2.1,
            0.5,
            0.5,
            1.8,
            1.5,
            1.9,
            2.8,
            2.2,
            0.5,
            2.7,
            0.5,
            2.3,
            0.7,
            0.5,
            2.2,
            0.5,
            1.8,
            2.8,
            2.8,
            1.8,
            2.8,
            1.9,
            0.7,
            2.4,
            2,
            2.7,
            0.5,
            2.5,
            1.9,
            1.6,
            1.8,
            0.7,
            0.5,
            1.7,
            1.6,
            2.5,
            0.9,
            0.5,
            1.8,
            0.7,
            0.5,
            1.4,
            0.5,
            2.1,
            2,
            0.5,
            1.6,
            2.6,
            0.5,
            2.5,
            2.5,
            1.7,
            1,
            2.7,
            0.5,
            2,
            2.6,
            0.5,
            0.5,
            0.5,
            2.5,
            0.9,
            1.6,
            0.8,
            2.3,
            1.6,
            1.2,
            2.8,
            1.6,
            2.5,
            0.5,
            0.5,
            2.8,
            1.7,
            1.8,
            0.5,
            2.5,
            1,
            0.6,
            0.5,
            0.5,
            0.5,
            1.6,
            1.4,
            0.5,
            2.3,
            1.8,
            2.2,
            1.8,
            1.5,
            1,
            0.8,
            0.6,
            2.9,
            1.8,
            0.7,
            1.6,
            2.7,
            1.5,
            1.4,
            2.4,
            0.7,
            0.6,
            1.6,
            1.4,
            0.5,
            1.2,
            2,
            1.8,
            1.8,
            1.3,
            2.8,
            0.5,
            2.8,
            1.2,
            0.6,
            1,
            0.5,
            1.5,
            2.8,
            1.9,
            1.4,
            0.6,
            1.5,
            0.6,
            2.6,
            2.6,
            1.7,
            2.3,
            0.8,
            1.7,
            1.3,
            2.3,
            2.6,
            0.7,
            1.1,
            0.5,
            0.5,
            0.6,
            2.6,
            1.1,
            0.5,
            2.3,
            2.3,
            0.5,
            1.9,
            0.5,
            0.9,
            0.7,
            1.2,
            2.3,
            1.9,
            2.8,
            1.3,
            1.8,
            2.9,
            2.9,
            0.5,
            3,
            2.5,
            1.5,
            1.3,
            0.5,
            0.5,
            0.6,
            0.5,
            0.8,
            0.6,
            0.5,
            2.8,
            1.3,
            1.2,
            0.9,
            1.6,
            2.4,
            0.5,
            2.5,
            1.2,
            1.3,
            1.3,
            2.3,
            1.6,
            0.5,
            0.7,
            1.5,
            2.2,
            0.5,
            0.5,
            1.9,
            0.8,
            0.8,
            2.3,
            0.5,
            0.5,
            1.7,
            2.8,
            2.3,
            1.4,
            1.9,
            2,
            2.6,
            0.5,
            2.9,
            2.9,
            2.2,
            2.8,
            0.5,
            1.9,
            0.5,
            1.6,
            1.1,
            0.5,
            0.5,
            1.9,
            2.5,
            0.5,
            2.1,
            1.6,
            2.4,
            1.5,
            1.1,
            0.7,
            0.5,
            0.5,
            0.5,
            1.5,
            0.5,
            0.5,
            0.5,
            2.3,
            1.5,
            0.5,
            2.3,
            1.9,
            0.5,
            0.5,
            2.1,
            2.7,
            1.4,
            0.5,
            1.9,
            2.1,
            0.5,
            2.7,
            1.3,
            2.1,
            1.4,
            2.4,
            2.5,
            1.4,
            2.1,
            2.8,
            1,
            1.7,
            0.7,
            2,
            2.2,
            0.5,
            2.5,
            2.9,
            2.8,
            2,
            2.2,
            0.9,
            2.9,
            0.5,
            2.5,
            2.5,
            0.5,
            1.4,
            2.7,
            2.5,
            0.9,
            0.5,
            0.5,
            2.1,
            2.8,
            0.5,
            1.5,
            2.1,
            0.9,
            0.5,
            0.9,
            0.5,
            0.5,
            2.3,
            0.8,
            1.7,
            2.6,
            1.2,
            0.9,
            0.6,
            2.2,
            2,
            2.1,
            1.9,
            1,
            2.6,
            1.1,
            1,
            2.7,
            2.3,
            2,
            1.5,
            2.1,
            2.3,
            0.6,
            0.5,
            2.3,
            1.2,
            0.5,
            1.3,
            1.2,
            0.8,
            1.9,
            0.5,
            0.5,
            2,
            3,
            0.5,
            2.9,
            1,
            0.5,
            0.7,
            0.5,
            0.5,
            0.7,
            1.4,
            1.8,
            1.5,
            2.5,
            2.8,
            1.6,
            0.6,
            1.5,
            2.3,
            2.4,
            0.5,
            2.6,
            1.9,
            1.4,
            2.2,
            3,
            1.6,
            0.5,
            2.4,
            2.3,
            1,
            2.5,
            0.5,
            1,
            2.8,
            2.2,
            2,
            2.3,
            1.3,
            0.5,
            1.9,
            1.8,
            0.6,
            0.8,
            3,
            0.7,
            0.7,
            1,
            0.5,
            0.5,
            1.8,
            0.5,
            2.2,
            0.5,
            0.5,
            1.6,
            1.3,
            0.9,
            0.7,
            1.9,
            1.2,
            0.6,
            1.4,
            1.2,
            0.5,
            2.9,
            0.5,
            2.3,
            1.9,
            2.2,
            0.7,
            0.5,
            1.5,
            2.9,
            2,
            2.4,
            1.6,
            0.7,
            0.5,
            1.9,
            1.8,
            2.7,
            1.5,
            1.7,
            0.5,
            2.6,
            1.9,
            2.4,
            1.7,
            2,
            2.4,
            0.6,
            2.6,
            0.5,
            2,
            1.1,
            0.5,
            0.5,
            0.6,
            2.2,
            0.6,
            3,
            0.5,
            2.8,
            2.9,
            0.5,
            2.1,
            1.2,
            1.6,
            2.2,
            0.8,
            1.9,
            0.5,
            1.7,
            0.8,
            2.3,
            2.7,
            2.3,
            1.2,
            1.6,
            2.8,
            2.7,
            0.5,
            0.8,
            0.8,
            1.1,
            1.4,
            1.1,
            2.7,
            1.5,
            3,
            2.7,
            0.5,
            2.9,
            2.6,
            1.2,
            2.3,
            0.5,
            2.9,
            1.7,
            0.8,
            2.8,
            0.5,
            0.5,
            2.6,
            2,
            2.8,
            2.8,
            0.5,
            2,
            1.7,
            1,
            0.5,
            0.5,
            2.5,
            1.8,
            2.9,
            0.6,
            2.8,
            2,
            2.1,
            3,
            0.5,
            2.9,
            1.3,
            1.5,
            1.3,
            0.5,
            2.5,
            1.8,
            0.5,
            2.9,
            0.5,
            0.5,
            1,
            1.5,
            1.6,
            3,
            1.5,
            1.9,
            1.2,
            2.8,
            2.7,
            0.7,
            1,
            1.7,
            0.5,
            0.5,
            2.8,
            2,
            0.5,
            1.6,
            2.8,
            1.9,
            1.9,
            1.6,
            0.5,
            0.5,
            0.5,
            0.5,
            0.6,
            2.2,
            1.1,
            2.1,
            0.9,
            1.8,
            0.5,
            2.1,
            2.9,
            1.4,
            2.7,
            1.4,
            2.1,
            0.5,
            0.5,
            0.5,
            1,
            0.5,
            0.5,
            1.4,
            2.8,
            0.8,
            1.9,
            2.7,
            0.8,
            0.6,
            2.4,
            0.9,
            2.3,
            2.6,
            2,
            0.5,
            2.7,
            1.4,
            2.8,
            1.1,
            1.1,
            3,
            0.5,
            2.1,
            0.8,
            0.5,
            2.6,
            1.9,
            0.9,
            1.8,
            0.5,
            1.4,
            1.9,
            1.6,
            0.5,
            0.8,
            0.5,
            1.1,
            2.3,
            0.9,
            0.5,
            1.3,
            0.5,
            1.3,
            1.7,
            2.3,
            1.9,
            0.5,
            0.6,
            0.7,
            1.3,
            1.7,
            0.5,
            1.2,
            2,
            2.5,
            2.3,
            1.6,
            0.5,
            0.6,
            2.7,
            1.6,
            1,
            1.4,
            2.5,
            3,
            2.2,
            1.3,
            1.5,
            2.9,
            0.7,
            1.3,
            2.4,
            0.5,
            0.5,
            0.7,
            0.5,
            1,
            0.5,
            0.9,
            0.5,
            0.5,
            2.2,
            2.8,
            1.9,
            0.7,
            2.8,
            0.5,
            1.1,
            1.5,
            1.7,
            1.9,
            0.8,
            2.2,
            2.3,
            0.5,
            1.8,
            1.4,
            0.5,
            2.1,
            2.6,
            2,
            0.5,
            0.9,
            1.6,
            0.5,
            1.7,
            1.5,
            2.4,
            0.5,
            0.7,
            2.4,
            1.5,
            1.5,
            0.6,
            1.8,
            2.1,
            2.1,
            2.2,
            1.7,
            1.6,
            0.8,
            1.4,
            1.5,
            2.2,
            1.2,
            1.3,
            0.6,
            0.9,
            0.6,
            1.5,
            0.8,
            1.8,
            0.5,
            1,
            0.5,
            2.9,
            0.6,
            1,
            1.2,
            0.6,
            2.5,
            1.8,
            0.5,
            2.9,
            1.1,
            2.6,
            0.7,
            2.8,
            1.4,
            2.5,
            1.9,
            2.7,
            0.6,
            2.8,
            0.5,
            2.6,
            2.3,
            0.5,
            1,
            1.7,
            1.5,
            2.9,
            0.6,
            2.9,
            0.9,
            1.6,
            1.2,
            0.5,
            1.8,
            1.6,
            1.2,
            2.1,
            2.7,
            1.7,
            0.9,
            2.3,
            2.1,
            2.4,
            2.3,
            2.8,
            2.9,
            0.5,
            2.7,
            1.8,
            2.1,
            1,
            0.5,
            0.9,
            0.5,
            1.9,
            1.4,
            1.5,
            1.4,
            1.6,
            0.8,
            0.8,
            2.4,
            1.4,
            0.5,
            1.9,
            1.2,
            1.2,
            1.2,
            2.5,
            1,
            2.1,
            2.7,
            1,
            2.4,
            0.5,
            2.5,
            1,
            2.1,
            1.6,
            0.5,
            0.5,
            0.7,
            2.4,
            1.6,
            1.3,
            0.9,
            1.6,
            2,
            1.7,
            0.6,
            2.7,
            0.6,
            0.5,
            2.5,
            0.5,
            2.2,
            2.5,
            2.9,
            0.5,
            2.9,
            1.2,
            0.5,
            1.4,
            0.9,
            1.6,
            2.8,
            1.5,
            2.5,
            2.7,
            2.1,
            0.5,
            0.6,
            2.6,
            1,
            2,
            0.6,
            0.5,
            1.9,
            0.7,
            1.4,
            1.2,
            2.9,
            1,
            1.3,
            1.8,
            1.1,
            0.8,
            3,
            0.5,
            0.8,
            0.5,
            1.1,
            2.2,
            1.2,
            1,
            2.6,
            1.6,
            1.8,
            2.2,
            2.1,
            0.9,
            1,
            2.9,
            2.8,
            2.4,
            2,
            2.9,
            0.5,
            2.2,
            0.5,
            2.6,
            0.9,
            0.9,
            2
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "dual_sim",
           "values": [
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "fc",
           "values": [
            1,
            0,
            2,
            0,
            13,
            3,
            4,
            0,
            0,
            2,
            0,
            5,
            2,
            7,
            13,
            3,
            1,
            7,
            11,
            4,
            12,
            1,
            4,
            4,
            5,
            0,
            7,
            1,
            12,
            3,
            0,
            16,
            3,
            0,
            0,
            16,
            1,
            6,
            16,
            15,
            5,
            8,
            5,
            5,
            1,
            0,
            1,
            3,
            2,
            9,
            2,
            5,
            0,
            1,
            0,
            10,
            13,
            0,
            8,
            2,
            3,
            1,
            0,
            15,
            2,
            13,
            11,
            0,
            0,
            0,
            6,
            1,
            8,
            3,
            4,
            8,
            4,
            1,
            15,
            6,
            0,
            11,
            2,
            11,
            5,
            5,
            4,
            1,
            0,
            0,
            2,
            7,
            0,
            11,
            6,
            18,
            10,
            3,
            0,
            6,
            3,
            4,
            0,
            0,
            1,
            2,
            5,
            3,
            0,
            3,
            0,
            0,
            0,
            13,
            9,
            1,
            0,
            3,
            8,
            0,
            3,
            1,
            7,
            5,
            3,
            5,
            0,
            4,
            11,
            0,
            2,
            9,
            1,
            7,
            1,
            0,
            0,
            5,
            12,
            1,
            0,
            1,
            4,
            0,
            3,
            1,
            0,
            3,
            4,
            6,
            1,
            4,
            3,
            2,
            6,
            4,
            3,
            16,
            0,
            12,
            4,
            10,
            0,
            11,
            11,
            3,
            0,
            2,
            0,
            17,
            2,
            2,
            5,
            0,
            0,
            8,
            2,
            0,
            0,
            2,
            12,
            11,
            0,
            15,
            0,
            1,
            0,
            0,
            3,
            2,
            8,
            1,
            4,
            5,
            0,
            1,
            0,
            0,
            3,
            9,
            0,
            7,
            0,
            10,
            6,
            0,
            16,
            7,
            1,
            0,
            0,
            0,
            9,
            4,
            0,
            3,
            0,
            5,
            0,
            4,
            6,
            5,
            8,
            1,
            3,
            1,
            18,
            0,
            2,
            17,
            1,
            4,
            4,
            0,
            12,
            6,
            3,
            14,
            0,
            0,
            0,
            0,
            0,
            2,
            7,
            2,
            3,
            3,
            4,
            3,
            0,
            0,
            0,
            2,
            9,
            5,
            4,
            1,
            0,
            6,
            7,
            0,
            0,
            4,
            8,
            5,
            10,
            10,
            4,
            0,
            2,
            0,
            7,
            11,
            5,
            3,
            0,
            3,
            12,
            0,
            3,
            0,
            9,
            6,
            7,
            0,
            6,
            0,
            16,
            0,
            8,
            10,
            6,
            6,
            1,
            0,
            7,
            1,
            11,
            8,
            17,
            2,
            16,
            11,
            1,
            18,
            0,
            6,
            11,
            1,
            5,
            0,
            1,
            6,
            0,
            3,
            6,
            6,
            7,
            6,
            0,
            2,
            1,
            14,
            5,
            5,
            0,
            6,
            1,
            5,
            5,
            0,
            0,
            1,
            8,
            0,
            0,
            2,
            2,
            0,
            5,
            5,
            8,
            6,
            2,
            0,
            2,
            6,
            7,
            2,
            15,
            16,
            2,
            7,
            10,
            7,
            0,
            1,
            10,
            1,
            10,
            13,
            4,
            0,
            0,
            8,
            0,
            0,
            8,
            8,
            10,
            4,
            17,
            0,
            5,
            0,
            0,
            1,
            2,
            0,
            10,
            2,
            3,
            0,
            12,
            4,
            12,
            14,
            1,
            1,
            3,
            0,
            15,
            3,
            0,
            8,
            4,
            0,
            10,
            0,
            0,
            16,
            8,
            0,
            6,
            1,
            0,
            9,
            12,
            2,
            5,
            2,
            12,
            0,
            2,
            5,
            1,
            2,
            1,
            2,
            7,
            6,
            10,
            2,
            4,
            3,
            9,
            0,
            3,
            10,
            6,
            3,
            0,
            0,
            4,
            0,
            8,
            1,
            10,
            0,
            6,
            1,
            3,
            13,
            2,
            7,
            5,
            15,
            5,
            1,
            3,
            0,
            2,
            10,
            1,
            6,
            4,
            12,
            0,
            6,
            9,
            12,
            3,
            5,
            3,
            5,
            2,
            7,
            0,
            10,
            1,
            8,
            0,
            1,
            0,
            9,
            0,
            4,
            14,
            2,
            2,
            1,
            14,
            0,
            0,
            0,
            12,
            13,
            0,
            5,
            7,
            5,
            3,
            6,
            0,
            5,
            11,
            13,
            1,
            3,
            1,
            16,
            1,
            0,
            0,
            1,
            2,
            7,
            0,
            6,
            9,
            6,
            11,
            10,
            2,
            2,
            0,
            8,
            3,
            9,
            11,
            9,
            6,
            5,
            0,
            3,
            8,
            6,
            7,
            9,
            3,
            3,
            3,
            3,
            0,
            0,
            8,
            7,
            4,
            5,
            5,
            1,
            2,
            3,
            0,
            16,
            0,
            0,
            0,
            0,
            10,
            1,
            0,
            12,
            6,
            6,
            10,
            0,
            4,
            2,
            9,
            15,
            11,
            0,
            16,
            12,
            4,
            14,
            15,
            0,
            7,
            9,
            13,
            0,
            4,
            9,
            4,
            7,
            1,
            2,
            4,
            8,
            1,
            0,
            17,
            2,
            8,
            0,
            2,
            0,
            6,
            0,
            12,
            0,
            0,
            1,
            10,
            0,
            6,
            4,
            2,
            0,
            0,
            0,
            0,
            5,
            7,
            1,
            6,
            1,
            3,
            0,
            0,
            0,
            9,
            1,
            6,
            1,
            0,
            7,
            1,
            5,
            0,
            7,
            5,
            12,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            5,
            13,
            7,
            2,
            10,
            4,
            0,
            0,
            0,
            0,
            2,
            4,
            6,
            4,
            13,
            1,
            0,
            7,
            10,
            1,
            8,
            7,
            2,
            11,
            3,
            13,
            9,
            1,
            1,
            6,
            6,
            0,
            4,
            4,
            1,
            4,
            0,
            11,
            4,
            0,
            6,
            2,
            8,
            2,
            5,
            10,
            10,
            0,
            0,
            12,
            1,
            13,
            3,
            10,
            0,
            8,
            5,
            2,
            7,
            2,
            1,
            0,
            6,
            0,
            3,
            4,
            0,
            7,
            1,
            1,
            4,
            0,
            2,
            2,
            3,
            2,
            10,
            0,
            1,
            8,
            14,
            0,
            0,
            5,
            5,
            3,
            1,
            12,
            0,
            6,
            2,
            6,
            13,
            0,
            0,
            4,
            9,
            8,
            2,
            13,
            0,
            12,
            3,
            3,
            1,
            13,
            0,
            0,
            0,
            0,
            5,
            6,
            16,
            7,
            1,
            5,
            1,
            2,
            0,
            6,
            5,
            8,
            7,
            0,
            0,
            2,
            3,
            6,
            1,
            0,
            0,
            7,
            5,
            0,
            1,
            4,
            9,
            7,
            16,
            0,
            1,
            0,
            1,
            2,
            9,
            2,
            0,
            15,
            10,
            6,
            11,
            5,
            6,
            7,
            2,
            12,
            8,
            0,
            1,
            3,
            8,
            4,
            0,
            0,
            0,
            2,
            3,
            12,
            15,
            4,
            2,
            0,
            0,
            3,
            5,
            4,
            8,
            4,
            9,
            0,
            10,
            0,
            1,
            0,
            1,
            7,
            2,
            0,
            6,
            0,
            0,
            7,
            8,
            15,
            5,
            6,
            4,
            1,
            1,
            2,
            4,
            0,
            9,
            0,
            15,
            2,
            1,
            5,
            3,
            8,
            3,
            7,
            13,
            8,
            13,
            11,
            10,
            0,
            5,
            2,
            2,
            7,
            6,
            0,
            6,
            9,
            2,
            0,
            11,
            0,
            9,
            3,
            3,
            9,
            3,
            0,
            6,
            7,
            6,
            10,
            0,
            2,
            0,
            1,
            2,
            5,
            7,
            3,
            0,
            1,
            1,
            8,
            6,
            13,
            0,
            8,
            0,
            0,
            4,
            0,
            12,
            5,
            3,
            3,
            4,
            7,
            0,
            0,
            9,
            10,
            3,
            3,
            4,
            6,
            5,
            0,
            0,
            3,
            0,
            2,
            1,
            15,
            4,
            4,
            3,
            0,
            0,
            2,
            0,
            5,
            0,
            15,
            2,
            13,
            15,
            5,
            0,
            0,
            3,
            3,
            11,
            2,
            2,
            1,
            1,
            5,
            9,
            9,
            0,
            3,
            0,
            0,
            0,
            2,
            3,
            2,
            6,
            7,
            0,
            9,
            6,
            0,
            4,
            9,
            1,
            3,
            3,
            7,
            2,
            4,
            4,
            4,
            1,
            0,
            0,
            7,
            7,
            5,
            5,
            6,
            1,
            0,
            0,
            0,
            4,
            4,
            3,
            11,
            0,
            2,
            2,
            0,
            9,
            5,
            0,
            4,
            10,
            13,
            13,
            5,
            11,
            14,
            5,
            2,
            0,
            1,
            3,
            2,
            5,
            12,
            3,
            15,
            0,
            11,
            16,
            2,
            4,
            5,
            7,
            0,
            0,
            3,
            4,
            0,
            1,
            12,
            3,
            0,
            1,
            13,
            7,
            9,
            13,
            1,
            7,
            0,
            0,
            6,
            3,
            8,
            0,
            7,
            1,
            0,
            1,
            0,
            2,
            12,
            2,
            0,
            5,
            3,
            10,
            4,
            1,
            1,
            3,
            0,
            3,
            1,
            1,
            11,
            0,
            1,
            0,
            9,
            0,
            3,
            4,
            16,
            5,
            3,
            7,
            6,
            5,
            4,
            1,
            2,
            1,
            0,
            13,
            0,
            3,
            0,
            8,
            0,
            1,
            3,
            1,
            7,
            11,
            0,
            1,
            0,
            8,
            6,
            9,
            2,
            8,
            0,
            3,
            0,
            1,
            6,
            2,
            4,
            0,
            1,
            13,
            4,
            2,
            4,
            11,
            1,
            0,
            3,
            1,
            5,
            0,
            1,
            0,
            1,
            14,
            2,
            11,
            0,
            11,
            8,
            15,
            4,
            1,
            9,
            4,
            1,
            8,
            13,
            0,
            2,
            2,
            0,
            3,
            2,
            1,
            1,
            1,
            2,
            10,
            3,
            3,
            10,
            10,
            1,
            0,
            5,
            3,
            0,
            0,
            9,
            11,
            2,
            2,
            1,
            0,
            2,
            0,
            10,
            0,
            13,
            0,
            8,
            11,
            8,
            9,
            0,
            0,
            8,
            9,
            12,
            4,
            2,
            0,
            2,
            3,
            12,
            2,
            9,
            0,
            0,
            0,
            11,
            0,
            0,
            0,
            11,
            5,
            7,
            1,
            10,
            12,
            4,
            0,
            2,
            7,
            16,
            0,
            0,
            2,
            1,
            0,
            0,
            6,
            11,
            2,
            0,
            2,
            13,
            2,
            0,
            7,
            10,
            1,
            8,
            3,
            1,
            5,
            5,
            3,
            0,
            0,
            2,
            7,
            0,
            6,
            3,
            2,
            0,
            4,
            5,
            4,
            5,
            0,
            9,
            0,
            5,
            2,
            4,
            0,
            2,
            7,
            11,
            5,
            10,
            0,
            1,
            0,
            0,
            2,
            1,
            2,
            1,
            1,
            6,
            5,
            4,
            0,
            14,
            5,
            0,
            1,
            8,
            1,
            1,
            1,
            13,
            3,
            4,
            1,
            7,
            0,
            1,
            3,
            4,
            4,
            10,
            2,
            2,
            12,
            4,
            5,
            4,
            5,
            2,
            0,
            0,
            1,
            10,
            7,
            6,
            10,
            0,
            7,
            1,
            10,
            0,
            1,
            10,
            2,
            5,
            1,
            5,
            2,
            1,
            5,
            2,
            7,
            0,
            2,
            0,
            3,
            0,
            3,
            5,
            1,
            0,
            13,
            3,
            9,
            6,
            4,
            7,
            6,
            0,
            3,
            2,
            14,
            0,
            15,
            1,
            5,
            3,
            9,
            8,
            5,
            4,
            3,
            7,
            0,
            3,
            12,
            9,
            2,
            8,
            1,
            0,
            5,
            0,
            0,
            8,
            12,
            7,
            0,
            8,
            0,
            7,
            0,
            9,
            2,
            9,
            0,
            1,
            2,
            4,
            0,
            8,
            8,
            4,
            2,
            0,
            3,
            2,
            3,
            1,
            0,
            1,
            0,
            0,
            2,
            0,
            7,
            1,
            12,
            6,
            0,
            9,
            18,
            3,
            6,
            1,
            15,
            4,
            0,
            0,
            8,
            6,
            2,
            0,
            7,
            0,
            6,
            6,
            5,
            3,
            3,
            18,
            1,
            1,
            7,
            0,
            1,
            6,
            6,
            2,
            10,
            18,
            0,
            1,
            2,
            4,
            4,
            1,
            3,
            5,
            1,
            8,
            0,
            11,
            9,
            1,
            0,
            9,
            0,
            0,
            9,
            5,
            1,
            8,
            4,
            2,
            0,
            5,
            0,
            7,
            7,
            6,
            16,
            0,
            9,
            3,
            0,
            1,
            1,
            5,
            0,
            16,
            6,
            10,
            3,
            7,
            1,
            4,
            11,
            0,
            0,
            3,
            4,
            0,
            0,
            0,
            5,
            1,
            0,
            14,
            2,
            6,
            0,
            0,
            2,
            8,
            7,
            0,
            1,
            11,
            0,
            1,
            9,
            0,
            7,
            5,
            9,
            1,
            3,
            0,
            4,
            2,
            2,
            1,
            5,
            6,
            0,
            2,
            7,
            4,
            5,
            4,
            0,
            7,
            0,
            9,
            7,
            1,
            5,
            2,
            0,
            13,
            3,
            1,
            3,
            9,
            6,
            14,
            0,
            0,
            2,
            1,
            5,
            1,
            4,
            5,
            0,
            5,
            0,
            12,
            9,
            9,
            7,
            9,
            1,
            5,
            9,
            9,
            7,
            1,
            2,
            2,
            4,
            0,
            17,
            0,
            2,
            0,
            1,
            18,
            1,
            1,
            0,
            12,
            7,
            5,
            0,
            2,
            1,
            1,
            3,
            6,
            0,
            0,
            6,
            2,
            0,
            6,
            0,
            11,
            2,
            2,
            6,
            0,
            9,
            2,
            0,
            2,
            0,
            6,
            11,
            5,
            1,
            11,
            3,
            1,
            0,
            3,
            0,
            16,
            10,
            1,
            1,
            0,
            7,
            10,
            2,
            4,
            0,
            1,
            8,
            3,
            0,
            9,
            0,
            12,
            3,
            1,
            2,
            1,
            0,
            7,
            5,
            7,
            6,
            0,
            3,
            3,
            1,
            5,
            1,
            6,
            5,
            11,
            12,
            6,
            1,
            0,
            11,
            13,
            1,
            10,
            2,
            3,
            0,
            16,
            6,
            14,
            9,
            1,
            0,
            2,
            0,
            10,
            2,
            4,
            5,
            3,
            0,
            10,
            14,
            4,
            5,
            4,
            6,
            10,
            10,
            1,
            0,
            3,
            16,
            1,
            0,
            14,
            5,
            4,
            5,
            6,
            7,
            3,
            0,
            10,
            4,
            5,
            6,
            5,
            0,
            0,
            3,
            14,
            5,
            13,
            5,
            13,
            5,
            0,
            1,
            6,
            18,
            0,
            4,
            4,
            0,
            3,
            1,
            0,
            2,
            0,
            8,
            8,
            19,
            0,
            16,
            1,
            15,
            0,
            0,
            8,
            2,
            10,
            5,
            0,
            0,
            2,
            0,
            6,
            7,
            3,
            7,
            2,
            2,
            3,
            5,
            0,
            12,
            6,
            6,
            12,
            0,
            4,
            11,
            2,
            4,
            15,
            4,
            5,
            9,
            2,
            2,
            0,
            6,
            3,
            11,
            5,
            0,
            10,
            1,
            5,
            1,
            0,
            1,
            2,
            1,
            5,
            3,
            2,
            5,
            3,
            4,
            3,
            3,
            0,
            15,
            0,
            0,
            8,
            0,
            0,
            3,
            3,
            1,
            6,
            1,
            5,
            0,
            7,
            9,
            3,
            10,
            8,
            3,
            3,
            7,
            16,
            1,
            9,
            0,
            0,
            0,
            2,
            4,
            1,
            6,
            12,
            14,
            0,
            8,
            3,
            2,
            12,
            4,
            5,
            4,
            0,
            7,
            1,
            1,
            0,
            1,
            4,
            1,
            1,
            6,
            1,
            3,
            0,
            13,
            0,
            13,
            3,
            4,
            9,
            4,
            8,
            4,
            9,
            6,
            1,
            4,
            3,
            6,
            0,
            0,
            12,
            2,
            1,
            1,
            2,
            0,
            8,
            3,
            3,
            14,
            3,
            11,
            9,
            13,
            1,
            8,
            1,
            14,
            6,
            1,
            3,
            4,
            3,
            5,
            3,
            3,
            1,
            11,
            7,
            7,
            1,
            7,
            8,
            10,
            0,
            1,
            1,
            5,
            0,
            0,
            0,
            0,
            18,
            8,
            18,
            5,
            3,
            0,
            11,
            0,
            18,
            1,
            3,
            4,
            7,
            1,
            0,
            3,
            1,
            2,
            3,
            9,
            2,
            1,
            9,
            1,
            3,
            7,
            0,
            1,
            2,
            2,
            5,
            0,
            1,
            9,
            3,
            1,
            2,
            0,
            0,
            1,
            1,
            0,
            9,
            8,
            2,
            5,
            0,
            2,
            8,
            0,
            5,
            0,
            1,
            3,
            0,
            6,
            1,
            0,
            7,
            12,
            1,
            4,
            5,
            7,
            2,
            5,
            0,
            0,
            0,
            4,
            1,
            10,
            4,
            0,
            0,
            4,
            2,
            0,
            6,
            3,
            0,
            13,
            9,
            3,
            4,
            2,
            0,
            2,
            0,
            5,
            0,
            0,
            6,
            2,
            9,
            7,
            2,
            1,
            3,
            1,
            4,
            8,
            4,
            12,
            0,
            8,
            6,
            9,
            2,
            2,
            8,
            11,
            1,
            0,
            1,
            0,
            0,
            1,
            4,
            5
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "four_g",
           "values": [
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "int_memory",
           "values": [
            7,
            53,
            41,
            10,
            44,
            22,
            10,
            24,
            53,
            9,
            9,
            33,
            33,
            17,
            52,
            46,
            13,
            23,
            49,
            19,
            39,
            13,
            47,
            38,
            8,
            57,
            51,
            41,
            52,
            21,
            5,
            33,
            41,
            51,
            22,
            60,
            61,
            6,
            11,
            50,
            44,
            41,
            5,
            34,
            20,
            27,
            42,
            40,
            57,
            64,
            14,
            63,
            43,
            16,
            51,
            46,
            60,
            61,
            49,
            48,
            12,
            63,
            50,
            55,
            9,
            44,
            36,
            30,
            10,
            43,
            45,
            22,
            9,
            43,
            38,
            42,
            9,
            33,
            16,
            29,
            58,
            64,
            45,
            61,
            57,
            25,
            42,
            64,
            3,
            33,
            57,
            54,
            15,
            14,
            7,
            7,
            23,
            60,
            37,
            31,
            10,
            12,
            57,
            55,
            8,
            39,
            23,
            42,
            46,
            41,
            43,
            32,
            4,
            7,
            22,
            18,
            53,
            40,
            46,
            15,
            10,
            10,
            52,
            36,
            25,
            16,
            58,
            25,
            31,
            19,
            54,
            2,
            41,
            30,
            36,
            16,
            22,
            46,
            3,
            14,
            31,
            14,
            43,
            33,
            64,
            9,
            31,
            40,
            47,
            41,
            56,
            56,
            61,
            32,
            45,
            46,
            60,
            6,
            29,
            34,
            37,
            51,
            14,
            26,
            3,
            33,
            27,
            10,
            58,
            44,
            32,
            8,
            10,
            42,
            16,
            25,
            29,
            23,
            22,
            63,
            30,
            41,
            38,
            20,
            54,
            4,
            5,
            46,
            6,
            43,
            46,
            7,
            64,
            57,
            20,
            23,
            41,
            61,
            26,
            36,
            22,
            20,
            16,
            30,
            61,
            47,
            8,
            49,
            51,
            35,
            23,
            41,
            63,
            39,
            50,
            53,
            49,
            17,
            24,
            51,
            29,
            40,
            13,
            25,
            38,
            17,
            49,
            31,
            37,
            24,
            39,
            9,
            31,
            27,
            6,
            11,
            42,
            6,
            38,
            49,
            49,
            44,
            51,
            10,
            37,
            50,
            23,
            48,
            7,
            30,
            55,
            63,
            63,
            7,
            19,
            57,
            4,
            32,
            51,
            6,
            59,
            25,
            56,
            28,
            18,
            57,
            40,
            38,
            47,
            28,
            7,
            4,
            45,
            3,
            30,
            62,
            61,
            8,
            2,
            39,
            36,
            40,
            54,
            6,
            56,
            27,
            16,
            24,
            26,
            42,
            63,
            16,
            9,
            48,
            34,
            56,
            12,
            52,
            56,
            38,
            58,
            57,
            56,
            12,
            29,
            52,
            37,
            8,
            42,
            40,
            44,
            41,
            50,
            41,
            49,
            49,
            64,
            14,
            42,
            64,
            5,
            29,
            64,
            63,
            18,
            25,
            21,
            26,
            27,
            13,
            20,
            9,
            20,
            7,
            47,
            18,
            47,
            48,
            36,
            22,
            54,
            2,
            17,
            37,
            6,
            52,
            10,
            3,
            57,
            42,
            44,
            2,
            33,
            8,
            19,
            64,
            17,
            50,
            62,
            61,
            21,
            57,
            32,
            21,
            36,
            2,
            9,
            34,
            46,
            52,
            52,
            36,
            55,
            58,
            42,
            30,
            26,
            14,
            19,
            46,
            8,
            48,
            59,
            54,
            33,
            18,
            44,
            23,
            12,
            63,
            26,
            39,
            23,
            9,
            57,
            7,
            57,
            33,
            28,
            42,
            7,
            50,
            19,
            55,
            13,
            34,
            45,
            17,
            34,
            51,
            5,
            44,
            48,
            49,
            25,
            51,
            54,
            63,
            54,
            4,
            7,
            57,
            43,
            11,
            48,
            56,
            47,
            24,
            20,
            28,
            17,
            64,
            57,
            17,
            23,
            50,
            13,
            36,
            3,
            8,
            57,
            11,
            3,
            26,
            3,
            28,
            64,
            62,
            51,
            51,
            35,
            54,
            34,
            25,
            64,
            48,
            41,
            53,
            28,
            44,
            43,
            6,
            51,
            13,
            40,
            42,
            37,
            4,
            18,
            61,
            48,
            16,
            45,
            60,
            22,
            2,
            61,
            26,
            15,
            58,
            56,
            34,
            3,
            43,
            27,
            41,
            2,
            27,
            20,
            48,
            17,
            31,
            17,
            50,
            15,
            39,
            5,
            31,
            33,
            34,
            54,
            32,
            6,
            32,
            29,
            19,
            23,
            15,
            59,
            44,
            50,
            10,
            7,
            45,
            9,
            2,
            12,
            61,
            57,
            19,
            55,
            46,
            14,
            51,
            23,
            26,
            54,
            27,
            61,
            12,
            38,
            8,
            30,
            17,
            16,
            31,
            37,
            49,
            53,
            51,
            16,
            38,
            39,
            21,
            16,
            19,
            50,
            31,
            7,
            35,
            45,
            41,
            43,
            45,
            21,
            26,
            53,
            32,
            30,
            13,
            61,
            14,
            13,
            3,
            38,
            32,
            30,
            52,
            50,
            34,
            8,
            32,
            46,
            55,
            24,
            20,
            6,
            24,
            23,
            23,
            19,
            18,
            24,
            5,
            5,
            58,
            29,
            40,
            40,
            17,
            35,
            39,
            64,
            55,
            54,
            58,
            25,
            11,
            3,
            11,
            40,
            44,
            12,
            7,
            28,
            56,
            20,
            35,
            16,
            46,
            56,
            4,
            30,
            24,
            21,
            44,
            34,
            39,
            16,
            21,
            5,
            36,
            2,
            30,
            48,
            54,
            30,
            34,
            58,
            35,
            21,
            26,
            19,
            54,
            9,
            15,
            9,
            27,
            43,
            47,
            14,
            3,
            31,
            46,
            3,
            14,
            60,
            25,
            53,
            35,
            64,
            15,
            25,
            35,
            54,
            7,
            14,
            45,
            50,
            50,
            36,
            21,
            46,
            61,
            26,
            33,
            44,
            48,
            11,
            3,
            27,
            62,
            32,
            32,
            45,
            22,
            64,
            13,
            12,
            60,
            12,
            34,
            41,
            27,
            31,
            25,
            60,
            19,
            41,
            57,
            26,
            52,
            38,
            23,
            33,
            57,
            43,
            22,
            17,
            19,
            2,
            62,
            48,
            39,
            20,
            30,
            50,
            49,
            35,
            21,
            25,
            20,
            28,
            2,
            9,
            11,
            6,
            3,
            49,
            37,
            52,
            8,
            61,
            54,
            51,
            45,
            58,
            7,
            21,
            43,
            2,
            49,
            42,
            61,
            13,
            25,
            57,
            42,
            34,
            15,
            35,
            37,
            8,
            18,
            58,
            16,
            43,
            17,
            4,
            45,
            53,
            16,
            5,
            51,
            45,
            63,
            11,
            12,
            34,
            58,
            39,
            49,
            56,
            14,
            15,
            7,
            16,
            27,
            30,
            25,
            64,
            23,
            27,
            50,
            45,
            63,
            18,
            58,
            8,
            11,
            11,
            50,
            50,
            14,
            47,
            56,
            55,
            42,
            2,
            55,
            46,
            24,
            38,
            53,
            25,
            56,
            10,
            41,
            8,
            22,
            20,
            3,
            52,
            6,
            19,
            5,
            21,
            10,
            10,
            44,
            33,
            8,
            25,
            16,
            54,
            52,
            64,
            8,
            48,
            57,
            61,
            30,
            14,
            52,
            25,
            7,
            2,
            33,
            4,
            27,
            43,
            31,
            55,
            9,
            21,
            33,
            63,
            11,
            30,
            48,
            39,
            35,
            34,
            16,
            51,
            55,
            32,
            16,
            38,
            16,
            63,
            58,
            41,
            8,
            10,
            52,
            56,
            21,
            29,
            6,
            15,
            35,
            14,
            35,
            11,
            34,
            15,
            8,
            2,
            32,
            9,
            9,
            4,
            64,
            11,
            13,
            34,
            48,
            53,
            18,
            47,
            21,
            30,
            22,
            39,
            6,
            31,
            50,
            3,
            36,
            8,
            40,
            58,
            61,
            32,
            49,
            49,
            8,
            34,
            12,
            46,
            27,
            26,
            53,
            45,
            17,
            27,
            53,
            45,
            30,
            12,
            10,
            23,
            53,
            36,
            49,
            14,
            35,
            5,
            60,
            30,
            58,
            8,
            63,
            48,
            29,
            38,
            13,
            55,
            13,
            27,
            27,
            44,
            9,
            23,
            7,
            34,
            13,
            54,
            46,
            39,
            33,
            52,
            7,
            8,
            46,
            50,
            42,
            13,
            41,
            13,
            46,
            38,
            54,
            47,
            57,
            6,
            52,
            36,
            33,
            7,
            61,
            7,
            55,
            8,
            19,
            34,
            13,
            46,
            38,
            14,
            52,
            52,
            30,
            54,
            47,
            23,
            58,
            59,
            47,
            34,
            58,
            44,
            19,
            11,
            51,
            20,
            21,
            45,
            11,
            19,
            48,
            27,
            3,
            43,
            49,
            10,
            19,
            29,
            20,
            28,
            57,
            50,
            46,
            53,
            58,
            13,
            34,
            31,
            51,
            19,
            6,
            53,
            53,
            33,
            52,
            54,
            28,
            63,
            64,
            23,
            59,
            61,
            58,
            55,
            49,
            53,
            37,
            4,
            38,
            18,
            43,
            12,
            41,
            57,
            13,
            57,
            60,
            28,
            46,
            14,
            63,
            52,
            18,
            39,
            63,
            5,
            30,
            7,
            22,
            21,
            35,
            37,
            3,
            27,
            19,
            64,
            33,
            18,
            16,
            32,
            28,
            51,
            18,
            17,
            42,
            23,
            23,
            45,
            27,
            41,
            60,
            31,
            8,
            62,
            10,
            39,
            59,
            24,
            23,
            14,
            40,
            33,
            37,
            14,
            55,
            30,
            55,
            4,
            18,
            30,
            58,
            58,
            62,
            2,
            10,
            35,
            35,
            61,
            60,
            14,
            32,
            57,
            12,
            55,
            47,
            18,
            46,
            45,
            43,
            35,
            59,
            57,
            36,
            64,
            22,
            44,
            58,
            63,
            46,
            16,
            35,
            41,
            12,
            9,
            54,
            50,
            16,
            5,
            52,
            31,
            62,
            10,
            55,
            9,
            14,
            8,
            60,
            10,
            55,
            43,
            24,
            56,
            31,
            51,
            2,
            14,
            12,
            32,
            44,
            46,
            35,
            2,
            38,
            20,
            49,
            61,
            50,
            21,
            40,
            28,
            29,
            59,
            16,
            40,
            42,
            27,
            9,
            44,
            51,
            15,
            51,
            57,
            18,
            3,
            47,
            14,
            11,
            30,
            7,
            24,
            2,
            13,
            62,
            40,
            9,
            56,
            62,
            39,
            52,
            11,
            10,
            37,
            27,
            60,
            2,
            34,
            49,
            26,
            58,
            32,
            35,
            45,
            57,
            41,
            22,
            50,
            20,
            51,
            5,
            58,
            2,
            24,
            11,
            32,
            56,
            29,
            57,
            51,
            9,
            24,
            5,
            27,
            37,
            56,
            6,
            35,
            55,
            64,
            16,
            20,
            10,
            36,
            61,
            62,
            40,
            55,
            32,
            4,
            39,
            7,
            35,
            24,
            50,
            59,
            47,
            21,
            20,
            29,
            54,
            41,
            43,
            31,
            49,
            52,
            6,
            13,
            52,
            49,
            7,
            60,
            20,
            27,
            12,
            39,
            13,
            7,
            4,
            19,
            54,
            42,
            31,
            25,
            12,
            46,
            6,
            8,
            37,
            23,
            58,
            12,
            6,
            10,
            15,
            12,
            9,
            19,
            49,
            28,
            58,
            46,
            60,
            31,
            36,
            59,
            56,
            44,
            42,
            2,
            52,
            6,
            19,
            5,
            33,
            58,
            30,
            42,
            26,
            6,
            5,
            44,
            23,
            6,
            27,
            41,
            32,
            59,
            17,
            5,
            17,
            13,
            29,
            20,
            27,
            42,
            47,
            17,
            22,
            39,
            60,
            29,
            16,
            24,
            14,
            47,
            34,
            5,
            36,
            12,
            16,
            58,
            44,
            10,
            42,
            42,
            51,
            18,
            7,
            14,
            14,
            26,
            14,
            2,
            42,
            31,
            63,
            9,
            40,
            57,
            31,
            28,
            27,
            42,
            12,
            39,
            11,
            14,
            62,
            51,
            23,
            15,
            6,
            40,
            52,
            32,
            63,
            24,
            49,
            57,
            62,
            33,
            56,
            50,
            10,
            2,
            29,
            52,
            17,
            63,
            52,
            42,
            14,
            28,
            6,
            30,
            28,
            26,
            14,
            29,
            35,
            13,
            55,
            18,
            27,
            40,
            21,
            60,
            63,
            33,
            54,
            14,
            30,
            15,
            10,
            32,
            15,
            2,
            50,
            44,
            58,
            19,
            6,
            34,
            59,
            6,
            19,
            37,
            9,
            10,
            41,
            44,
            27,
            14,
            24,
            32,
            40,
            18,
            16,
            31,
            22,
            13,
            64,
            53,
            27,
            13,
            5,
            6,
            11,
            44,
            50,
            55,
            12,
            31,
            50,
            22,
            57,
            44,
            33,
            42,
            11,
            31,
            10,
            44,
            28,
            20,
            33,
            40,
            10,
            33,
            40,
            20,
            11,
            63,
            10,
            29,
            5,
            2,
            39,
            44,
            54,
            12,
            40,
            31,
            50,
            9,
            21,
            22,
            3,
            36,
            8,
            12,
            2,
            29,
            48,
            26,
            20,
            40,
            20,
            64,
            43,
            60,
            15,
            21,
            60,
            4,
            8,
            36,
            11,
            23,
            48,
            35,
            3,
            34,
            14,
            27,
            20,
            42,
            14,
            22,
            60,
            24,
            33,
            57,
            38,
            50,
            8,
            60,
            12,
            30,
            55,
            58,
            9,
            59,
            38,
            11,
            6,
            57,
            16,
            19,
            30,
            5,
            18,
            30,
            44,
            40,
            15,
            20,
            27,
            15,
            45,
            39,
            23,
            55,
            18,
            36,
            42,
            43,
            28,
            54,
            2,
            26,
            14,
            50,
            28,
            23,
            39,
            12,
            16,
            14,
            3,
            27,
            34,
            16,
            43,
            56,
            42,
            12,
            49,
            24,
            15,
            54,
            19,
            19,
            22,
            46,
            2,
            26,
            31,
            35,
            11,
            36,
            7,
            62,
            7,
            42,
            30,
            5,
            8,
            53,
            59,
            35,
            27,
            24,
            39,
            5,
            27,
            17,
            33,
            21,
            58,
            44,
            15,
            6,
            37,
            40,
            24,
            39,
            41,
            52,
            48,
            15,
            17,
            41,
            62,
            38,
            58,
            11,
            16,
            56,
            30,
            34,
            20,
            47,
            44,
            28,
            62,
            4,
            47,
            8,
            2,
            41,
            57,
            14,
            5,
            45,
            5,
            59,
            33,
            63,
            7,
            5,
            50,
            11,
            40,
            57,
            62,
            27,
            16,
            42,
            61,
            10,
            13,
            4,
            20,
            61,
            16,
            18,
            46,
            2,
            42,
            54,
            57,
            9,
            25,
            26,
            32,
            23,
            51,
            17,
            45,
            49,
            37,
            7,
            41,
            49,
            62,
            5,
            24,
            35,
            4,
            33,
            17,
            52,
            12,
            53,
            33,
            48,
            26,
            20,
            45,
            36,
            33,
            20,
            27,
            2,
            35,
            54,
            21,
            29,
            42,
            56,
            47,
            14,
            49,
            7,
            7,
            2,
            5,
            28,
            27,
            9,
            30,
            44,
            27,
            8,
            21,
            15,
            26,
            21,
            24,
            20,
            5,
            17,
            19,
            11,
            48,
            5,
            52,
            24,
            12,
            32,
            26,
            21,
            42,
            28,
            59,
            52,
            45,
            57,
            28,
            21,
            50,
            44,
            10,
            43,
            35,
            57,
            26,
            56,
            27,
            44,
            23,
            46,
            40,
            22,
            44,
            16,
            32,
            23,
            63,
            8,
            48,
            64,
            12,
            7,
            30,
            19,
            16,
            11,
            16,
            47,
            64,
            23,
            29,
            28,
            26,
            34,
            47,
            23,
            51,
            54,
            11,
            15,
            64,
            47,
            53,
            64,
            8,
            12,
            20,
            60,
            11,
            16,
            37,
            47,
            2,
            60,
            53,
            58,
            30,
            27,
            55,
            38,
            6,
            63,
            16,
            19,
            5,
            51,
            58,
            31,
            34,
            17,
            53,
            26,
            57,
            56,
            2,
            5,
            48,
            60,
            26,
            17,
            41,
            21,
            37,
            12,
            45,
            14,
            24,
            42,
            52,
            44,
            35,
            14,
            6,
            7,
            16,
            62,
            61,
            59,
            49,
            60,
            2,
            30,
            19,
            14,
            60,
            9,
            19,
            25,
            7,
            29,
            19,
            54,
            53,
            44,
            20,
            22,
            24,
            17,
            55,
            3,
            27,
            39,
            5,
            53,
            2,
            64,
            53,
            2,
            27,
            16,
            10,
            28,
            42,
            2,
            52,
            12,
            8,
            15,
            56,
            32,
            15,
            62,
            46,
            30,
            11,
            63,
            36,
            21,
            13,
            9,
            47,
            45,
            18,
            38,
            11,
            45,
            2,
            35,
            13,
            41,
            63,
            39,
            18,
            62,
            10,
            23,
            45,
            16,
            41,
            45,
            37,
            12,
            14,
            40,
            63,
            33,
            35,
            32,
            33,
            30,
            2,
            8,
            21,
            40,
            37,
            49,
            4,
            16,
            21,
            7,
            6,
            7,
            21,
            32,
            6,
            25,
            64,
            27,
            23,
            18,
            64,
            36,
            24,
            17,
            5,
            45,
            24,
            27,
            61,
            16,
            55,
            40,
            3,
            14,
            42,
            21,
            6,
            29,
            25,
            46,
            9,
            33,
            27,
            10,
            18,
            53,
            9,
            4,
            6,
            41,
            34,
            37,
            59,
            58,
            13,
            57,
            15,
            36,
            44,
            21,
            18,
            50,
            2,
            39,
            36,
            46,
            45
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "m_dep",
           "values": [
            0.6,
            0.7,
            0.9,
            0.8,
            0.6,
            0.7,
            0.8,
            0.8,
            0.7,
            0.1,
            0.1,
            0.5,
            0.6,
            1,
            0.7,
            0.7,
            0.1,
            0.1,
            0.6,
            1,
            0.8,
            1,
            0.3,
            0.7,
            0.4,
            0.6,
            0.3,
            1,
            0.3,
            0.4,
            0.2,
            1,
            0.9,
            0.6,
            0.7,
            0.4,
            0.6,
            0.5,
            0.3,
            0.3,
            0.6,
            0.2,
            0.2,
            0.1,
            0.8,
            0.2,
            1,
            0.8,
            0.2,
            0.8,
            0.7,
            0.8,
            0.3,
            0.3,
            0.5,
            0.6,
            0.5,
            0.4,
            0.5,
            1,
            0.6,
            0.8,
            0.8,
            0.5,
            0.7,
            0.8,
            0.3,
            0.5,
            0.6,
            0.8,
            0.8,
            0.7,
            0.1,
            0.7,
            0.5,
            0.9,
            0.4,
            0.6,
            0.2,
            0.7,
            0.9,
            0.2,
            0.9,
            1,
            0.8,
            0.8,
            0.5,
            0.8,
            0.1,
            0.5,
            0.5,
            1,
            0.4,
            0.7,
            0.8,
            1,
            0.2,
            0.2,
            0.7,
            0.4,
            0.3,
            0.8,
            0.7,
            0.6,
            0.5,
            0.7,
            1,
            0.1,
            0.3,
            0.1,
            1,
            0.5,
            0.7,
            1,
            0.6,
            0.2,
            0.3,
            0.9,
            0.9,
            0.5,
            0.3,
            0.5,
            0.9,
            0.8,
            0.8,
            0.6,
            0.3,
            0.5,
            0.6,
            0.1,
            0.8,
            0.1,
            1,
            0.2,
            0.7,
            0.5,
            0.9,
            0.5,
            0.3,
            0.1,
            0.1,
            0.5,
            0.3,
            0.5,
            0.9,
            0.4,
            0.2,
            0.5,
            0.4,
            0.8,
            0.1,
            0.2,
            0.9,
            0.7,
            0.7,
            0.9,
            0.8,
            0.4,
            0.5,
            0.7,
            0.6,
            0.3,
            0.5,
            0.5,
            0.1,
            0.8,
            0.9,
            0.4,
            0.2,
            0.3,
            0.5,
            0.1,
            0.2,
            0.3,
            1,
            0.8,
            0.8,
            0.1,
            0.1,
            0.4,
            0.3,
            0.2,
            0.1,
            1,
            0.2,
            0.8,
            0.1,
            0.1,
            0.5,
            0.9,
            0.6,
            0.2,
            0.3,
            0.2,
            0.1,
            0.2,
            0.8,
            0.7,
            0.3,
            0.4,
            0.6,
            0.4,
            0.3,
            0.7,
            0.1,
            0.3,
            0.3,
            0.7,
            0.1,
            0.4,
            0.1,
            0.1,
            0.5,
            0.5,
            0.4,
            1,
            0.2,
            0.2,
            0.4,
            0.1,
            0.5,
            0.5,
            0.5,
            0.9,
            0.1,
            0.2,
            0.1,
            0.6,
            0.1,
            0.3,
            0.8,
            0.4,
            0.5,
            0.1,
            0.8,
            0.1,
            0.1,
            0.1,
            1,
            0.4,
            0.1,
            0.5,
            0.3,
            0.6,
            0.4,
            0.2,
            0.4,
            0.4,
            0.8,
            0.7,
            0.6,
            0.1,
            1,
            0.2,
            0.6,
            0.2,
            0.1,
            0.5,
            0.5,
            0.5,
            0.7,
            0.2,
            0.6,
            0.7,
            0.8,
            1,
            0.3,
            0.4,
            0.9,
            0.3,
            0.9,
            0.1,
            0.3,
            0.6,
            0.9,
            0.5,
            0.8,
            0.3,
            0.8,
            0.4,
            1,
            0.9,
            0.5,
            0.5,
            0.1,
            0.7,
            0.3,
            0.2,
            0.8,
            0.6,
            0.2,
            0.1,
            0.8,
            0.6,
            0.3,
            0.7,
            0.3,
            0.9,
            0.7,
            0.1,
            0.6,
            0.6,
            0.1,
            0.9,
            0.4,
            0.3,
            0.5,
            0.6,
            0.8,
            0.5,
            0.5,
            0.8,
            0.5,
            0.7,
            0.6,
            0.9,
            0.8,
            0.4,
            0.3,
            0.2,
            0.6,
            0.8,
            0.9,
            1,
            0.9,
            0.4,
            0.4,
            0.2,
            0.5,
            0.4,
            0.8,
            0.1,
            0.1,
            0.6,
            0.2,
            0.3,
            0.9,
            0.3,
            0.8,
            0.8,
            0.1,
            0.3,
            0.1,
            0.7,
            0.3,
            0.7,
            0.2,
            0.7,
            0.6,
            0.4,
            0.3,
            0.1,
            1,
            0.5,
            0.8,
            0.9,
            0.9,
            0.5,
            0.3,
            0.3,
            0.9,
            0.9,
            0.8,
            0.8,
            0.3,
            0.5,
            0.4,
            0.8,
            0.7,
            1,
            0.1,
            0.2,
            0.7,
            0.5,
            0.6,
            0.4,
            0.1,
            0.4,
            0.4,
            0.4,
            0.9,
            0.2,
            0.1,
            0.7,
            0.2,
            0.7,
            0.2,
            1,
            0.6,
            0.5,
            0.1,
            0.2,
            0.6,
            0.9,
            0.3,
            0.4,
            0.9,
            0.7,
            0.2,
            0.2,
            0.2,
            0.8,
            0.6,
            0.1,
            0.2,
            0.7,
            0.1,
            0.7,
            0.7,
            0.6,
            0.2,
            0.5,
            0.7,
            0.6,
            0.8,
            0.3,
            0.3,
            0.2,
            0.2,
            0.8,
            0.4,
            0.3,
            0.2,
            0.4,
            0.1,
            0.4,
            0.3,
            0.1,
            0.5,
            0.8,
            0.2,
            0.7,
            0.4,
            0.7,
            0.8,
            0.3,
            0.8,
            0.5,
            0.6,
            0.7,
            0.9,
            0.3,
            0.3,
            1,
            0.1,
            0.4,
            0.9,
            0.3,
            0.8,
            0.7,
            0.2,
            0.7,
            0.8,
            1,
            0.7,
            0.7,
            0.1,
            0.4,
            0.9,
            0.1,
            0.4,
            0.7,
            0.1,
            1,
            0.6,
            0.8,
            0.9,
            0.7,
            0.1,
            0.3,
            0.7,
            0.1,
            1,
            0.7,
            0.8,
            0.6,
            0.8,
            0.3,
            0.1,
            0.1,
            0.9,
            0.1,
            0.3,
            0.7,
            0.2,
            0.3,
            0.7,
            0.3,
            0.8,
            0.6,
            0.3,
            1,
            0.2,
            0.8,
            0.1,
            0.6,
            1,
            0.8,
            0.4,
            0.1,
            0.1,
            0.6,
            1,
            0.6,
            0.3,
            0.7,
            0.2,
            0.9,
            0.7,
            0.7,
            0.6,
            0.9,
            0.5,
            0.9,
            0.9,
            0.6,
            0.8,
            0.1,
            0.1,
            1,
            0.7,
            0.9,
            0.5,
            0.3,
            0.1,
            0.1,
            0.5,
            0.1,
            0.1,
            0.3,
            0.7,
            0.1,
            0.9,
            0.5,
            1,
            0.4,
            0.9,
            0.6,
            0.2,
            0.5,
            0.1,
            0.6,
            0.1,
            1,
            0.1,
            0.2,
            0.8,
            0.2,
            0.9,
            0.5,
            0.9,
            0.5,
            0.1,
            0.1,
            0.2,
            0.8,
            0.3,
            0.7,
            0.3,
            0.9,
            0.1,
            0.8,
            0.6,
            0.1,
            0.1,
            0.1,
            0.4,
            0.8,
            0.1,
            0.1,
            0.8,
            0.1,
            0.9,
            0.9,
            0.3,
            0.1,
            0.7,
            0.8,
            0.1,
            0.4,
            0.9,
            0.1,
            0.3,
            0.2,
            0.1,
            0.1,
            0.2,
            0.6,
            0.9,
            0.7,
            0.6,
            0.7,
            0.3,
            0.6,
            0.1,
            0.6,
            0.1,
            0.1,
            0.9,
            0.1,
            0.4,
            0.7,
            0.3,
            0.8,
            0.8,
            0.3,
            0.1,
            0.9,
            0.9,
            0.3,
            0.1,
            0.2,
            0.2,
            0.5,
            0.1,
            0.5,
            0.5,
            0.4,
            0.1,
            0.7,
            1,
            0.1,
            0.6,
            0.5,
            0.8,
            0.3,
            0.2,
            0.2,
            0.5,
            0.6,
            0.7,
            0.8,
            0.2,
            0.8,
            0.5,
            0.5,
            0.3,
            0.3,
            0.9,
            0.3,
            0.8,
            0.1,
            0.9,
            0.8,
            0.1,
            0.6,
            0.7,
            0.9,
            0.6,
            0.1,
            0.8,
            0.3,
            0.2,
            0.2,
            0.4,
            0.5,
            0.7,
            1,
            0.2,
            0.2,
            0.3,
            0.5,
            0.7,
            0.9,
            0.7,
            0.5,
            0.5,
            0.8,
            0.7,
            0.2,
            0.5,
            0.7,
            0.2,
            0.8,
            0.3,
            0.1,
            0.9,
            1,
            0.5,
            0.9,
            0.9,
            0.3,
            0.9,
            0.9,
            0.1,
            0.8,
            0.1,
            0.9,
            0.1,
            0.9,
            0.1,
            0.1,
            0.4,
            0.8,
            0.4,
            0.3,
            0.4,
            0.5,
            0.7,
            1,
            0.1,
            0.7,
            0.7,
            0.1,
            0.6,
            0.7,
            0.7,
            0.4,
            0.6,
            0.8,
            0.6,
            0.5,
            0.4,
            0.3,
            0.6,
            0.5,
            0.7,
            0.2,
            0.7,
            0.9,
            0.5,
            0.1,
            0.1,
            0.1,
            0.3,
            0.3,
            0.2,
            0.3,
            0.7,
            0.4,
            0.3,
            0.8,
            0.8,
            0.6,
            0.3,
            1,
            0.6,
            0.6,
            0.2,
            0.1,
            0.2,
            0.7,
            0.2,
            0.5,
            0.8,
            0.7,
            0.8,
            0.9,
            0.2,
            0.8,
            0.3,
            0.5,
            0.9,
            0.4,
            0.8,
            0.1,
            0.3,
            0.1,
            0.2,
            0.2,
            0.3,
            0.8,
            0.6,
            0.7,
            0.4,
            0.2,
            0.5,
            0.3,
            0.2,
            0.3,
            0.6,
            0.1,
            0.1,
            0.8,
            0.3,
            0.7,
            0.9,
            0.7,
            0.4,
            0.2,
            0.6,
            0.2,
            0.1,
            0.2,
            0.4,
            0.6,
            0.9,
            0.7,
            0.2,
            0.3,
            0.1,
            0.7,
            0.3,
            0.8,
            0.6,
            0.7,
            0.3,
            0.6,
            0.4,
            0.8,
            0.8,
            0.1,
            1,
            0.6,
            1,
            0.4,
            0.2,
            0.6,
            0.7,
            0.7,
            0.9,
            0.3,
            0.3,
            0.9,
            0.3,
            0.1,
            0.1,
            0.1,
            0.2,
            0.1,
            0.5,
            0.5,
            0.4,
            0.7,
            0.8,
            0.9,
            0.3,
            0.9,
            1,
            0.9,
            0.5,
            0.5,
            0.7,
            0.1,
            0.6,
            1,
            0.7,
            0.6,
            0.5,
            0.6,
            0.8,
            0.8,
            0.5,
            0.6,
            0.4,
            0.9,
            0.6,
            0.7,
            0.4,
            0.1,
            0.8,
            0.2,
            0.8,
            0.9,
            0.5,
            0.1,
            0.5,
            0.8,
            0.6,
            0.1,
            0.4,
            0.8,
            0.7,
            0.7,
            0.2,
            0.8,
            0.5,
            0.1,
            0.1,
            0.3,
            0.1,
            0.8,
            0.8,
            0.2,
            0.5,
            0.1,
            0.2,
            0.5,
            0.2,
            0.1,
            0.7,
            0.2,
            0.9,
            0.3,
            0.1,
            0.9,
            0.5,
            0.7,
            0.9,
            0.8,
            0.1,
            1,
            0.3,
            0.6,
            0.7,
            0.5,
            0.8,
            0.4,
            0.4,
            0.9,
            0.4,
            0.3,
            0.4,
            0.3,
            0.2,
            0.2,
            0.6,
            0.3,
            0.4,
            0.8,
            0.6,
            0.9,
            0.5,
            0.2,
            0.4,
            0.5,
            0.9,
            0.4,
            0.5,
            0.4,
            0.7,
            1,
            0.5,
            0.8,
            0.2,
            1,
            0.1,
            0.4,
            0.1,
            0.9,
            0.1,
            0.2,
            0.1,
            0.7,
            0.5,
            0.9,
            0.6,
            1,
            0.2,
            0.4,
            0.1,
            0.3,
            0.1,
            0.1,
            0.2,
            0.3,
            0.1,
            0.2,
            0.5,
            0.6,
            0.3,
            0.7,
            0.4,
            0.4,
            1,
            0.1,
            0.1,
            1,
            1,
            0.2,
            0.7,
            0.1,
            0.8,
            0.5,
            1,
            0.3,
            0.6,
            0.5,
            0.5,
            0.1,
            0.2,
            0.9,
            0.8,
            0.9,
            0.9,
            0.4,
            0.5,
            0.1,
            0.3,
            0.8,
            0.4,
            0.2,
            0.5,
            0.3,
            0.2,
            0.5,
            0.9,
            0.2,
            0.4,
            0.4,
            0.6,
            0.5,
            0.1,
            0.3,
            0.9,
            0.2,
            0.5,
            0.9,
            0.9,
            0.6,
            0.1,
            0.5,
            0.1,
            0.3,
            1,
            0.5,
            0.3,
            0.1,
            0.4,
            0.3,
            0.9,
            0.1,
            0.7,
            1,
            0.9,
            0.5,
            0.1,
            0.5,
            0.7,
            0.2,
            0.4,
            0.5,
            0.3,
            0.2,
            0.1,
            1,
            0.2,
            0.2,
            0.1,
            0.3,
            0.3,
            1,
            0.9,
            0.1,
            0.3,
            0.9,
            0.8,
            0.4,
            0.6,
            0.9,
            0.7,
            0.6,
            0.2,
            0.2,
            0.8,
            0.7,
            0.5,
            0.8,
            0.2,
            0.1,
            0.8,
            0.3,
            0.4,
            0.1,
            0.4,
            0.1,
            1,
            0.9,
            0.2,
            0.8,
            0.1,
            0.2,
            0.6,
            0.7,
            0.7,
            0.1,
            0.7,
            0.2,
            0.5,
            0.6,
            0.1,
            0.8,
            0.6,
            0.5,
            0.6,
            0.2,
            0.4,
            0.9,
            0.9,
            0.1,
            0.2,
            0.9,
            0.6,
            0.7,
            0.7,
            0.1,
            0.4,
            0.9,
            0.1,
            0.9,
            0.7,
            0.8,
            0.1,
            0.9,
            1,
            0.1,
            0.8,
            0.1,
            0.9,
            0.1,
            0.3,
            0.7,
            0.1,
            0.1,
            0.7,
            0.7,
            0.4,
            0.1,
            0.8,
            0.8,
            0.8,
            0.7,
            0.3,
            0.4,
            0.2,
            0.5,
            0.2,
            0.3,
            0.9,
            0.8,
            0.6,
            0.1,
            0.4,
            0.9,
            0.1,
            0.7,
            0.3,
            0.2,
            0.9,
            0.1,
            0.9,
            0.7,
            0.6,
            0.8,
            0.2,
            0.5,
            0.7,
            0.1,
            0.2,
            0.5,
            0.6,
            1,
            0.4,
            0.8,
            1,
            0.6,
            0.9,
            0.9,
            1,
            0.1,
            0.8,
            0.2,
            0.6,
            0.3,
            0.8,
            0.5,
            0.3,
            0.9,
            0.1,
            0.5,
            0.2,
            0.6,
            0.1,
            0.8,
            0.7,
            0.9,
            0.1,
            0.3,
            0.2,
            0.3,
            0.1,
            0.3,
            0.2,
            0.7,
            0.4,
            0.3,
            0.5,
            0.1,
            0.4,
            0.6,
            0.1,
            0.9,
            0.1,
            0.7,
            0.8,
            0.9,
            0.3,
            0.1,
            0.6,
            0.7,
            0.5,
            0.1,
            0.7,
            0.9,
            0.8,
            0.6,
            0.1,
            0.5,
            0.3,
            0.1,
            0.3,
            0.5,
            0.6,
            0.7,
            1,
            0.3,
            0.1,
            0.8,
            0.4,
            0.7,
            0.9,
            0.4,
            0.7,
            1,
            0.7,
            0.2,
            0.7,
            0.6,
            0.9,
            0.8,
            0.4,
            0.4,
            0.1,
            0.1,
            0.9,
            0.4,
            0.7,
            0.4,
            0.6,
            0.1,
            0.4,
            0.6,
            0.3,
            0.3,
            0.3,
            0.3,
            0.5,
            0.1,
            0.5,
            0.1,
            0.6,
            0.4,
            0.6,
            1,
            0.8,
            0.8,
            0.3,
            0.5,
            0.6,
            0.4,
            0.3,
            0.5,
            0.5,
            0.6,
            0.9,
            0.1,
            0.7,
            0.6,
            0.2,
            0.1,
            0.8,
            0.8,
            0.9,
            0.2,
            0.2,
            0.7,
            0.7,
            0.3,
            0.9,
            0.4,
            0.9,
            0.8,
            0.8,
            0.2,
            0.6,
            0.9,
            0.5,
            0.2,
            0.1,
            0.4,
            0.7,
            0.2,
            0.1,
            0.3,
            0.3,
            0.8,
            0.3,
            0.7,
            0.2,
            0.5,
            0.4,
            0.6,
            0.3,
            0.1,
            0.7,
            0.3,
            1,
            0.1,
            0.1,
            0.1,
            0.9,
            0.8,
            0.8,
            0.4,
            0.8,
            0.2,
            0.6,
            0.7,
            0.5,
            0.1,
            0.7,
            0.8,
            0.5,
            0.1,
            0.6,
            0.7,
            0.9,
            0.1,
            0.2,
            0.8,
            0.2,
            0.5,
            0.8,
            0.5,
            0.4,
            0.9,
            0.1,
            0.1,
            0.7,
            0.1,
            0.5,
            0.2,
            0.2,
            0.4,
            0.2,
            0.9,
            0.6,
            0.4,
            0.1,
            0.1,
            0.5,
            0.8,
            0.8,
            0.8,
            0.9,
            0.1,
            0.3,
            0.8,
            0.1,
            0.4,
            0.8,
            0.6,
            0.8,
            0.9,
            0.6,
            0.4,
            0.7,
            0.1,
            0.5,
            0.1,
            0.2,
            0.4,
            0.7,
            0.5,
            0.1,
            0.8,
            0.9,
            0.1,
            0.4,
            0.2,
            0.8,
            0.8,
            0.1,
            0.1,
            0.4,
            0.6,
            0.5,
            0.3,
            0.5,
            0.7,
            0.2,
            0.4,
            0.1,
            0.2,
            0.9,
            0.2,
            0.4,
            0.3,
            0.3,
            0.9,
            0.6,
            0.6,
            0.8,
            0.9,
            1,
            0.2,
            0.2,
            0.1,
            0.1,
            0.1,
            1,
            0.1,
            0.3,
            0.3,
            0.7,
            0.8,
            0.5,
            0.7,
            0.5,
            1,
            0.6,
            0.3,
            0.6,
            0.4,
            0.1,
            0.4,
            0.2,
            0.7,
            0.6,
            0.3,
            0.6,
            0.1,
            0.1,
            0.7,
            0.9,
            0.6,
            0.8,
            0.4,
            0.6,
            0.4,
            0.5,
            0.2,
            0.5,
            0.7,
            1,
            0.2,
            0.7,
            0.5,
            0.3,
            0.8,
            0.4,
            0.3,
            0.5,
            0.1,
            0.8,
            0.3,
            0.7,
            0.4,
            0.7,
            0.9,
            0.8,
            0.5,
            0.6,
            0.1,
            0.6,
            0.1,
            0.9,
            0.7,
            0.2,
            0.4,
            0.4,
            0.4,
            0.9,
            0.7,
            1,
            0.1,
            0.1,
            0.1,
            0.5,
            0.8,
            0.6,
            0.1,
            0.9,
            0.4,
            0.9,
            0.2,
            0.2,
            0.4,
            0.2,
            0.1,
            0.7,
            0.1,
            0.8,
            0.1,
            0.5,
            0.7,
            1,
            0.8,
            0.4,
            0.6,
            0.5,
            0.7,
            0.7,
            0.1,
            0.5,
            0.3,
            0.9,
            0.4,
            0.5,
            0.9,
            0.1,
            1,
            0.4,
            0.1,
            0.5,
            0.8,
            0.3,
            0.2,
            0.1,
            0.5,
            0.2,
            0.9,
            0.4,
            0.9,
            0.2,
            0.4,
            0.1,
            0.7,
            0.9,
            0.8,
            0.1,
            0.6,
            0.6,
            0.7,
            0.5,
            0.8,
            0.8,
            0.9,
            0.1,
            1,
            0.6,
            0.6,
            0.2,
            0.8,
            0.8,
            0.8,
            0.2,
            0.4,
            0.8,
            0.7,
            0.4,
            0.2,
            1,
            0.1,
            0.5,
            0.7,
            0.9,
            0.9,
            0.3,
            0.5,
            1,
            1,
            0.1,
            0.1,
            0.8,
            0.9,
            0.6,
            0.3,
            0.2,
            0.1,
            0.5,
            0.7,
            0.1,
            0.5,
            0.6,
            0.9,
            0.1,
            0.7,
            0.6,
            0.9,
            0.7,
            0.9,
            0.3,
            0.3,
            0.1,
            0.8,
            0.3,
            0.6,
            0.1,
            0.6,
            0.3,
            0.1,
            0.3,
            0.7,
            0.1,
            0.1,
            1,
            0.6,
            0.1,
            0.7,
            0.9,
            0.9,
            0.7,
            0.1,
            0.3,
            0.6,
            0.1,
            0.1,
            0.7,
            0.5,
            0.1,
            0.2,
            0.2,
            0.5,
            0.9,
            0.4,
            0.2,
            0.5,
            0.6,
            0.3,
            0.7,
            0.2,
            0.5,
            0.2,
            0.2,
            0.1,
            0.1,
            0.2,
            1,
            0.7,
            0.2,
            0.1,
            0.1,
            1,
            0.8,
            1,
            0.3,
            0.1,
            1,
            0.9,
            0.2,
            0.8,
            0.5,
            0.5,
            0.8,
            0.8,
            0.5,
            0.5,
            0.8,
            0.8,
            1,
            0.9,
            0.1,
            0.7,
            0.6,
            0.2,
            0.5,
            0.1,
            1,
            0.6,
            0.2,
            0.3,
            0.8,
            0.2,
            0.3,
            0.5,
            1,
            1,
            1,
            0.2,
            0.3,
            0.2,
            0.6,
            0.1,
            0.2,
            0.6,
            0.1,
            0.6,
            0.9,
            0.8,
            0.9,
            0.4,
            0.8,
            0.3,
            0.7,
            0.9,
            0.4,
            0.1,
            0.9,
            0.1,
            0.6,
            0.2,
            0.1,
            0.7,
            0.3,
            0.3,
            1,
            0.2,
            0.4,
            0.3,
            0.9,
            0.5,
            0.4,
            0.6,
            0.5,
            0.4,
            0.5,
            0.3,
            0.9,
            0.7,
            0.7,
            0.4,
            0.6,
            0.5,
            0.1,
            0.3,
            0.9,
            0.1,
            0.5,
            0.6,
            0.4,
            0.2,
            0.6,
            0.8,
            0.8,
            0.2,
            0.6,
            0.7,
            0.6,
            0.3,
            0.4,
            1,
            0.2,
            0.1,
            0.9,
            0.1,
            0.9,
            0.1,
            0.1,
            0.1,
            1,
            0.5,
            0.9,
            0.9,
            0.8,
            0.1,
            0.8,
            0.5,
            0.6,
            0.7,
            0.6,
            0.1,
            0.1,
            0.5,
            0.5,
            0.8,
            0.3,
            0.2,
            0.2,
            0.8,
            0.5,
            0.7,
            0.5,
            0.1,
            0.2,
            0.1,
            0.1,
            0.5,
            0.2,
            0.7,
            0.1,
            0.6,
            0.5,
            0.9,
            0.2,
            0.3,
            0.4,
            0.5,
            0.4,
            0.3,
            0.3,
            0.7,
            0.1,
            0.6,
            1,
            0.9,
            0.4,
            0.9,
            0.9,
            0.2,
            0.1,
            1,
            0.3,
            0.7,
            0.9,
            0.5,
            0.5,
            0.3,
            0.6,
            0.9,
            0.4,
            0.9,
            0.2,
            0.5,
            0.4,
            0.9,
            0.4,
            0.6,
            1,
            0.4,
            0.6,
            1,
            0.9,
            0.4,
            0.6,
            0.2,
            0.2,
            0.8,
            0.3,
            0.1,
            0.1,
            0.3,
            0.1,
            0.8,
            0.7,
            0.6,
            0.4,
            0.5,
            0.2,
            0.1,
            0.1,
            0.4,
            0.1,
            0.5,
            0.1,
            1,
            0.2,
            0.9,
            0.7,
            0.3,
            0.4,
            0.3,
            0.4,
            0.1,
            0.8,
            0.7,
            0.3,
            0.1,
            0.8,
            0.8,
            0.7,
            0.9,
            0.6,
            0.8,
            0.5,
            0.2,
            0.8,
            0.6,
            0.3,
            0.1,
            0.2,
            0.6,
            0.1,
            0.6,
            0.9,
            0.7,
            1,
            0.9,
            0.9,
            0.8,
            0.6,
            0.5,
            0.2,
            0.5,
            0.7,
            0.5,
            0.2,
            1,
            0.8,
            0.8,
            0.9,
            0.5,
            0.9,
            0.4,
            1,
            0.4,
            0.3,
            0.6,
            0.9,
            0.9,
            0.8,
            0.5,
            0.5,
            0.1,
            0.6,
            0.1,
            1,
            0.1,
            0.1,
            0.6,
            0.1,
            0.8,
            0.3,
            0.2,
            0.7,
            0.9,
            0.6,
            0.2,
            1,
            0.9,
            0.4,
            0.9,
            0.7,
            1,
            0.8,
            1,
            0.2,
            0.1,
            0.5,
            0.6,
            0.6,
            0.5,
            0.4,
            0.2,
            0.3,
            0.2,
            0.5,
            0.6,
            0.6,
            0.1,
            0.9,
            0.3,
            0.5,
            0.7,
            0.2,
            0.8,
            0.3,
            0.7,
            1,
            0.1,
            0.4,
            0.4,
            0.4,
            0.6,
            0.3,
            0.5,
            0.2,
            0.7,
            0.8,
            0.3,
            0.3,
            0.7,
            0.5,
            0.7,
            0.2,
            0.8,
            0.4,
            0.2,
            1,
            0.5,
            0.8,
            0.5,
            0.6,
            0.2,
            0.1,
            0.9,
            0.2,
            0.8,
            1,
            0.5,
            0.2,
            0.1,
            1,
            0.2,
            0.6,
            0.1,
            0.2,
            0.7,
            0.9,
            0.6,
            0.6,
            0.3,
            0.8,
            0.1,
            0.3,
            0.1,
            1,
            0.2,
            0.7,
            0.1,
            0.9,
            0.4,
            0.4,
            0.1,
            0.9,
            0.1,
            0.5,
            0.1,
            0.4,
            0.2,
            0.8,
            0.8,
            0.2,
            0.6,
            0.1,
            0.8,
            0.2,
            0.7,
            0.1,
            0.9
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "mobile_wt",
           "values": [
            188,
            136,
            145,
            131,
            141,
            164,
            139,
            187,
            174,
            93,
            182,
            177,
            159,
            198,
            185,
            159,
            196,
            121,
            101,
            121,
            81,
            156,
            199,
            114,
            111,
            114,
            132,
            143,
            96,
            200,
            88,
            150,
            107,
            100,
            157,
            160,
            160,
            119,
            87,
            159,
            132,
            185,
            152,
            166,
            110,
            118,
            164,
            196,
            162,
            111,
            198,
            127,
            109,
            102,
            145,
            104,
            148,
            107,
            180,
            128,
            134,
            144,
            159,
            168,
            141,
            81,
            155,
            182,
            165,
            80,
            138,
            104,
            142,
            141,
            90,
            188,
            197,
            172,
            116,
            111,
            85,
            199,
            180,
            114,
            163,
            96,
            182,
            178,
            155,
            100,
            177,
            171,
            103,
            83,
            140,
            196,
            132,
            194,
            141,
            156,
            146,
            119,
            192,
            199,
            121,
            103,
            172,
            198,
            83,
            163,
            106,
            199,
            135,
            194,
            153,
            89,
            82,
            107,
            199,
            102,
            194,
            80,
            130,
            200,
            198,
            89,
            88,
            189,
            181,
            100,
            99,
            184,
            197,
            195,
            89,
            108,
            133,
            179,
            182,
            155,
            134,
            147,
            137,
            141,
            134,
            141,
            148,
            141,
            199,
            159,
            147,
            190,
            176,
            84,
            166,
            139,
            97,
            109,
            198,
            142,
            96,
            152,
            124,
            166,
            118,
            183,
            150,
            160,
            157,
            110,
            101,
            101,
            183,
            197,
            189,
            143,
            93,
            101,
            197,
            165,
            113,
            178,
            198,
            84,
            138,
            92,
            95,
            103,
            151,
            150,
            165,
            190,
            150,
            117,
            163,
            117,
            94,
            106,
            93,
            119,
            192,
            173,
            166,
            105,
            168,
            115,
            171,
            182,
            110,
            142,
            111,
            159,
            135,
            128,
            91,
            181,
            181,
            194,
            112,
            88,
            133,
            111,
            116,
            123,
            166,
            200,
            109,
            128,
            138,
            127,
            112,
            105,
            83,
            192,
            142,
            129,
            199,
            183,
            183,
            107,
            139,
            187,
            157,
            110,
            129,
            127,
            165,
            99,
            107,
            105,
            183,
            93,
            195,
            101,
            131,
            190,
            154,
            141,
            168,
            162,
            178,
            88,
            191,
            175,
            177,
            100,
            86,
            136,
            93,
            148,
            114,
            89,
            103,
            98,
            133,
            125,
            126,
            144,
            177,
            187,
            119,
            100,
            200,
            110,
            158,
            164,
            102,
            170,
            84,
            163,
            134,
            151,
            139,
            143,
            108,
            114,
            121,
            181,
            80,
            105,
            189,
            181,
            113,
            117,
            191,
            98,
            87,
            178,
            177,
            185,
            180,
            80,
            199,
            184,
            155,
            165,
            170,
            199,
            94,
            161,
            101,
            177,
            182,
            83,
            160,
            115,
            189,
            129,
            183,
            193,
            138,
            191,
            143,
            126,
            140,
            106,
            91,
            114,
            182,
            169,
            109,
            106,
            193,
            120,
            86,
            196,
            184,
            185,
            118,
            149,
            117,
            123,
            191,
            138,
            148,
            181,
            199,
            194,
            175,
            86,
            91,
            172,
            171,
            187,
            195,
            92,
            162,
            99,
            178,
            185,
            183,
            131,
            138,
            135,
            115,
            189,
            151,
            97,
            166,
            145,
            166,
            142,
            174,
            187,
            191,
            138,
            187,
            146,
            144,
            137,
            134,
            119,
            162,
            145,
            195,
            141,
            91,
            173,
            131,
            169,
            184,
            93,
            156,
            171,
            123,
            83,
            95,
            173,
            142,
            193,
            171,
            112,
            99,
            101,
            131,
            186,
            161,
            176,
            176,
            100,
            105,
            84,
            166,
            158,
            160,
            182,
            186,
            134,
            149,
            194,
            126,
            116,
            165,
            153,
            111,
            134,
            82,
            130,
            200,
            123,
            135,
            146,
            134,
            156,
            198,
            183,
            145,
            114,
            195,
            158,
            84,
            107,
            98,
            178,
            154,
            94,
            95,
            84,
            106,
            186,
            170,
            162,
            165,
            109,
            122,
            118,
            176,
            133,
            153,
            96,
            87,
            100,
            174,
            104,
            115,
            120,
            159,
            101,
            132,
            128,
            114,
            95,
            83,
            148,
            142,
            200,
            145,
            169,
            186,
            109,
            174,
            132,
            195,
            124,
            155,
            131,
            124,
            186,
            172,
            104,
            104,
            170,
            109,
            176,
            119,
            159,
            123,
            83,
            185,
            134,
            151,
            160,
            192,
            166,
            103,
            90,
            200,
            134,
            154,
            187,
            91,
            135,
            146,
            117,
            86,
            100,
            145,
            190,
            129,
            114,
            119,
            193,
            148,
            182,
            117,
            152,
            161,
            193,
            186,
            137,
            166,
            176,
            184,
            160,
            194,
            185,
            181,
            146,
            192,
            83,
            89,
            190,
            101,
            113,
            160,
            123,
            180,
            200,
            102,
            169,
            90,
            191,
            190,
            143,
            90,
            113,
            83,
            118,
            121,
            150,
            169,
            180,
            115,
            171,
            80,
            129,
            170,
            112,
            82,
            92,
            95,
            88,
            169,
            198,
            166,
            88,
            80,
            147,
            185,
            196,
            181,
            182,
            175,
            118,
            151,
            142,
            145,
            163,
            173,
            193,
            169,
            85,
            122,
            128,
            163,
            139,
            81,
            188,
            128,
            102,
            179,
            190,
            184,
            125,
            145,
            172,
            162,
            98,
            166,
            196,
            126,
            197,
            138,
            114,
            150,
            131,
            185,
            130,
            107,
            118,
            123,
            101,
            154,
            185,
            87,
            160,
            111,
            147,
            143,
            167,
            112,
            178,
            95,
            149,
            171,
            167,
            132,
            189,
            121,
            136,
            128,
            107,
            182,
            186,
            81,
            146,
            148,
            132,
            179,
            117,
            179,
            192,
            200,
            120,
            147,
            128,
            198,
            191,
            124,
            168,
            102,
            104,
            165,
            151,
            190,
            145,
            103,
            106,
            169,
            144,
            103,
            102,
            89,
            127,
            108,
            131,
            154,
            159,
            155,
            155,
            164,
            85,
            198,
            183,
            105,
            81,
            116,
            119,
            93,
            176,
            193,
            130,
            113,
            185,
            135,
            112,
            108,
            154,
            165,
            169,
            183,
            112,
            150,
            169,
            114,
            144,
            133,
            121,
            140,
            184,
            145,
            192,
            89,
            159,
            197,
            175,
            128,
            122,
            113,
            158,
            172,
            125,
            98,
            96,
            105,
            86,
            185,
            187,
            182,
            138,
            141,
            102,
            195,
            83,
            157,
            168,
            185,
            142,
            96,
            157,
            130,
            165,
            186,
            122,
            185,
            155,
            131,
            105,
            171,
            138,
            164,
            142,
            129,
            143,
            144,
            174,
            80,
            197,
            106,
            196,
            163,
            145,
            121,
            198,
            124,
            166,
            160,
            161,
            136,
            83,
            139,
            97,
            181,
            146,
            110,
            187,
            124,
            179,
            187,
            188,
            181,
            199,
            102,
            108,
            92,
            145,
            199,
            144,
            100,
            153,
            147,
            112,
            161,
            198,
            163,
            102,
            114,
            163,
            196,
            82,
            108,
            158,
            145,
            125,
            198,
            131,
            97,
            123,
            168,
            188,
            98,
            95,
            175,
            130,
            88,
            126,
            150,
            101,
            84,
            124,
            141,
            92,
            83,
            115,
            126,
            157,
            122,
            93,
            125,
            191,
            152,
            120,
            118,
            169,
            95,
            105,
            200,
            134,
            94,
            98,
            137,
            163,
            177,
            80,
            107,
            97,
            173,
            133,
            85,
            127,
            160,
            190,
            89,
            127,
            129,
            197,
            177,
            170,
            136,
            143,
            187,
            172,
            170,
            156,
            187,
            128,
            184,
            82,
            161,
            136,
            185,
            122,
            173,
            139,
            199,
            167,
            153,
            182,
            200,
            112,
            157,
            198,
            107,
            116,
            92,
            100,
            100,
            158,
            193,
            190,
            133,
            150,
            105,
            192,
            151,
            166,
            125,
            87,
            146,
            83,
            175,
            155,
            187,
            101,
            193,
            90,
            198,
            168,
            164,
            162,
            182,
            200,
            185,
            102,
            144,
            182,
            126,
            155,
            105,
            168,
            146,
            168,
            138,
            94,
            119,
            113,
            88,
            152,
            130,
            84,
            80,
            95,
            109,
            153,
            93,
            93,
            107,
            151,
            166,
            135,
            199,
            102,
            104,
            147,
            157,
            159,
            107,
            101,
            109,
            147,
            149,
            134,
            184,
            148,
            91,
            134,
            115,
            155,
            191,
            162,
            102,
            122,
            84,
            138,
            95,
            94,
            101,
            132,
            86,
            89,
            89,
            100,
            99,
            164,
            90,
            124,
            104,
            170,
            104,
            82,
            146,
            187,
            91,
            91,
            177,
            193,
            188,
            197,
            141,
            188,
            132,
            124,
            175,
            87,
            131,
            104,
            159,
            157,
            173,
            158,
            173,
            100,
            135,
            186,
            111,
            167,
            172,
            139,
            179,
            162,
            164,
            97,
            146,
            90,
            156,
            99,
            146,
            199,
            160,
            159,
            167,
            87,
            113,
            132,
            145,
            97,
            131,
            199,
            184,
            172,
            156,
            85,
            89,
            135,
            150,
            189,
            154,
            162,
            165,
            88,
            88,
            103,
            154,
            102,
            154,
            137,
            106,
            160,
            187,
            175,
            151,
            124,
            111,
            191,
            89,
            111,
            123,
            92,
            197,
            137,
            88,
            197,
            86,
            198,
            128,
            144,
            91,
            111,
            156,
            179,
            105,
            118,
            129,
            126,
            104,
            140,
            152,
            146,
            105,
            162,
            105,
            136,
            123,
            90,
            89,
            185,
            159,
            194,
            130,
            90,
            178,
            174,
            186,
            105,
            124,
            138,
            86,
            138,
            161,
            182,
            102,
            148,
            188,
            107,
            88,
            174,
            139,
            80,
            138,
            158,
            153,
            173,
            144,
            186,
            131,
            107,
            97,
            143,
            179,
            105,
            151,
            134,
            174,
            129,
            155,
            190,
            130,
            83,
            200,
            86,
            153,
            86,
            196,
            112,
            140,
            182,
            154,
            198,
            131,
            118,
            135,
            103,
            102,
            116,
            192,
            101,
            172,
            148,
            160,
            178,
            80,
            146,
            159,
            186,
            170,
            182,
            167,
            127,
            182,
            104,
            112,
            85,
            88,
            136,
            84,
            115,
            187,
            121,
            97,
            104,
            146,
            161,
            81,
            158,
            116,
            150,
            125,
            156,
            188,
            180,
            167,
            170,
            147,
            101,
            187,
            115,
            140,
            185,
            116,
            123,
            88,
            105,
            194,
            84,
            96,
            196,
            186,
            142,
            132,
            134,
            168,
            181,
            148,
            181,
            107,
            185,
            123,
            181,
            163,
            140,
            135,
            146,
            87,
            172,
            184,
            194,
            126,
            109,
            161,
            130,
            173,
            198,
            145,
            88,
            170,
            186,
            86,
            114,
            85,
            146,
            88,
            152,
            156,
            144,
            113,
            139,
            164,
            126,
            124,
            163,
            103,
            113,
            88,
            99,
            126,
            136,
            185,
            105,
            139,
            101,
            181,
            172,
            153,
            123,
            84,
            131,
            96,
            111,
            143,
            134,
            126,
            87,
            91,
            131,
            118,
            170,
            142,
            156,
            110,
            170,
            174,
            158,
            81,
            178,
            158,
            100,
            168,
            141,
            165,
            130,
            84,
            172,
            125,
            177,
            146,
            95,
            137,
            133,
            153,
            178,
            164,
            192,
            162,
            198,
            86,
            80,
            84,
            177,
            85,
            129,
            103,
            156,
            198,
            112,
            131,
            135,
            136,
            157,
            152,
            124,
            121,
            149,
            185,
            86,
            123,
            182,
            95,
            173,
            142,
            82,
            194,
            170,
            94,
            196,
            130,
            125,
            98,
            104,
            178,
            94,
            150,
            131,
            145,
            128,
            151,
            144,
            178,
            168,
            191,
            173,
            150,
            92,
            148,
            124,
            196,
            109,
            192,
            123,
            111,
            145,
            151,
            113,
            82,
            114,
            161,
            188,
            197,
            117,
            200,
            192,
            168,
            169,
            99,
            100,
            103,
            93,
            86,
            199,
            135,
            106,
            132,
            179,
            100,
            184,
            133,
            182,
            90,
            160,
            111,
            120,
            106,
            197,
            182,
            103,
            160,
            166,
            137,
            99,
            157,
            99,
            133,
            124,
            119,
            144,
            184,
            95,
            171,
            111,
            110,
            101,
            115,
            145,
            81,
            109,
            126,
            101,
            100,
            119,
            95,
            160,
            147,
            149,
            173,
            183,
            92,
            88,
            80,
            169,
            81,
            172,
            172,
            80,
            180,
            151,
            115,
            109,
            141,
            154,
            91,
            135,
            92,
            186,
            196,
            90,
            98,
            169,
            190,
            185,
            113,
            141,
            140,
            121,
            114,
            173,
            198,
            159,
            151,
            150,
            91,
            190,
            120,
            198,
            184,
            158,
            146,
            110,
            175,
            153,
            89,
            94,
            148,
            197,
            88,
            127,
            113,
            98,
            189,
            151,
            149,
            117,
            114,
            99,
            145,
            158,
            150,
            104,
            125,
            196,
            196,
            134,
            102,
            103,
            130,
            120,
            118,
            169,
            179,
            147,
            160,
            176,
            107,
            132,
            160,
            157,
            163,
            160,
            151,
            185,
            105,
            92,
            87,
            155,
            84,
            81,
            123,
            122,
            101,
            199,
            197,
            180,
            87,
            147,
            117,
            89,
            104,
            169,
            156,
            103,
            193,
            189,
            101,
            183,
            84,
            86,
            193,
            199,
            86,
            138,
            160,
            146,
            189,
            182,
            111,
            89,
            177,
            100,
            82,
            199,
            92,
            159,
            108,
            155,
            140,
            153,
            151,
            192,
            104,
            130,
            112,
            95,
            172,
            191,
            146,
            182,
            178,
            124,
            154,
            115,
            135,
            93,
            124,
            113,
            164,
            124,
            136,
            138,
            153,
            132,
            155,
            120,
            154,
            122,
            167,
            86,
            193,
            141,
            159,
            171,
            200,
            87,
            87,
            191,
            187,
            142,
            121,
            127,
            165,
            103,
            112,
            80,
            172,
            134,
            101,
            119,
            154,
            174,
            89,
            105,
            116,
            170,
            175,
            150,
            191,
            145,
            117,
            135,
            89,
            192,
            82,
            97,
            189,
            161,
            97,
            182,
            193,
            135,
            81,
            89,
            165,
            131,
            165,
            179,
            186,
            169,
            182,
            104,
            134,
            189,
            185,
            88,
            180,
            151,
            164,
            199,
            167,
            194,
            159,
            157,
            174,
            88,
            136,
            81,
            109,
            93,
            158,
            114,
            134,
            175,
            162,
            196,
            114,
            155,
            146,
            93,
            115,
            119,
            96,
            199,
            125,
            120,
            190,
            111,
            118,
            152,
            112,
            80,
            117,
            178,
            101,
            121,
            172,
            127,
            138,
            158,
            178,
            143,
            87,
            190,
            142,
            169,
            200,
            114,
            153,
            196,
            106,
            138,
            127,
            82,
            117,
            180,
            139,
            174,
            180,
            91,
            118,
            110,
            80,
            156,
            181,
            153,
            146,
            82,
            183,
            156,
            131,
            85,
            197,
            200,
            194,
            145,
            89,
            147,
            148,
            146,
            139,
            177,
            153,
            129,
            158,
            129,
            192,
            167,
            194,
            132,
            199,
            121,
            90,
            83,
            136,
            104,
            187,
            186,
            110,
            182,
            130,
            161,
            86,
            188,
            129,
            158,
            80,
            82,
            121,
            104,
            111,
            157,
            181,
            92,
            180,
            128,
            139,
            198,
            105,
            164,
            82,
            88,
            106,
            150,
            113,
            105,
            123,
            111,
            105,
            138,
            89,
            126,
            114,
            119,
            85,
            125,
            146,
            98,
            106,
            133,
            152,
            136,
            190,
            117,
            80,
            88,
            109,
            109,
            100,
            83,
            129,
            128,
            179,
            152,
            93,
            167,
            147,
            173,
            161,
            82,
            107,
            85,
            87,
            98,
            112,
            197,
            196,
            103,
            121,
            195,
            89,
            141,
            199,
            109,
            104,
            155,
            185,
            157,
            195,
            102,
            83,
            124,
            196,
            106,
            139,
            162,
            200,
            197,
            171,
            152,
            150,
            168,
            94,
            130,
            165,
            169,
            108,
            103,
            158,
            131,
            189,
            101,
            147,
            137,
            170,
            123,
            132,
            131,
            195,
            185,
            150,
            125,
            115,
            198,
            117,
            190,
            200,
            164,
            83,
            131,
            109,
            114,
            160,
            158,
            122,
            170,
            174,
            100,
            105,
            127,
            173,
            137,
            88,
            114,
            94,
            187,
            188,
            116,
            196,
            163,
            108,
            94,
            101,
            187,
            186,
            128,
            181,
            176,
            182,
            102,
            91,
            134,
            123,
            122,
            157,
            162,
            138,
            197,
            122,
            169,
            158,
            146,
            97,
            130,
            103,
            155,
            111,
            97,
            169,
            132,
            162,
            153,
            168,
            101,
            97,
            136,
            90,
            124,
            124,
            101,
            80,
            160,
            156,
            153,
            93,
            99,
            163,
            195,
            93,
            154,
            107,
            110,
            157,
            134,
            178,
            148,
            94,
            162,
            105,
            102,
            129,
            185,
            141,
            141,
            110,
            157,
            174,
            89,
            81,
            193,
            194,
            176,
            163,
            112,
            136,
            149,
            86,
            194,
            110,
            113,
            191,
            85,
            148,
            104,
            138,
            143,
            111,
            163,
            89,
            117,
            199,
            88,
            99,
            129,
            169,
            161,
            144,
            199,
            164,
            149,
            144,
            91,
            161,
            80,
            114,
            83,
            85,
            113,
            198,
            122,
            84,
            106,
            187,
            108,
            145,
            168
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "n_cores",
           "values": [
            2,
            3,
            5,
            6,
            2,
            1,
            8,
            4,
            7,
            5,
            5,
            8,
            4,
            4,
            1,
            2,
            8,
            3,
            5,
            4,
            7,
            2,
            4,
            3,
            3,
            8,
            4,
            7,
            2,
            5,
            7,
            8,
            1,
            4,
            8,
            8,
            4,
            2,
            6,
            2,
            1,
            2,
            2,
            3,
            2,
            3,
            8,
            7,
            8,
            8,
            3,
            8,
            2,
            3,
            7,
            3,
            8,
            3,
            6,
            5,
            3,
            8,
            2,
            2,
            3,
            3,
            4,
            3,
            2,
            4,
            7,
            4,
            3,
            1,
            4,
            6,
            3,
            1,
            4,
            6,
            7,
            4,
            5,
            3,
            1,
            8,
            5,
            3,
            5,
            4,
            3,
            7,
            7,
            6,
            3,
            3,
            5,
            6,
            2,
            7,
            6,
            8,
            5,
            4,
            7,
            4,
            5,
            5,
            1,
            1,
            3,
            7,
            6,
            5,
            7,
            7,
            8,
            3,
            1,
            3,
            8,
            4,
            3,
            2,
            5,
            1,
            4,
            6,
            3,
            6,
            7,
            1,
            7,
            8,
            4,
            5,
            6,
            8,
            7,
            8,
            1,
            3,
            1,
            7,
            3,
            2,
            2,
            7,
            3,
            1,
            5,
            7,
            6,
            2,
            2,
            1,
            4,
            6,
            6,
            1,
            5,
            6,
            5,
            1,
            8,
            4,
            8,
            6,
            7,
            4,
            7,
            5,
            4,
            5,
            5,
            6,
            7,
            5,
            2,
            7,
            1,
            6,
            8,
            4,
            5,
            3,
            6,
            1,
            1,
            5,
            1,
            5,
            8,
            2,
            4,
            7,
            3,
            3,
            4,
            3,
            1,
            7,
            3,
            7,
            6,
            8,
            6,
            5,
            7,
            6,
            3,
            8,
            5,
            5,
            1,
            3,
            8,
            5,
            3,
            5,
            5,
            2,
            4,
            6,
            4,
            2,
            1,
            8,
            4,
            3,
            5,
            1,
            8,
            3,
            2,
            1,
            5,
            5,
            7,
            4,
            8,
            7,
            6,
            5,
            7,
            7,
            2,
            4,
            5,
            2,
            2,
            5,
            8,
            4,
            1,
            7,
            4,
            7,
            3,
            6,
            7,
            4,
            1,
            4,
            2,
            2,
            6,
            6,
            5,
            1,
            7,
            8,
            1,
            3,
            2,
            4,
            4,
            4,
            7,
            8,
            6,
            3,
            7,
            3,
            8,
            1,
            7,
            8,
            1,
            6,
            2,
            7,
            6,
            5,
            5,
            7,
            8,
            6,
            8,
            7,
            1,
            6,
            1,
            2,
            2,
            3,
            3,
            6,
            6,
            4,
            2,
            7,
            3,
            8,
            2,
            8,
            4,
            2,
            1,
            3,
            2,
            1,
            1,
            1,
            6,
            7,
            1,
            1,
            1,
            5,
            1,
            6,
            4,
            3,
            2,
            4,
            3,
            1,
            1,
            7,
            3,
            6,
            8,
            7,
            1,
            4,
            5,
            6,
            1,
            3,
            1,
            5,
            8,
            8,
            7,
            4,
            1,
            7,
            5,
            7,
            7,
            2,
            2,
            4,
            4,
            8,
            6,
            8,
            1,
            6,
            2,
            7,
            5,
            7,
            5,
            7,
            2,
            6,
            4,
            2,
            7,
            7,
            3,
            2,
            1,
            3,
            8,
            8,
            7,
            7,
            6,
            7,
            4,
            7,
            4,
            2,
            8,
            2,
            4,
            1,
            3,
            7,
            3,
            4,
            6,
            4,
            5,
            5,
            8,
            4,
            5,
            6,
            5,
            5,
            3,
            4,
            8,
            5,
            7,
            2,
            5,
            3,
            4,
            2,
            4,
            3,
            6,
            7,
            6,
            2,
            8,
            8,
            1,
            6,
            4,
            2,
            7,
            6,
            2,
            7,
            8,
            7,
            7,
            3,
            7,
            2,
            2,
            5,
            1,
            8,
            2,
            3,
            3,
            6,
            7,
            3,
            3,
            5,
            5,
            4,
            7,
            7,
            8,
            2,
            8,
            3,
            4,
            8,
            7,
            1,
            7,
            5,
            3,
            4,
            3,
            4,
            1,
            4,
            4,
            4,
            6,
            1,
            5,
            2,
            6,
            4,
            5,
            4,
            5,
            3,
            6,
            1,
            7,
            8,
            5,
            5,
            3,
            2,
            3,
            2,
            8,
            6,
            7,
            7,
            2,
            1,
            7,
            3,
            5,
            3,
            4,
            1,
            1,
            4,
            6,
            7,
            8,
            3,
            4,
            3,
            1,
            8,
            3,
            5,
            2,
            8,
            8,
            4,
            2,
            1,
            6,
            5,
            5,
            4,
            7,
            8,
            6,
            4,
            6,
            8,
            1,
            4,
            8,
            5,
            2,
            6,
            8,
            2,
            1,
            2,
            5,
            8,
            8,
            8,
            4,
            5,
            8,
            1,
            1,
            1,
            8,
            2,
            8,
            6,
            6,
            7,
            8,
            2,
            2,
            2,
            5,
            6,
            2,
            1,
            7,
            5,
            1,
            5,
            4,
            1,
            8,
            3,
            5,
            3,
            3,
            3,
            1,
            4,
            8,
            5,
            5,
            1,
            5,
            3,
            4,
            3,
            2,
            8,
            5,
            7,
            6,
            4,
            8,
            7,
            2,
            5,
            7,
            7,
            8,
            4,
            3,
            2,
            7,
            3,
            7,
            8,
            1,
            1,
            5,
            2,
            4,
            6,
            7,
            1,
            4,
            4,
            3,
            3,
            6,
            8,
            7,
            5,
            8,
            3,
            6,
            3,
            7,
            8,
            5,
            8,
            5,
            7,
            6,
            3,
            5,
            8,
            5,
            1,
            4,
            1,
            7,
            8,
            7,
            7,
            3,
            8,
            7,
            4,
            6,
            4,
            3,
            8,
            3,
            7,
            4,
            4,
            8,
            3,
            6,
            4,
            4,
            7,
            7,
            5,
            3,
            1,
            2,
            6,
            6,
            5,
            7,
            7,
            1,
            7,
            3,
            7,
            5,
            2,
            4,
            6,
            5,
            4,
            5,
            8,
            1,
            3,
            5,
            6,
            5,
            3,
            3,
            1,
            1,
            5,
            7,
            7,
            5,
            6,
            2,
            7,
            2,
            8,
            2,
            7,
            2,
            3,
            4,
            3,
            5,
            7,
            2,
            4,
            7,
            8,
            3,
            6,
            7,
            3,
            3,
            1,
            6,
            1,
            7,
            7,
            7,
            4,
            6,
            7,
            7,
            3,
            6,
            2,
            6,
            1,
            1,
            8,
            1,
            2,
            3,
            3,
            2,
            2,
            6,
            4,
            1,
            5,
            3,
            6,
            3,
            6,
            4,
            5,
            7,
            8,
            5,
            7,
            2,
            6,
            6,
            2,
            8,
            2,
            2,
            1,
            7,
            3,
            2,
            7,
            1,
            8,
            5,
            7,
            2,
            1,
            4,
            8,
            3,
            2,
            1,
            3,
            5,
            5,
            6,
            6,
            8,
            5,
            7,
            7,
            7,
            3,
            1,
            1,
            3,
            4,
            6,
            5,
            3,
            2,
            1,
            7,
            4,
            5,
            1,
            1,
            6,
            1,
            4,
            7,
            5,
            1,
            3,
            2,
            7,
            8,
            6,
            3,
            6,
            6,
            7,
            1,
            3,
            2,
            6,
            7,
            8,
            1,
            5,
            1,
            8,
            1,
            4,
            3,
            2,
            8,
            3,
            7,
            1,
            6,
            3,
            2,
            7,
            4,
            2,
            6,
            4,
            8,
            3,
            2,
            8,
            3,
            5,
            7,
            2,
            8,
            7,
            2,
            7,
            7,
            3,
            4,
            3,
            2,
            3,
            6,
            2,
            1,
            6,
            5,
            5,
            2,
            5,
            6,
            3,
            1,
            6,
            7,
            4,
            8,
            1,
            5,
            1,
            7,
            5,
            2,
            3,
            3,
            1,
            5,
            4,
            4,
            8,
            6,
            2,
            4,
            3,
            3,
            1,
            1,
            3,
            4,
            4,
            2,
            2,
            4,
            4,
            1,
            3,
            3,
            7,
            6,
            8,
            3,
            1,
            5,
            1,
            1,
            4,
            4,
            7,
            2,
            1,
            3,
            4,
            3,
            3,
            1,
            6,
            8,
            3,
            3,
            8,
            2,
            7,
            4,
            3,
            6,
            8,
            1,
            2,
            4,
            2,
            6,
            7,
            5,
            8,
            3,
            8,
            4,
            1,
            1,
            2,
            6,
            1,
            7,
            5,
            8,
            2,
            8,
            1,
            3,
            8,
            2,
            4,
            1,
            4,
            1,
            6,
            5,
            2,
            5,
            1,
            8,
            3,
            7,
            4,
            4,
            7,
            7,
            5,
            5,
            7,
            7,
            4,
            6,
            6,
            3,
            2,
            8,
            6,
            6,
            5,
            2,
            3,
            7,
            8,
            5,
            1,
            2,
            8,
            5,
            4,
            2,
            3,
            6,
            2,
            4,
            7,
            1,
            8,
            4,
            4,
            4,
            3,
            3,
            5,
            3,
            4,
            2,
            2,
            5,
            1,
            6,
            7,
            4,
            7,
            3,
            7,
            2,
            3,
            5,
            7,
            7,
            3,
            2,
            7,
            7,
            5,
            8,
            7,
            1,
            1,
            3,
            5,
            8,
            5,
            2,
            4,
            6,
            8,
            8,
            4,
            1,
            1,
            6,
            6,
            3,
            6,
            6,
            1,
            4,
            7,
            7,
            7,
            3,
            6,
            1,
            5,
            3,
            6,
            8,
            5,
            6,
            4,
            7,
            5,
            5,
            1,
            3,
            3,
            2,
            5,
            2,
            4,
            4,
            3,
            7,
            5,
            2,
            7,
            4,
            5,
            1,
            3,
            1,
            1,
            2,
            7,
            7,
            1,
            2,
            8,
            6,
            2,
            8,
            2,
            6,
            2,
            8,
            2,
            4,
            8,
            5,
            8,
            1,
            3,
            3,
            5,
            1,
            8,
            7,
            1,
            8,
            4,
            5,
            8,
            1,
            2,
            2,
            6,
            6,
            5,
            4,
            3,
            3,
            7,
            4,
            4,
            6,
            7,
            6,
            1,
            3,
            8,
            5,
            7,
            4,
            5,
            8,
            7,
            3,
            8,
            7,
            3,
            8,
            3,
            6,
            4,
            2,
            7,
            5,
            8,
            6,
            3,
            2,
            2,
            7,
            4,
            2,
            3,
            4,
            6,
            1,
            8,
            1,
            7,
            6,
            8,
            6,
            7,
            2,
            3,
            7,
            1,
            1,
            8,
            7,
            2,
            2,
            7,
            2,
            6,
            3,
            5,
            3,
            4,
            8,
            5,
            3,
            3,
            1,
            3,
            8,
            4,
            4,
            3,
            7,
            2,
            8,
            7,
            3,
            4,
            6,
            8,
            1,
            3,
            4,
            3,
            1,
            1,
            4,
            2,
            1,
            6,
            1,
            5,
            7,
            7,
            5,
            1,
            2,
            8,
            7,
            8,
            4,
            6,
            8,
            7,
            2,
            5,
            2,
            1,
            5,
            6,
            6,
            2,
            6,
            6,
            6,
            7,
            4,
            1,
            8,
            7,
            1,
            1,
            1,
            3,
            1,
            2,
            3,
            6,
            4,
            2,
            7,
            1,
            1,
            8,
            8,
            7,
            1,
            5,
            6,
            6,
            6,
            5,
            2,
            3,
            6,
            5,
            5,
            4,
            7,
            4,
            5,
            8,
            1,
            8,
            7,
            7,
            3,
            2,
            2,
            5,
            2,
            7,
            8,
            3,
            5,
            3,
            5,
            4,
            1,
            4,
            5,
            2,
            3,
            6,
            4,
            1,
            7,
            7,
            5,
            6,
            2,
            5,
            3,
            8,
            5,
            1,
            3,
            8,
            4,
            4,
            7,
            8,
            8,
            4,
            4,
            2,
            8,
            7,
            5,
            6,
            5,
            5,
            2,
            8,
            7,
            4,
            2,
            1,
            4,
            2,
            8,
            8,
            8,
            2,
            2,
            4,
            5,
            6,
            5,
            1,
            1,
            4,
            2,
            4,
            8,
            8,
            5,
            2,
            4,
            7,
            5,
            2,
            5,
            4,
            4,
            2,
            1,
            8,
            4,
            8,
            3,
            5,
            4,
            4,
            4,
            2,
            6,
            5,
            3,
            5,
            1,
            8,
            4,
            6,
            8,
            7,
            8,
            4,
            8,
            4,
            3,
            2,
            8,
            4,
            2,
            5,
            2,
            1,
            4,
            2,
            8,
            7,
            4,
            5,
            8,
            5,
            4,
            6,
            6,
            1,
            7,
            6,
            6,
            8,
            3,
            8,
            4,
            8,
            4,
            8,
            8,
            6,
            5,
            2,
            3,
            3,
            4,
            4,
            8,
            5,
            6,
            4,
            8,
            8,
            6,
            4,
            8,
            3,
            4,
            5,
            1,
            1,
            4,
            1,
            6,
            1,
            8,
            2,
            6,
            2,
            6,
            4,
            6,
            6,
            8,
            3,
            4,
            3,
            3,
            1,
            4,
            6,
            1,
            1,
            8,
            1,
            1,
            6,
            4,
            6,
            2,
            5,
            8,
            3,
            7,
            5,
            7,
            3,
            1,
            1,
            8,
            6,
            2,
            8,
            6,
            3,
            3,
            2,
            2,
            8,
            4,
            6,
            5,
            4,
            7,
            6,
            8,
            6,
            4,
            5,
            8,
            1,
            2,
            7,
            4,
            7,
            2,
            1,
            8,
            2,
            1,
            4,
            4,
            3,
            8,
            1,
            8,
            6,
            6,
            7,
            5,
            8,
            2,
            3,
            4,
            7,
            8,
            2,
            5,
            2,
            8,
            1,
            8,
            6,
            1,
            3,
            2,
            7,
            7,
            8,
            6,
            2,
            5,
            8,
            2,
            4,
            6,
            4,
            5,
            1,
            2,
            6,
            4,
            2,
            4,
            6,
            7,
            2,
            4,
            8,
            8,
            2,
            8,
            8,
            1,
            5,
            6,
            2,
            2,
            7,
            8,
            4,
            1,
            5,
            2,
            4,
            2,
            8,
            6,
            4,
            1,
            7,
            3,
            5,
            7,
            6,
            1,
            4,
            2,
            1,
            8,
            5,
            8,
            6,
            4,
            2,
            5,
            5,
            5,
            6,
            1,
            3,
            8,
            7,
            1,
            6,
            8,
            3,
            8,
            4,
            1,
            2,
            5,
            5,
            4,
            4,
            6,
            2,
            8,
            6,
            2,
            4,
            3,
            2,
            5,
            1,
            7,
            8,
            1,
            3,
            6,
            2,
            4,
            2,
            3,
            4,
            7,
            1,
            4,
            2,
            5,
            6,
            8,
            1,
            1,
            7,
            4,
            8,
            4,
            8,
            4,
            6,
            8,
            6,
            1,
            1,
            2,
            4,
            5,
            8,
            4,
            6,
            6,
            1,
            7,
            4,
            1,
            8,
            4,
            5,
            5,
            2,
            1,
            4,
            8,
            6,
            6,
            5,
            2,
            1,
            2,
            1,
            7,
            4,
            2,
            6,
            7,
            3,
            3,
            5,
            4,
            4,
            4,
            1,
            4,
            7,
            8,
            4,
            4,
            8,
            5,
            8,
            8,
            7,
            4,
            4,
            4,
            3,
            4,
            7,
            1,
            5,
            2,
            2,
            6,
            5,
            7,
            2,
            5,
            5,
            2,
            5,
            8,
            1,
            3,
            3,
            6,
            8,
            6,
            8,
            5,
            8,
            7,
            2,
            3,
            2,
            5,
            4,
            5,
            2,
            4,
            6,
            1,
            6,
            4,
            2,
            4,
            3,
            8,
            8,
            2,
            2,
            8,
            7,
            6,
            6,
            7,
            8,
            7,
            3,
            3,
            8,
            2,
            8,
            7,
            3,
            3,
            3,
            3,
            5,
            6,
            2,
            4,
            4,
            1,
            4,
            8,
            5,
            2,
            4,
            6,
            2,
            1,
            4,
            7,
            6,
            5,
            3,
            6,
            6,
            2,
            2,
            5,
            6,
            1,
            8,
            7,
            5,
            7,
            5,
            4,
            2,
            5,
            4,
            4,
            5,
            5,
            5,
            2,
            7,
            5,
            6,
            3,
            1,
            1,
            3,
            6,
            1,
            7,
            1,
            3,
            7,
            4,
            8,
            4,
            4,
            2,
            2,
            7,
            3,
            1,
            7,
            1,
            2,
            7,
            7,
            3,
            7,
            4,
            4,
            6,
            7,
            6,
            4,
            5,
            1,
            7,
            2,
            2,
            8,
            3,
            7,
            4,
            3,
            7,
            8,
            3,
            6,
            7,
            3,
            5,
            8,
            4,
            6,
            6,
            4,
            1,
            5,
            5,
            7,
            6,
            1,
            1,
            4,
            3,
            7,
            8,
            1,
            2,
            2,
            5,
            4,
            6,
            3,
            1,
            5,
            4,
            1,
            8,
            4,
            5,
            5,
            7,
            8,
            5,
            5,
            4,
            3,
            2,
            8,
            4,
            2,
            3,
            2,
            7,
            7,
            2,
            6,
            1,
            8,
            3,
            5,
            6,
            7,
            7,
            3,
            7,
            8,
            1,
            3,
            8,
            4,
            2,
            5,
            4,
            1,
            5,
            4,
            6,
            6,
            6,
            5,
            8,
            6,
            6,
            7,
            7,
            3,
            7,
            8,
            1,
            4,
            5,
            5,
            5,
            4,
            6,
            6,
            5,
            7,
            3,
            4,
            8,
            2,
            6,
            2,
            8,
            5,
            7,
            6,
            1,
            2,
            8,
            7,
            2,
            5,
            3,
            8,
            3,
            6,
            5,
            7,
            5,
            2,
            4,
            1,
            3,
            1,
            8,
            3,
            5,
            1,
            6,
            4,
            8,
            5,
            6
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "pc",
           "values": [
            2,
            6,
            6,
            9,
            14,
            7,
            10,
            0,
            14,
            15,
            1,
            18,
            17,
            11,
            17,
            16,
            4,
            17,
            18,
            11,
            14,
            2,
            7,
            20,
            13,
            3,
            19,
            6,
            18,
            7,
            9,
            20,
            18,
            0,
            3,
            17,
            10,
            18,
            17,
            20,
            14,
            10,
            19,
            7,
            14,
            1,
            14,
            10,
            8,
            15,
            19,
            19,
            0,
            10,
            0,
            20,
            19,
            9,
            14,
            4,
            5,
            6,
            0,
            18,
            3,
            17,
            14,
            0,
            1,
            1,
            11,
            4,
            16,
            5,
            6,
            20,
            15,
            3,
            20,
            8,
            7,
            19,
            3,
            15,
            13,
            17,
            5,
            11,
            13,
            14,
            5,
            18,
            0,
            15,
            9,
            19,
            16,
            18,
            1,
            7,
            13,
            9,
            4,
            10,
            3,
            3,
            10,
            16,
            4,
            7,
            4,
            20,
            1,
            14,
            19,
            9,
            0,
            11,
            12,
            0,
            7,
            2,
            10,
            7,
            4,
            20,
            1,
            17,
            16,
            1,
            3,
            14,
            10,
            12,
            5,
            0,
            8,
            7,
            14,
            2,
            18,
            4,
            5,
            6,
            11,
            2,
            7,
            4,
            7,
            7,
            17,
            14,
            12,
            9,
            8,
            6,
            4,
            18,
            0,
            16,
            13,
            16,
            3,
            20,
            17,
            8,
            4,
            3,
            8,
            20,
            6,
            5,
            8,
            4,
            1,
            15,
            10,
            1,
            13,
            4,
            20,
            12,
            5,
            19,
            12,
            13,
            9,
            17,
            5,
            5,
            18,
            15,
            8,
            6,
            15,
            8,
            13,
            7,
            4,
            20,
            0,
            15,
            9,
            17,
            19,
            1,
            17,
            16,
            4,
            6,
            1,
            0,
            17,
            16,
            0,
            7,
            2,
            9,
            1,
            6,
            19,
            10,
            12,
            11,
            4,
            5,
            19,
            1,
            19,
            18,
            6,
            6,
            17,
            19,
            15,
            8,
            12,
            16,
            1,
            2,
            1,
            0,
            20,
            10,
            14,
            12,
            15,
            15,
            10,
            4,
            1,
            0,
            7,
            7,
            17,
            16,
            13,
            11,
            1,
            17,
            10,
            1,
            1,
            10,
            16,
            15,
            14,
            12,
            5,
            6,
            18,
            2,
            12,
            14,
            15,
            8,
            13,
            19,
            13,
            4,
            20,
            14,
            11,
            7,
            9,
            3,
            7,
            0,
            20,
            4,
            11,
            13,
            18,
            14,
            5,
            1,
            19,
            2,
            13,
            17,
            19,
            7,
            20,
            14,
            8,
            20,
            2,
            20,
            13,
            8,
            6,
            2,
            14,
            17,
            0,
            17,
            9,
            13,
            8,
            8,
            20,
            13,
            2,
            17,
            20,
            14,
            11,
            15,
            16,
            8,
            10,
            3,
            10,
            17,
            13,
            1,
            1,
            19,
            6,
            1,
            9,
            13,
            15,
            20,
            4,
            4,
            7,
            13,
            15,
            4,
            16,
            20,
            5,
            16,
            18,
            12,
            1,
            3,
            13,
            3,
            19,
            18,
            7,
            2,
            12,
            14,
            2,
            17,
            12,
            12,
            12,
            18,
            19,
            0,
            14,
            10,
            0,
            10,
            14,
            9,
            13,
            10,
            4,
            1,
            20,
            5,
            17,
            15,
            8,
            12,
            16,
            3,
            16,
            19,
            1,
            13,
            12,
            2,
            12,
            4,
            12,
            18,
            10,
            5,
            9,
            3,
            2,
            11,
            15,
            3,
            8,
            13,
            17,
            0,
            17,
            12,
            5,
            4,
            2,
            12,
            12,
            17,
            14,
            13,
            6,
            4,
            10,
            2,
            7,
            19,
            7,
            15,
            2,
            8,
            5,
            11,
            15,
            10,
            16,
            0,
            9,
            2,
            9,
            16,
            12,
            13,
            13,
            16,
            18,
            3,
            10,
            4,
            5,
            12,
            2,
            19,
            7,
            14,
            5,
            12,
            12,
            17,
            20,
            14,
            15,
            10,
            6,
            11,
            0,
            12,
            13,
            10,
            4,
            2,
            7,
            14,
            13,
            18,
            16,
            4,
            4,
            3,
            17,
            2,
            18,
            3,
            15,
            14,
            4,
            14,
            20,
            10,
            9,
            9,
            0,
            17,
            12,
            20,
            2,
            4,
            16,
            18,
            2,
            6,
            0,
            2,
            10,
            15,
            6,
            15,
            20,
            13,
            14,
            19,
            14,
            6,
            0,
            18,
            4,
            20,
            17,
            10,
            7,
            11,
            0,
            16,
            15,
            11,
            11,
            18,
            8,
            10,
            17,
            13,
            2,
            7,
            20,
            16,
            6,
            11,
            9,
            7,
            9,
            18,
            0,
            17,
            4,
            7,
            3,
            4,
            14,
            6,
            0,
            20,
            10,
            7,
            18,
            10,
            7,
            7,
            10,
            16,
            14,
            4,
            17,
            15,
            15,
            15,
            20,
            9,
            8,
            16,
            15,
            1,
            13,
            10,
            16,
            12,
            9,
            15,
            18,
            17,
            17,
            8,
            20,
            4,
            10,
            15,
            18,
            4,
            12,
            5,
            18,
            1,
            20,
            2,
            12,
            10,
            7,
            13,
            5,
            20,
            7,
            6,
            0,
            13,
            14,
            15,
            11,
            8,
            5,
            3,
            15,
            0,
            14,
            15,
            7,
            9,
            6,
            10,
            2,
            18,
            1,
            16,
            6,
            19,
            3,
            3,
            3,
            2,
            2,
            2,
            0,
            10,
            18,
            11,
            5,
            11,
            8,
            6,
            2,
            1,
            10,
            3,
            6,
            8,
            7,
            14,
            16,
            0,
            16,
            11,
            18,
            20,
            13,
            7,
            16,
            17,
            17,
            10,
            4,
            2,
            9,
            12,
            0,
            5,
            13,
            7,
            8,
            3,
            17,
            13,
            1,
            9,
            4,
            11,
            7,
            6,
            17,
            15,
            3,
            7,
            13,
            3,
            19,
            9,
            14,
            1,
            12,
            9,
            3,
            15,
            7,
            3,
            14,
            12,
            6,
            6,
            7,
            9,
            13,
            3,
            4,
            12,
            0,
            8,
            6,
            7,
            15,
            14,
            1,
            3,
            10,
            20,
            0,
            11,
            13,
            15,
            12,
            13,
            15,
            1,
            9,
            3,
            8,
            18,
            2,
            17,
            10,
            11,
            10,
            10,
            17,
            8,
            20,
            6,
            11,
            2,
            14,
            10,
            9,
            1,
            0,
            7,
            13,
            20,
            18,
            3,
            9,
            9,
            3,
            7,
            18,
            9,
            10,
            18,
            1,
            10,
            3,
            20,
            18,
            4,
            1,
            0,
            19,
            10,
            2,
            13,
            5,
            19,
            8,
            17,
            4,
            2,
            0,
            2,
            16,
            10,
            4,
            1,
            16,
            18,
            17,
            15,
            10,
            17,
            18,
            10,
            15,
            17,
            1,
            8,
            15,
            15,
            9,
            3,
            3,
            2,
            6,
            19,
            17,
            17,
            16,
            19,
            2,
            2,
            18,
            9,
            18,
            10,
            8,
            20,
            10,
            18,
            2,
            14,
            14,
            5,
            16,
            14,
            0,
            10,
            3,
            0,
            19,
            9,
            20,
            8,
            9,
            6,
            8,
            16,
            3,
            7,
            12,
            18,
            14,
            16,
            5,
            6,
            6,
            6,
            10,
            7,
            9,
            15,
            16,
            18,
            18,
            18,
            19,
            7,
            10,
            6,
            12,
            7,
            3,
            11,
            19,
            4,
            7,
            20,
            2,
            14,
            4,
            15,
            19,
            4,
            7,
            18,
            10,
            14,
            19,
            0,
            4,
            0,
            13,
            17,
            9,
            11,
            12,
            6,
            16,
            5,
            13,
            10,
            18,
            1,
            20,
            1,
            4,
            6,
            1,
            16,
            9,
            11,
            9,
            7,
            17,
            6,
            17,
            11,
            18,
            11,
            4,
            19,
            11,
            12,
            3,
            0,
            6,
            14,
            9,
            20,
            20,
            5,
            13,
            4,
            2,
            1,
            15,
            11,
            20,
            4,
            16,
            4,
            14,
            16,
            7,
            9,
            1,
            10,
            14,
            12,
            3,
            7,
            2,
            2,
            15,
            16,
            10,
            1,
            4,
            2,
            0,
            0,
            17,
            16,
            16,
            15,
            18,
            10,
            16,
            7,
            14,
            16,
            14,
            3,
            14,
            13,
            9,
            6,
            6,
            8,
            19,
            17,
            0,
            0,
            10,
            11,
            9,
            10,
            12,
            12,
            11,
            18,
            11,
            14,
            6,
            9,
            14,
            0,
            3,
            9,
            0,
            15,
            17,
            0,
            6,
            14,
            19,
            18,
            16,
            15,
            15,
            20,
            9,
            9,
            12,
            5,
            3,
            12,
            16,
            12,
            19,
            1,
            15,
            17,
            6,
            7,
            17,
            10,
            0,
            0,
            19,
            19,
            15,
            7,
            18,
            20,
            1,
            3,
            20,
            16,
            10,
            17,
            2,
            12,
            0,
            0,
            7,
            10,
            17,
            1,
            11,
            2,
            3,
            13,
            4,
            14,
            14,
            8,
            7,
            14,
            14,
            14,
            13,
            5,
            9,
            5,
            7,
            7,
            4,
            7,
            13,
            3,
            7,
            0,
            12,
            11,
            6,
            5,
            17,
            20,
            6,
            8,
            19,
            7,
            5,
            20,
            18,
            12,
            1,
            14,
            0,
            12,
            7,
            16,
            0,
            3,
            4,
            2,
            13,
            20,
            1,
            3,
            13,
            15,
            12,
            14,
            3,
            10,
            15,
            13,
            2,
            14,
            16,
            3,
            9,
            0,
            13,
            20,
            7,
            13,
            12,
            13,
            2,
            1,
            9,
            11,
            10,
            0,
            5,
            1,
            7,
            20,
            9,
            19,
            1,
            17,
            10,
            20,
            5,
            6,
            11,
            9,
            13,
            14,
            18,
            9,
            17,
            9,
            1,
            6,
            16,
            3,
            3,
            3,
            6,
            12,
            6,
            4,
            17,
            11,
            2,
            0,
            6,
            11,
            1,
            1,
            14,
            13,
            14,
            6,
            3,
            1,
            3,
            15,
            15,
            0,
            18,
            2,
            9,
            12,
            13,
            17,
            1,
            0,
            20,
            12,
            14,
            12,
            7,
            1,
            9,
            12,
            15,
            9,
            10,
            1,
            0,
            1,
            15,
            0,
            1,
            2,
            13,
            7,
            11,
            10,
            14,
            18,
            8,
            8,
            6,
            15,
            17,
            2,
            1,
            3,
            20,
            0,
            1,
            12,
            20,
            14,
            2,
            8,
            15,
            11,
            10,
            9,
            15,
            7,
            11,
            4,
            20,
            12,
            10,
            20,
            5,
            3,
            5,
            9,
            1,
            9,
            16,
            7,
            3,
            8,
            6,
            5,
            6,
            6,
            13,
            19,
            7,
            3,
            19,
            9,
            4,
            19,
            17,
            20,
            13,
            15,
            9,
            0,
            9,
            10,
            14,
            3,
            2,
            3,
            20,
            8,
            12,
            0,
            18,
            6,
            17,
            10,
            12,
            11,
            5,
            4,
            18,
            4,
            5,
            2,
            17,
            10,
            8,
            17,
            5,
            15,
            14,
            3,
            7,
            13,
            16,
            8,
            6,
            8,
            6,
            1,
            0,
            13,
            15,
            8,
            11,
            14,
            0,
            9,
            4,
            19,
            0,
            11,
            20,
            3,
            19,
            9,
            16,
            17,
            13,
            7,
            10,
            11,
            9,
            11,
            5,
            9,
            0,
            10,
            17,
            8,
            0,
            18,
            9,
            17,
            7,
            13,
            14,
            11,
            0,
            6,
            4,
            20,
            0,
            18,
            20,
            6,
            7,
            18,
            9,
            13,
            16,
            8,
            19,
            1,
            8,
            13,
            12,
            18,
            9,
            15,
            1,
            19,
            1,
            4,
            12,
            15,
            15,
            1,
            19,
            9,
            12,
            1,
            20,
            9,
            16,
            7,
            13,
            8,
            6,
            15,
            16,
            9,
            17,
            17,
            0,
            11,
            6,
            8,
            2,
            2,
            2,
            5,
            0,
            8,
            2,
            10,
            8,
            19,
            20,
            2,
            19,
            20,
            10,
            10,
            3,
            19,
            8,
            1,
            1,
            9,
            8,
            6,
            1,
            17,
            8,
            7,
            7,
            18,
            5,
            7,
            20,
            10,
            4,
            9,
            7,
            2,
            17,
            15,
            5,
            13,
            19,
            18,
            2,
            4,
            7,
            10,
            4,
            5,
            9,
            9,
            20,
            1,
            19,
            19,
            4,
            6,
            13,
            2,
            0,
            20,
            8,
            9,
            20,
            8,
            19,
            1,
            7,
            1,
            12,
            10,
            8,
            18,
            2,
            18,
            8,
            3,
            4,
            8,
            8,
            0,
            20,
            7,
            20,
            12,
            10,
            5,
            11,
            13,
            9,
            10,
            7,
            20,
            2,
            20,
            0,
            6,
            20,
            0,
            20,
            13,
            11,
            1,
            10,
            11,
            20,
            11,
            14,
            8,
            15,
            1,
            6,
            15,
            14,
            8,
            14,
            14,
            14,
            12,
            2,
            6,
            17,
            7,
            6,
            7,
            15,
            0,
            4,
            10,
            18,
            9,
            16,
            20,
            18,
            2,
            17,
            10,
            10,
            10,
            16,
            18,
            17,
            4,
            2,
            4,
            16,
            7,
            17,
            1,
            10,
            18,
            3,
            6,
            10,
            9,
            11,
            5,
            19,
            10,
            14,
            15,
            10,
            12,
            16,
            8,
            7,
            16,
            19,
            8,
            5,
            8,
            16,
            10,
            1,
            18,
            1,
            19,
            1,
            3,
            19,
            2,
            3,
            3,
            19,
            19,
            10,
            0,
            10,
            2,
            14,
            4,
            9,
            1,
            2,
            17,
            6,
            10,
            14,
            2,
            13,
            3,
            5,
            17,
            1,
            20,
            7,
            1,
            19,
            0,
            10,
            14,
            15,
            12,
            14,
            7,
            11,
            12,
            19,
            2,
            17,
            13,
            2,
            3,
            2,
            11,
            16,
            10,
            17,
            18,
            17,
            13,
            4,
            2,
            10,
            20,
            17,
            11,
            15,
            3,
            7,
            19,
            19,
            12,
            8,
            7,
            1,
            20,
            10,
            2,
            10,
            12,
            14,
            12,
            13,
            14,
            12,
            6,
            4,
            14,
            20,
            7,
            13,
            13,
            7,
            2,
            17,
            12,
            17,
            16,
            7,
            1,
            3,
            17,
            14,
            3,
            13,
            8,
            9,
            1,
            18,
            20,
            17,
            19,
            15,
            9,
            17,
            11,
            9,
            0,
            6,
            19,
            3,
            7,
            20,
            7,
            6,
            10,
            13,
            9,
            5,
            1,
            14,
            7,
            9,
            9,
            6,
            0,
            4,
            5,
            15,
            9,
            18,
            19,
            14,
            7,
            0,
            8,
            14,
            19,
            12,
            10,
            5,
            0,
            9,
            7,
            1,
            9,
            0,
            9,
            16,
            20,
            13,
            20,
            12,
            16,
            14,
            1,
            11,
            9,
            12,
            20,
            3,
            1,
            6,
            2,
            8,
            20,
            14,
            10,
            3,
            19,
            7,
            16,
            3,
            16,
            8,
            7,
            19,
            1,
            7,
            20,
            3,
            8,
            18,
            5,
            18,
            10,
            8,
            6,
            0,
            14,
            9,
            20,
            6,
            1,
            11,
            9,
            9,
            3,
            1,
            3,
            6,
            9,
            10,
            4,
            3,
            6,
            15,
            17,
            8,
            11,
            13,
            19,
            9,
            10,
            19,
            2,
            7,
            4,
            10,
            3,
            16,
            4,
            7,
            0,
            9,
            19,
            4,
            16,
            15,
            6,
            16,
            11,
            18,
            2,
            20,
            12,
            7,
            6,
            9,
            11,
            2,
            9,
            20,
            18,
            1,
            10,
            17,
            6,
            19,
            16,
            6,
            17,
            0,
            8,
            2,
            2,
            1,
            16,
            8,
            7,
            4,
            10,
            15,
            12,
            2,
            17,
            0,
            16,
            14,
            10,
            14,
            17,
            17,
            8,
            12,
            12,
            2,
            16,
            4,
            7,
            6,
            11,
            14,
            3,
            2,
            10,
            7,
            19,
            9,
            4,
            6,
            15,
            8,
            14,
            10,
            14,
            4,
            19,
            3,
            19,
            15,
            4,
            13,
            12,
            5,
            20,
            17,
            8,
            20,
            20,
            12,
            13,
            4,
            16,
            15,
            20,
            0,
            8,
            2,
            7,
            1,
            0,
            0,
            4,
            20,
            16,
            20,
            10,
            18,
            14,
            20,
            1,
            20,
            8,
            13,
            6,
            12,
            18,
            4,
            12,
            9,
            16,
            9,
            20,
            9,
            2,
            15,
            2,
            4,
            11,
            0,
            11,
            11,
            10,
            14,
            0,
            8,
            11,
            8,
            18,
            10,
            9,
            11,
            7,
            17,
            1,
            11,
            11,
            20,
            6,
            14,
            8,
            15,
            7,
            7,
            0,
            12,
            10,
            7,
            12,
            15,
            15,
            8,
            20,
            16,
            11,
            6,
            8,
            15,
            12,
            1,
            16,
            14,
            5,
            10,
            16,
            20,
            9,
            1,
            6,
            12,
            2,
            19,
            4,
            7,
            20,
            17,
            4,
            11,
            6,
            4,
            8,
            0,
            20,
            4,
            7,
            9,
            3,
            16,
            8,
            7,
            16,
            11,
            2,
            10,
            20,
            20,
            16,
            2,
            15,
            11,
            15,
            3,
            11,
            9,
            19,
            4,
            0,
            2,
            14,
            3,
            3,
            5,
            16
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "px_height",
           "values": [
            20,
            905,
            1263,
            1216,
            1208,
            1004,
            381,
            512,
            386,
            1137,
            248,
            151,
            607,
            344,
            356,
            862,
            984,
            441,
            658,
            902,
            1314,
            974,
            407,
            466,
            201,
            291,
            550,
            511,
            187,
            1171,
            1358,
            413,
            85,
            178,
            311,
            1134,
            429,
            609,
            347,
            1448,
            434,
            1725,
            685,
            880,
            1580,
            186,
            202,
            27,
            1025,
            885,
            1042,
            1382,
            546,
            1013,
            690,
            667,
            822,
            581,
            265,
            127,
            916,
            361,
            322,
            753,
            504,
            651,
            1565,
            108,
            459,
            205,
            570,
            1172,
            160,
            4,
            763,
            339,
            1040,
            166,
            1168,
            990,
            319,
            1397,
            708,
            1077,
            928,
            516,
            96,
            1025,
            666,
            557,
            1605,
            1226,
            769,
            403,
            177,
            942,
            651,
            1572,
            476,
            1233,
            836,
            824,
            391,
            320,
            325,
            125,
            846,
            1770,
            293,
            1633,
            1198,
            768,
            70,
            117,
            1158,
            1055,
            635,
            322,
            275,
            1445,
            550,
            1242,
            597,
            538,
            59,
            819,
            179,
            1259,
            396,
            713,
            262,
            622,
            327,
            522,
            313,
            226,
            605,
            692,
            1386,
            304,
            1590,
            229,
            839,
            507,
            436,
            756,
            387,
            1265,
            1852,
            875,
            140,
            577,
            975,
            510,
            583,
            674,
            1125,
            747,
            1612,
            311,
            116,
            554,
            811,
            730,
            577,
            660,
            450,
            1223,
            1109,
            45,
            299,
            506,
            635,
            1045,
            393,
            650,
            433,
            839,
            1176,
            650,
            218,
            564,
            986,
            561,
            65,
            420,
            1365,
            619,
            849,
            611,
            953,
            642,
            1417,
            247,
            710,
            718,
            1100,
            59,
            938,
            825,
            286,
            1273,
            295,
            852,
            710,
            1277,
            129,
            690,
            701,
            309,
            213,
            1112,
            199,
            55,
            1230,
            1052,
            541,
            55,
            809,
            1738,
            404,
            262,
            759,
            42,
            397,
            964,
            233,
            1022,
            411,
            954,
            1528,
            416,
            638,
            231,
            574,
            148,
            730,
            564,
            1290,
            632,
            529,
            1273,
            667,
            1170,
            169,
            105,
            793,
            490,
            1201,
            246,
            417,
            1177,
            78,
            323,
            707,
            457,
            356,
            936,
            912,
            179,
            1914,
            526,
            156,
            344,
            865,
            1552,
            546,
            1097,
            372,
            1285,
            1242,
            302,
            594,
            303,
            1858,
            403,
            32,
            590,
            235,
            630,
            357,
            246,
            475,
            1054,
            555,
            607,
            1360,
            142,
            681,
            104,
            637,
            410,
            1830,
            802,
            311,
            56,
            344,
            1053,
            221,
            506,
            1728,
            63,
            418,
            854,
            178,
            1869,
            185,
            170,
            454,
            866,
            44,
            1331,
            1285,
            741,
            520,
            114,
            1153,
            474,
            39,
            419,
            584,
            312,
            19,
            207,
            363,
            209,
            159,
            649,
            1132,
            1392,
            1001,
            712,
            418,
            384,
            119,
            627,
            1086,
            195,
            14,
            745,
            253,
            333,
            629,
            707,
            1314,
            1715,
            513,
            13,
            1243,
            300,
            58,
            1107,
            134,
            275,
            176,
            1236,
            336,
            1116,
            1010,
            205,
            1453,
            788,
            1301,
            463,
            188,
            1028,
            1419,
            1471,
            168,
            1125,
            130,
            481,
            1411,
            79,
            1187,
            561,
            96,
            77,
            493,
            1173,
            964,
            634,
            639,
            127,
            431,
            71,
            111,
            356,
            798,
            747,
            408,
            373,
            1362,
            475,
            743,
            447,
            42,
            30,
            162,
            347,
            346,
            727,
            180,
            1641,
            269,
            250,
            1186,
            77,
            507,
            379,
            264,
            1486,
            694,
            579,
            495,
            315,
            329,
            548,
            531,
            191,
            823,
            96,
            841,
            195,
            21,
            235,
            1417,
            286,
            649,
            966,
            1172,
            270,
            781,
            418,
            126,
            228,
            1091,
            1452,
            674,
            398,
            449,
            1184,
            675,
            1792,
            177,
            1013,
            1058,
            592,
            810,
            533,
            558,
            681,
            1017,
            765,
            1176,
            947,
            1314,
            609,
            754,
            619,
            453,
            205,
            282,
            130,
            494,
            275,
            447,
            281,
            6,
            292,
            164,
            631,
            897,
            268,
            861,
            331,
            1064,
            509,
            622,
            957,
            494,
            236,
            980,
            1789,
            831,
            902,
            636,
            831,
            518,
            233,
            725,
            170,
            157,
            1142,
            840,
            401,
            395,
            48,
            982,
            478,
            581,
            1215,
            594,
            683,
            536,
            83,
            905,
            249,
            803,
            459,
            584,
            1596,
            418,
            364,
            946,
            186,
            1428,
            1613,
            1127,
            691,
            1802,
            751,
            859,
            187,
            206,
            1673,
            710,
            1331,
            263,
            97,
            129,
            1213,
            189,
            628,
            320,
            837,
            1087,
            781,
            251,
            262,
            767,
            1226,
            389,
            920,
            1027,
            271,
            369,
            1258,
            417,
            1345,
            613,
            912,
            666,
            236,
            186,
            416,
            430,
            168,
            1467,
            179,
            159,
            120,
            762,
            600,
            286,
            1350,
            27,
            1597,
            1177,
            402,
            577,
            52,
            364,
            934,
            250,
            214,
            612,
            1111,
            163,
            202,
            729,
            1079,
            361,
            338,
            1698,
            1052,
            1108,
            951,
            1197,
            473,
            1294,
            410,
            178,
            782,
            434,
            464,
            137,
            492,
            1250,
            385,
            427,
            1024,
            240,
            886,
            1233,
            183,
            88,
            1016,
            80,
            611,
            569,
            304,
            276,
            1131,
            551,
            436,
            553,
            1391,
            981,
            680,
            687,
            1060,
            217,
            1140,
            304,
            504,
            832,
            935,
            446,
            942,
            897,
            1208,
            159,
            632,
            1308,
            1002,
            1331,
            150,
            1248,
            108,
            320,
            437,
            295,
            1058,
            79,
            593,
            693,
            300,
            234,
            1545,
            59,
            1103,
            91,
            177,
            58,
            447,
            942,
            88,
            371,
            1446,
            181,
            1311,
            1791,
            1568,
            706,
            871,
            1074,
            501,
            271,
            920,
            1171,
            293,
            833,
            485,
            182,
            207,
            1698,
            165,
            1086,
            192,
            1156,
            377,
            709,
            729,
            543,
            553,
            526,
            637,
            113,
            1114,
            461,
            1430,
            304,
            471,
            806,
            80,
            204,
            545,
            112,
            1331,
            1055,
            609,
            290,
            1039,
            1485,
            1122,
            462,
            1384,
            1502,
            589,
            82,
            68,
            689,
            208,
            1339,
            722,
            717,
            433,
            1298,
            186,
            1397,
            618,
            1214,
            613,
            649,
            193,
            22,
            1464,
            280,
            690,
            39,
            337,
            1036,
            1347,
            22,
            327,
            778,
            482,
            591,
            229,
            901,
            715,
            172,
            681,
            867,
            286,
            451,
            1119,
            526,
            1299,
            217,
            1109,
            434,
            671,
            1404,
            856,
            381,
            85,
            562,
            1111,
            209,
            997,
            91,
            730,
            606,
            445,
            286,
            985,
            1617,
            375,
            1795,
            486,
            757,
            273,
            491,
            762,
            448,
            231,
            1713,
            438,
            68,
            698,
            510,
            965,
            1399,
            532,
            8,
            154,
            198,
            860,
            461,
            109,
            193,
            961,
            243,
            1626,
            480,
            123,
            400,
            190,
            626,
            1,
            548,
            632,
            349,
            1686,
            1455,
            95,
            580,
            297,
            951,
            667,
            1047,
            408,
            1150,
            333,
            641,
            653,
            103,
            517,
            785,
            531,
            564,
            761,
            1179,
            673,
            472,
            412,
            977,
            359,
            224,
            714,
            195,
            843,
            308,
            522,
            364,
            806,
            518,
            519,
            792,
            382,
            214,
            859,
            342,
            713,
            172,
            118,
            540,
            18,
            987,
            535,
            1092,
            327,
            517,
            994,
            491,
            268,
            210,
            144,
            729,
            82,
            188,
            150,
            1222,
            331,
            1178,
            347,
            75,
            492,
            65,
            881,
            351,
            1842,
            758,
            1323,
            1175,
            1092,
            647,
            1102,
            319,
            215,
            945,
            381,
            546,
            1096,
            575,
            519,
            342,
            291,
            1091,
            352,
            165,
            385,
            1920,
            396,
            1134,
            721,
            687,
            777,
            176,
            458,
            485,
            641,
            798,
            256,
            370,
            191,
            1571,
            276,
            573,
            160,
            90,
            138,
            983,
            275,
            149,
            42,
            793,
            625,
            521,
            1530,
            442,
            152,
            1242,
            29,
            501,
            87,
            648,
            521,
            120,
            985,
            341,
            751,
            993,
            43,
            1262,
            361,
            104,
            618,
            815,
            1587,
            138,
            584,
            750,
            820,
            8,
            171,
            960,
            160,
            612,
            709,
            624,
            538,
            1295,
            1164,
            829,
            199,
            825,
            519,
            169,
            1734,
            76,
            1698,
            819,
            387,
            985,
            970,
            767,
            512,
            56,
            590,
            462,
            126,
            578,
            655,
            1028,
            88,
            1692,
            358,
            64,
            259,
            236,
            873,
            858,
            1405,
            237,
            650,
            1949,
            541,
            361,
            335,
            1638,
            1109,
            111,
            1285,
            1480,
            322,
            1220,
            511,
            1011,
            629,
            140,
            1081,
            297,
            1512,
            275,
            1136,
            1145,
            477,
            954,
            741,
            547,
            1194,
            146,
            471,
            169,
            1895,
            1537,
            236,
            821,
            466,
            393,
            400,
            774,
            284,
            465,
            705,
            38,
            51,
            850,
            349,
            387,
            706,
            951,
            179,
            426,
            653,
            1578,
            280,
            251,
            1463,
            1432,
            477,
            1122,
            145,
            416,
            847,
            868,
            594,
            518,
            663,
            371,
            1749,
            586,
            409,
            35,
            1008,
            442,
            983,
            875,
            952,
            244,
            65,
            1749,
            716,
            433,
            134,
            179,
            804,
            787,
            704,
            991,
            379,
            489,
            1518,
            855,
            1261,
            115,
            1203,
            545,
            818,
            42,
            1103,
            168,
            719,
            163,
            1162,
            626,
            329,
            330,
            956,
            1140,
            217,
            271,
            444,
            1003,
            227,
            253,
            88,
            334,
            1281,
            168,
            898,
            46,
            118,
            1187,
            1191,
            526,
            346,
            89,
            137,
            1226,
            473,
            368,
            239,
            622,
            654,
            853,
            686,
            992,
            115,
            275,
            461,
            1428,
            614,
            193,
            1105,
            398,
            480,
            775,
            1482,
            154,
            454,
            737,
            419,
            853,
            1211,
            314,
            172,
            527,
            874,
            674,
            657,
            475,
            1209,
            15,
            293,
            813,
            173,
            457,
            533,
            912,
            798,
            412,
            130,
            201,
            299,
            1277,
            626,
            378,
            333,
            805,
            1901,
            1573,
            360,
            20,
            98,
            703,
            273,
            919,
            982,
            879,
            1287,
            1109,
            211,
            1058,
            532,
            290,
            890,
            327,
            1260,
            570,
            621,
            328,
            1307,
            497,
            219,
            1163,
            24,
            468,
            1272,
            503,
            1495,
            1333,
            267,
            149,
            1070,
            206,
            158,
            48,
            249,
            935,
            140,
            184,
            238,
            800,
            211,
            670,
            725,
            81,
            1211,
            1362,
            945,
            1043,
            444,
            311,
            812,
            202,
            531,
            404,
            954,
            274,
            956,
            1438,
            1446,
            692,
            777,
            1157,
            454,
            478,
            217,
            468,
            1107,
            626,
            485,
            43,
            322,
            880,
            1511,
            667,
            765,
            614,
            925,
            1185,
            1706,
            347,
            1603,
            806,
            1207,
            431,
            179,
            423,
            1131,
            925,
            62,
            800,
            211,
            81,
            667,
            405,
            678,
            1619,
            935,
            864,
            1495,
            870,
            1379,
            1055,
            220,
            419,
            1111,
            1097,
            447,
            273,
            590,
            116,
            306,
            300,
            1259,
            1573,
            1392,
            113,
            526,
            1209,
            655,
            1123,
            692,
            796,
            1021,
            995,
            1244,
            293,
            1240,
            759,
            398,
            358,
            403,
            211,
            194,
            1076,
            1238,
            229,
            327,
            135,
            5,
            715,
            511,
            549,
            420,
            838,
            314,
            739,
            62,
            3,
            963,
            1438,
            1621,
            19,
            887,
            567,
            584,
            496,
            1245,
            682,
            724,
            1384,
            257,
            17,
            983,
            439,
            925,
            409,
            293,
            384,
            149,
            447,
            717,
            195,
            153,
            1699,
            501,
            530,
            396,
            670,
            193,
            351,
            846,
            111,
            1213,
            755,
            1406,
            497,
            1826,
            50,
            817,
            901,
            212,
            329,
            837,
            248,
            632,
            366,
            174,
            882,
            688,
            263,
            118,
            1012,
            586,
            294,
            818,
            426,
            151,
            409,
            641,
            958,
            1328,
            354,
            1709,
            1099,
            783,
            194,
            238,
            919,
            758,
            1039,
            1054,
            122,
            100,
            1221,
            312,
            599,
            1274,
            62,
            315,
            1123,
            1878,
            318,
            574,
            1353,
            86,
            340,
            602,
            963,
            127,
            142,
            274,
            371,
            788,
            395,
            643,
            854,
            1335,
            1541,
            382,
            846,
            960,
            154,
            468,
            1094,
            272,
            165,
            394,
            629,
            831,
            903,
            923,
            362,
            693,
            1250,
            1135,
            66,
            924,
            204,
            376,
            1083,
            229,
            875,
            35,
            18,
            103,
            105,
            168,
            18,
            659,
            534,
            285,
            482,
            1571,
            605,
            158,
            1573,
            127,
            343,
            289,
            1703,
            1482,
            907,
            491,
            287,
            1151,
            1024,
            1085,
            991,
            1052,
            1078,
            98,
            396,
            323,
            410,
            1750,
            1475,
            1836,
            283,
            539,
            440,
            397,
            204,
            347,
            1003,
            0,
            264,
            543,
            467,
            875,
            1116,
            1325,
            1313,
            552,
            335,
            50,
            593,
            268,
            382,
            443,
            576,
            596,
            115,
            356,
            10,
            1399,
            68,
            643,
            809,
            683,
            56,
            492,
            699,
            290,
            23,
            394,
            713,
            1063,
            1421,
            623,
            83,
            1563,
            777,
            83,
            203,
            938,
            1077,
            773,
            332,
            570,
            742,
            1420,
            1801,
            1693,
            31,
            1098,
            1765,
            887,
            760,
            486,
            2,
            202,
            90,
            469,
            86,
            1180,
            140,
            1100,
            194,
            440,
            1563,
            163,
            192,
            837,
            356,
            155,
            1032,
            40,
            1194,
            1179,
            902,
            58,
            1385,
            614,
            672,
            1088,
            499,
            157,
            485,
            571,
            431,
            499,
            563,
            944,
            1138,
            161,
            126,
            15,
            500,
            203,
            961,
            608,
            277,
            114,
            630,
            404,
            248,
            225,
            699,
            649,
            416,
            1310,
            930,
            835,
            664,
            460,
            778,
            225,
            785,
            1581,
            9,
            7,
            174,
            437,
            278,
            1288,
            979,
            56,
            913,
            282,
            322,
            1252,
            1128,
            606,
            655,
            954,
            68,
            134,
            922,
            1874,
            251,
            1362,
            281,
            224,
            778,
            546,
            397,
            386,
            214,
            461,
            125,
            335,
            342,
            625,
            494,
            1145,
            600,
            484,
            199,
            21,
            885,
            308,
            1015,
            1303,
            406,
            124,
            150,
            108,
            90,
            588,
            1028,
            776,
            278,
            897,
            651,
            1010,
            737,
            1399,
            376,
            1117,
            234,
            691,
            380,
            603,
            117,
            766,
            1499,
            1175,
            988,
            190,
            674,
            518,
            1618,
            857,
            371,
            73,
            98,
            395,
            1064,
            712,
            774,
            1012,
            730,
            777,
            973,
            245,
            937,
            937,
            574,
            678,
            82,
            398,
            1096,
            1419,
            157,
            430,
            1524,
            1649,
            254,
            1790,
            1438,
            1304,
            813,
            948,
            1161,
            867,
            181,
            754,
            242,
            405,
            1569,
            105,
            978,
            725,
            657,
            1147,
            910,
            295,
            26,
            332,
            519,
            161,
            1080,
            649,
            71,
            744,
            437,
            115,
            547,
            495,
            1466,
            117,
            760,
            764,
            776,
            53,
            371,
            728,
            1791,
            606,
            1225,
            419,
            367,
            1230,
            267,
            273,
            705,
            214,
            424,
            84,
            605,
            1175,
            657,
            371,
            833,
            102,
            1299,
            258,
            56,
            443,
            1279,
            313,
            523,
            919,
            570,
            380,
            119,
            296,
            358,
            1364,
            424,
            961,
            431,
            950,
            647,
            1960,
            116,
            306,
            808,
            94,
            380,
            593,
            1684,
            1221,
            410,
            600,
            189,
            308,
            148,
            297,
            1050,
            1191,
            1399,
            1658,
            1153,
            35,
            553,
            46,
            207,
            67,
            818,
            1257,
            138,
            11,
            889,
            831,
            713,
            527,
            1284,
            911,
            1047,
            123,
            406,
            927,
            407,
            1041,
            781,
            1191,
            103,
            125,
            919,
            536,
            638,
            639,
            496,
            686,
            1064,
            240,
            525,
            348,
            1262,
            1899,
            258,
            1188,
            730,
            484,
            547,
            800,
            347,
            679,
            1224,
            948,
            366,
            223,
            1619,
            332,
            167,
            782,
            881,
            91,
            487,
            120,
            1626,
            74,
            634,
            334,
            931,
            921,
            805,
            1057,
            616,
            740,
            542,
            344,
            211,
            79,
            887,
            670,
            383,
            718,
            144,
            1442,
            291,
            1352,
            209,
            861,
            426,
            266,
            523,
            40,
            262,
            623,
            4,
            1001,
            334,
            453,
            952,
            321,
            1451,
            1048,
            398,
            417,
            470,
            1217,
            298,
            36,
            85,
            655,
            167,
            212,
            617,
            271,
            159,
            786,
            46,
            645,
            1371,
            881,
            449,
            338,
            1332,
            119,
            256,
            428,
            1546,
            1210,
            789,
            390,
            110,
            1330,
            674,
            253,
            317,
            183,
            892,
            388,
            52,
            914,
            287,
            773,
            675,
            367,
            510,
            1325,
            1661,
            665,
            662,
            0,
            284,
            661,
            757,
            560,
            470,
            146,
            1315,
            533,
            560,
            405,
            227,
            479,
            227,
            149,
            88,
            114,
            662,
            441,
            888,
            664,
            64,
            747,
            956,
            776,
            652,
            1168,
            201,
            724,
            599,
            3,
            1652,
            190,
            1171,
            1081,
            1211,
            976,
            675,
            455,
            42,
            1196,
            503,
            1694,
            306,
            838,
            291,
            173,
            1017,
            698,
            610,
            223,
            206,
            1457,
            742,
            591,
            347,
            241,
            743,
            4,
            576,
            888,
            528,
            1222,
            915,
            868,
            336,
            483
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "px_width",
           "values": [
            756,
            1988,
            1716,
            1786,
            1212,
            1654,
            1018,
            1149,
            836,
            1224,
            874,
            1005,
            748,
            1440,
            563,
            1864,
            1850,
            810,
            878,
            1064,
            1854,
            1385,
            822,
            788,
            1245,
            1434,
            645,
            1075,
            1311,
            1263,
            1739,
            654,
            1152,
            1919,
            881,
            1249,
            815,
            1307,
            730,
            1613,
            967,
            1932,
            714,
            1456,
            1652,
            1810,
            1791,
            774,
            1433,
            1854,
            1832,
            1383,
            629,
            1287,
            804,
            1036,
            1449,
            820,
            713,
            683,
            969,
            975,
            547,
            1353,
            1570,
            1618,
            1858,
            1781,
            1225,
            603,
            1724,
            1217,
            1026,
            638,
            1413,
            1242,
            1071,
            778,
            1552,
            1486,
            1206,
            1616,
            1752,
            1122,
            1049,
            1894,
            741,
            1118,
            1203,
            1402,
            1924,
            1242,
            802,
            1105,
            1990,
            1179,
            891,
            1684,
            961,
            1317,
            1739,
            881,
            984,
            520,
            902,
            1504,
            1634,
            1796,
            1846,
            1869,
            1471,
            874,
            1974,
            513,
            1244,
            1661,
            973,
            771,
            989,
            1954,
            1338,
            1712,
            863,
            1376,
            1203,
            902,
            786,
            1965,
            1732,
            1398,
            829,
            792,
            580,
            1056,
            1264,
            1248,
            669,
            1686,
            1539,
            1042,
            1926,
            616,
            1131,
            1697,
            536,
            786,
            1269,
            1298,
            1967,
            1025,
            800,
            1359,
            1996,
            1013,
            590,
            1925,
            1989,
            826,
            1983,
            1545,
            1533,
            621,
            1272,
            1729,
            1071,
            974,
            1554,
            1611,
            1937,
            1942,
            845,
            627,
            842,
            1737,
            1199,
            1740,
            648,
            1136,
            1220,
            879,
            1065,
            831,
            1191,
            1631,
            588,
            500,
            1698,
            1299,
            898,
            1381,
            984,
            1533,
            1464,
            539,
            1179,
            751,
            1497,
            1215,
            1948,
            1542,
            1235,
            1345,
            589,
            1182,
            1052,
            1429,
            873,
            836,
            1478,
            1460,
            705,
            1666,
            1077,
            1020,
            1235,
            1516,
            1499,
            583,
            1958,
            1995,
            642,
            1178,
            1404,
            1040,
            1453,
            1677,
            517,
            1560,
            1226,
            1200,
            1647,
            1308,
            1615,
            687,
            637,
            1606,
            1543,
            980,
            1441,
            1383,
            1009,
            1754,
            860,
            1543,
            1108,
            673,
            1758,
            886,
            1485,
            1038,
            946,
            1462,
            654,
            1604,
            1882,
            1816,
            765,
            1398,
            1980,
            1559,
            1928,
            1529,
            952,
            1517,
            1741,
            1596,
            1564,
            1222,
            692,
            1617,
            1571,
            1247,
            1767,
            714,
            1935,
            1933,
            1509,
            954,
            662,
            888,
            527,
            820,
            1493,
            1929,
            764,
            1092,
            1891,
            1699,
            683,
            1664,
            763,
            572,
            1963,
            1489,
            1294,
            997,
            655,
            1458,
            1243,
            519,
            1767,
            566,
            515,
            1168,
            882,
            1942,
            776,
            564,
            1975,
            1212,
            1118,
            1388,
            1462,
            1130,
            754,
            1726,
            1876,
            709,
            557,
            1914,
            775,
            1910,
            1923,
            1370,
            994,
            1078,
            724,
            1104,
            1182,
            1795,
            1224,
            1442,
            1023,
            1361,
            652,
            935,
            1345,
            1292,
            1256,
            1364,
            1786,
            1429,
            682,
            1199,
            1688,
            1898,
            1438,
            1463,
            1584,
            1059,
            1253,
            1442,
            939,
            986,
            1029,
            1280,
            898,
            1381,
            1565,
            1299,
            1726,
            1331,
            1922,
            1038,
            992,
            1186,
            1920,
            1866,
            981,
            1195,
            1706,
            749,
            1711,
            681,
            1974,
            1510,
            814,
            1025,
            1247,
            1814,
            1233,
            642,
            721,
            1670,
            550,
            699,
            1138,
            1903,
            817,
            1126,
            1139,
            926,
            1506,
            726,
            1753,
            794,
            1161,
            1350,
            619,
            660,
            1468,
            1705,
            1476,
            1666,
            1026,
            1033,
            1529,
            559,
            973,
            1223,
            956,
            1797,
            882,
            596,
            574,
            990,
            1053,
            858,
            758,
            1495,
            1104,
            1352,
            1304,
            1205,
            562,
            585,
            1441,
            1075,
            1629,
            1975,
            1798,
            545,
            1657,
            1341,
            817,
            1058,
            1203,
            1744,
            740,
            1133,
            889,
            1719,
            1285,
            1850,
            1905,
            1158,
            1325,
            772,
            1743,
            882,
            1208,
            1050,
            1366,
            899,
            1224,
            1131,
            1884,
            1370,
            1497,
            867,
            681,
            884,
            1358,
            998,
            1989,
            862,
            1785,
            799,
            793,
            695,
            728,
            1940,
            1304,
            1078,
            1259,
            790,
            1594,
            548,
            1256,
            1911,
            1649,
            1264,
            1262,
            1972,
            1484,
            1090,
            1259,
            1439,
            592,
            1777,
            870,
            1106,
            948,
            1393,
            982,
            1586,
            1579,
            644,
            1157,
            539,
            750,
            1472,
            1337,
            1663,
            861,
            773,
            1313,
            1849,
            1613,
            1259,
            1151,
            1807,
            1763,
            1029,
            952,
            654,
            1827,
            1916,
            1924,
            1580,
            1942,
            1582,
            1234,
            517,
            1917,
            1759,
            939,
            1463,
            848,
            1803,
            915,
            1354,
            1143,
            632,
            899,
            1642,
            1450,
            1463,
            1013,
            1587,
            1665,
            1815,
            739,
            963,
            1508,
            1000,
            544,
            1627,
            759,
            1416,
            1088,
            1596,
            760,
            857,
            1100,
            676,
            658,
            1239,
            1901,
            1813,
            1578,
            1878,
            1864,
            1109,
            1171,
            1949,
            1722,
            1920,
            1809,
            964,
            1237,
            1082,
            828,
            1241,
            545,
            1710,
            1793,
            1463,
            1100,
            1951,
            1704,
            1897,
            809,
            1306,
            1771,
            1536,
            1509,
            1545,
            1987,
            1022,
            1337,
            522,
            1883,
            1787,
            910,
            781,
            1501,
            1176,
            1285,
            583,
            684,
            1838,
            904,
            1552,
            1854,
            559,
            1682,
            1383,
            655,
            1715,
            906,
            637,
            1230,
            1315,
            599,
            1302,
            727,
            1787,
            1096,
            1173,
            816,
            1970,
            1101,
            1654,
            1191,
            1089,
            1033,
            1781,
            901,
            1701,
            1308,
            1232,
            630,
            1184,
            1991,
            1256,
            1917,
            510,
            1511,
            1813,
            747,
            1947,
            935,
            1421,
            1652,
            672,
            1031,
            527,
            739,
            1596,
            575,
            1364,
            1478,
            1282,
            1353,
            1435,
            1651,
            516,
            1000,
            1754,
            1703,
            1935,
            1898,
            1832,
            721,
            1048,
            1873,
            1465,
            1477,
            1748,
            1383,
            1159,
            1630,
            857,
            1293,
            1162,
            1747,
            736,
            1418,
            757,
            1750,
            549,
            818,
            974,
            938,
            1505,
            1116,
            991,
            577,
            1374,
            1251,
            1747,
            1674,
            800,
            1016,
            1327,
            501,
            1621,
            858,
            1532,
            1294,
            810,
            811,
            1318,
            1655,
            1702,
            1013,
            1883,
            1862,
            1301,
            1605,
            718,
            699,
            1742,
            1558,
            766,
            1799,
            1658,
            1666,
            1998,
            1491,
            1891,
            1262,
            650,
            907,
            1234,
            519,
            1595,
            895,
            1589,
            1564,
            1384,
            1611,
            1733,
            1634,
            922,
            1291,
            874,
            1089,
            1604,
            1162,
            1648,
            910,
            723,
            999,
            1300,
            819,
            1238,
            643,
            1633,
            735,
            1226,
            1339,
            1445,
            1424,
            883,
            1452,
            806,
            849,
            1812,
            947,
            1605,
            1916,
            1895,
            1123,
            1409,
            1484,
            1005,
            1675,
            1724,
            1958,
            1930,
            1518,
            1704,
            589,
            859,
            821,
            1671,
            1865,
            822,
            1018,
            829,
            522,
            1009,
            1564,
            1448,
            896,
            605,
            1056,
            1330,
            670,
            1909,
            1185,
            1008,
            1642,
            1668,
            1482,
            1229,
            823,
            1326,
            932,
            926,
            1641,
            660,
            1051,
            1889,
            1982,
            893,
            1163,
            589,
            962,
            697,
            1312,
            1862,
            1469,
            952,
            704,
            671,
            1583,
            809,
            1151,
            791,
            1391,
            1336,
            1638,
            690,
            1073,
            643,
            1660,
            1395,
            783,
            1208,
            799,
            1145,
            1169,
            563,
            1360,
            1423,
            1152,
            1577,
            1181,
            1228,
            1523,
            867,
            774,
            1513,
            812,
            580,
            783,
            662,
            1081,
            697,
            1406,
            1001,
            1995,
            1500,
            530,
            832,
            1026,
            672,
            1267,
            670,
            928,
            1897,
            1382,
            1011,
            1929,
            1076,
            688,
            1403,
            734,
            1234,
            1159,
            1968,
            1165,
            1481,
            1569,
            1554,
            1876,
            1793,
            1367,
            1125,
            1913,
            1203,
            828,
            1155,
            618,
            1545,
            670,
            563,
            1293,
            1084,
            1723,
            831,
            1933,
            1980,
            1469,
            1514,
            937,
            1119,
            1556,
            1263,
            1819,
            1638,
            1435,
            1394,
            1197,
            1108,
            1956,
            1026,
            614,
            516,
            1334,
            806,
            1230,
            687,
            558,
            507,
            1670,
            765,
            1190,
            1744,
            1248,
            714,
            1973,
            1670,
            506,
            736,
            1019,
            674,
            1409,
            1420,
            774,
            1377,
            1517,
            1663,
            1538,
            511,
            541,
            940,
            1666,
            1659,
            1330,
            983,
            772,
            1366,
            1219,
            507,
            1735,
            1291,
            719,
            1915,
            917,
            730,
            1808,
            1908,
            1977,
            1452,
            1972,
            930,
            1190,
            1968,
            1165,
            1877,
            1997,
            671,
            1284,
            974,
            1759,
            1465,
            760,
            661,
            1304,
            698,
            1143,
            1782,
            1445,
            1261,
            1902,
            1782,
            1667,
            1040,
            1960,
            1394,
            1591,
            1910,
            890,
            864,
            1994,
            823,
            1552,
            1463,
            1910,
            1392,
            510,
            1427,
            1731,
            875,
            1348,
            616,
            1263,
            635,
            622,
            1767,
            653,
            1716,
            570,
            1813,
            1356,
            1553,
            1985,
            1471,
            705,
            1208,
            906,
            663,
            1916,
            1976,
            1761,
            1520,
            915,
            1768,
            1096,
            631,
            843,
            519,
            1152,
            1810,
            1682,
            739,
            1005,
            676,
            1595,
            1623,
            1178,
            772,
            1964,
            1413,
            1963,
            1795,
            1801,
            1992,
            1649,
            571,
            1746,
            1903,
            1055,
            893,
            877,
            1088,
            830,
            1805,
            541,
            1781,
            722,
            646,
            1309,
            1758,
            1079,
            1087,
            982,
            1191,
            1361,
            1055,
            1767,
            1626,
            1661,
            521,
            1247,
            1564,
            1515,
            822,
            1638,
            932,
            710,
            1883,
            1401,
            1383,
            1791,
            1432,
            1300,
            1090,
            1262,
            1637,
            1068,
            755,
            874,
            1531,
            1348,
            656,
            1434,
            1292,
            1983,
            1623,
            869,
            1555,
            1827,
            772,
            1764,
            1046,
            594,
            1896,
            1320,
            1687,
            1069,
            1170,
            1595,
            1347,
            1324,
            966,
            1538,
            846,
            1389,
            574,
            607,
            1636,
            709,
            882,
            1781,
            1305,
            1433,
            1352,
            1966,
            1178,
            1500,
            1212,
            989,
            1602,
            1263,
            728,
            1607,
            1490,
            550,
            1762,
            1455,
            556,
            1737,
            1229,
            1606,
            1239,
            734,
            1264,
            864,
            1014,
            500,
            1411,
            525,
            1967,
            1377,
            1843,
            652,
            797,
            1043,
            1418,
            565,
            887,
            656,
            874,
            1352,
            1195,
            751,
            639,
            1804,
            1920,
            1581,
            912,
            745,
            511,
            1011,
            1278,
            1344,
            1291,
            896,
            1892,
            1247,
            684,
            1109,
            1247,
            773,
            1019,
            645,
            1788,
            1422,
            1558,
            1858,
            1767,
            618,
            1037,
            1554,
            759,
            571,
            1469,
            551,
            1579,
            1838,
            1841,
            876,
            1079,
            620,
            747,
            1012,
            522,
            1499,
            768,
            740,
            807,
            1089,
            507,
            804,
            1335,
            1544,
            1378,
            1378,
            1923,
            1702,
            739,
            1796,
            1456,
            1884,
            707,
            898,
            1609,
            603,
            1723,
            1593,
            1988,
            1057,
            1032,
            1951,
            1473,
            1477,
            1106,
            741,
            1656,
            1134,
            922,
            627,
            1385,
            1145,
            1756,
            711,
            1899,
            679,
            1252,
            1387,
            1886,
            1477,
            1930,
            860,
            1539,
            1144,
            613,
            909,
            1743,
            1469,
            544,
            873,
            947,
            1389,
            798,
            519,
            1676,
            1651,
            1147,
            1011,
            1688,
            1613,
            1469,
            1738,
            1838,
            1261,
            1362,
            1461,
            819,
            1062,
            997,
            598,
            564,
            1316,
            1746,
            1694,
            1783,
            1524,
            1728,
            1678,
            828,
            1622,
            979,
            1052,
            1958,
            1310,
            1504,
            970,
            1931,
            1858,
            1528,
            791,
            1614,
            1409,
            1437,
            1472,
            1418,
            1004,
            1683,
            1892,
            1744,
            1163,
            621,
            1383,
            784,
            1066,
            1400,
            1766,
            846,
            1662,
            1520,
            1923,
            1773,
            501,
            1274,
            986,
            676,
            974,
            1309,
            888,
            794,
            1454,
            1654,
            1084,
            1210,
            1306,
            1206,
            603,
            952,
            1878,
            1285,
            568,
            1206,
            533,
            696,
            1850,
            640,
            1496,
            1378,
            1552,
            887,
            1769,
            855,
            1167,
            1386,
            1284,
            1517,
            627,
            1836,
            1356,
            941,
            1793,
            1269,
            1039,
            1182,
            884,
            1162,
            613,
            518,
            1195,
            1591,
            1345,
            764,
            1226,
            1552,
            1233,
            1827,
            865,
            1133,
            644,
            1156,
            967,
            1970,
            530,
            1781,
            1553,
            1227,
            1393,
            1893,
            1076,
            891,
            1273,
            1393,
            686,
            1708,
            1804,
            1376,
            1073,
            1340,
            581,
            1796,
            1709,
            1981,
            831,
            1267,
            1915,
            1681,
            1728,
            1156,
            1062,
            507,
            1039,
            1079,
            682,
            1007,
            1027,
            1527,
            935,
            1873,
            1619,
            1163,
            1144,
            1925,
            941,
            694,
            1496,
            870,
            1103,
            1301,
            1299,
            1713,
            910,
            1759,
            1250,
            850,
            1435,
            1923,
            1448,
            1877,
            1132,
            1957,
            1492,
            1473,
            1316,
            991,
            903,
            646,
            588,
            540,
            1004,
            730,
            855,
            512,
            1098,
            1717,
            1193,
            1657,
            1639,
            620,
            1431,
            724,
            1880,
            1591,
            925,
            1506,
            627,
            1723,
            1363,
            1134,
            1803,
            1897,
            1564,
            977,
            1648,
            978,
            1551,
            1857,
            1988,
            1873,
            676,
            1240,
            563,
            800,
            1021,
            727,
            1284,
            1987,
            519,
            703,
            675,
            925,
            1724,
            1945,
            1331,
            1802,
            679,
            1105,
            1327,
            1010,
            1364,
            892,
            884,
            679,
            636,
            1407,
            1567,
            1684,
            584,
            790,
            1988,
            1349,
            527,
            818,
            1492,
            804,
            826,
            849,
            1742,
            1205,
            1463,
            816,
            816,
            1692,
            1995,
            559,
            915,
            1226,
            1183,
            881,
            866,
            1645,
            821,
            1539,
            1923,
            1790,
            1536,
            1373,
            1786,
            1775,
            1964,
            1032,
            660,
            1787,
            1614,
            853,
            1546,
            1350,
            1046,
            1437,
            892,
            569,
            1803,
            1011,
            1311,
            1405,
            588,
            663,
            1776,
            1435,
            1727,
            1262,
            969,
            963,
            1780,
            1617,
            1088,
            1718,
            695,
            763,
            1399,
            817,
            542,
            1090,
            1055,
            1003,
            1491,
            647,
            1575,
            1196,
            1713,
            1771,
            1950,
            1970,
            1155,
            1849,
            738,
            1742,
            755,
            1234,
            851,
            895,
            1658,
            1420,
            1983,
            1175,
            1806,
            1583,
            1615,
            1545,
            1118,
            1713,
            1944,
            1294,
            1175,
            638,
            623,
            1372,
            1190,
            1931,
            1429,
            710,
            710,
            1963,
            1702,
            688,
            1459,
            970,
            1042,
            1197,
            1836,
            1976,
            599,
            1853,
            1159,
            1994,
            844,
            1424,
            891,
            1046,
            1970,
            833,
            675,
            1009,
            1641,
            761,
            509,
            1487,
            1042,
            711,
            679,
            832,
            1811,
            1208,
            1947,
            1814,
            1036,
            833,
            1677,
            1963,
            557,
            1809,
            1196,
            1247,
            584,
            1103,
            700,
            1684,
            1759,
            1646,
            1288,
            1330,
            1947,
            1028,
            1550,
            1483,
            681,
            877,
            1952,
            1578,
            1503,
            1905,
            1375,
            1374,
            1916,
            989,
            665,
            583,
            755,
            1853,
            1540,
            1726,
            939,
            1040,
            1148,
            1090,
            1633,
            829,
            1083,
            1164,
            1171,
            1630,
            512,
            1775,
            1570,
            1698,
            770,
            1207,
            1848,
            1829,
            954,
            1988,
            1832,
            1673,
            1180,
            1007,
            1288,
            1258,
            1199,
            1532,
            1127,
            742,
            1763,
            1099,
            1986,
            1882,
            1657,
            1393,
            1566,
            503,
            508,
            764,
            533,
            1029,
            1289,
            1290,
            1686,
            1244,
            1113,
            1111,
            957,
            1035,
            1717,
            1115,
            1722,
            1873,
            1005,
            1504,
            1702,
            818,
            1896,
            1316,
            1717,
            736,
            1264,
            1263,
            1161,
            891,
            1905,
            679,
            1032,
            793,
            1429,
            1586,
            938,
            1023,
            1441,
            1195,
            1935,
            1889,
            1150,
            708,
            1793,
            712,
            565,
            1482,
            1021,
            682,
            529,
            839,
            614,
            1851,
            1639,
            1030,
            1727,
            1739,
            1849,
            1963,
            1432,
            538,
            1517,
            1334,
            877,
            732,
            1746,
            1415,
            1643,
            1211,
            1411,
            591,
            742,
            918,
            1726,
            1882,
            1663,
            1794,
            1817,
            510,
            874,
            908,
            1177,
            1924,
            1006,
            1411,
            1371,
            1020,
            1635,
            1709,
            1854,
            1393,
            1643,
            1220,
            1534,
            1477,
            681,
            1135,
            528,
            1430,
            1364,
            1839,
            1760,
            1227,
            1343,
            1656,
            1831,
            1087,
            1804,
            1418,
            1985,
            1414,
            704,
            730,
            1520,
            1904,
            789,
            1948,
            751,
            1238,
            1852,
            1455,
            513,
            1231,
            1676,
            1205,
            1234,
            1777,
            1962,
            970,
            909,
            951,
            1129,
            1317,
            722,
            1429,
            1913,
            865,
            1079,
            1079,
            1811,
            1351,
            956,
            1931,
            912,
            840,
            678,
            1551,
            1608,
            615,
            893,
            1152,
            1249,
            1109,
            896,
            1904,
            1318,
            1775,
            725,
            1888,
            1629,
            642,
            542,
            1311,
            1436,
            769,
            1351,
            1176,
            896,
            963,
            1726,
            1870,
            1469,
            1123,
            724,
            520,
            844,
            1670,
            1037,
            642,
            1451,
            1255,
            990,
            623,
            1386,
            1769,
            1738,
            872,
            562,
            1275,
            1677,
            1382,
            790,
            534,
            1814,
            1531,
            823,
            1254,
            1879,
            1989,
            1211,
            756,
            1317,
            1686,
            1455,
            590,
            1805,
            661,
            1603,
            605,
            539,
            979,
            593,
            1353,
            1163,
            1985,
            980,
            1800,
            1836,
            718,
            997,
            994,
            1036,
            1951,
            1912,
            1177,
            775,
            822,
            1710,
            1696,
            1633,
            1141,
            610,
            831,
            509,
            1022,
            709,
            819,
            874,
            1721,
            1466,
            711,
            745,
            1247,
            1010,
            1397,
            1933,
            1179,
            582,
            1127,
            1299,
            629,
            1983,
            657,
            1673,
            1979,
            1396,
            1353,
            742,
            537,
            807,
            1651,
            986,
            1798,
            558,
            885,
            651,
            1219,
            1289,
            1018,
            1437,
            737,
            1167,
            1919,
            999,
            724,
            957,
            854,
            1426,
            743,
            1809,
            1099,
            1416,
            1890,
            1965,
            1632,
            670,
            754
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "ram",
           "values": [
            2549,
            2631,
            2603,
            2769,
            1411,
            1067,
            3220,
            700,
            1099,
            513,
            3946,
            3826,
            1482,
            2680,
            373,
            568,
            3554,
            3752,
            1835,
            2337,
            2819,
            3283,
            1433,
            1037,
            2583,
            2782,
            3763,
            3286,
            2373,
            478,
            3532,
            508,
            2227,
            3845,
            1262,
            1326,
            2113,
            3429,
            3169,
            2150,
            2484,
            3339,
            1878,
            1629,
            504,
            1152,
            3587,
            2296,
            1270,
            3238,
            2059,
            2053,
            3112,
            1440,
            2908,
            2552,
            905,
            3963,
            2056,
            2910,
            1457,
            431,
            470,
            2148,
            2955,
            3366,
            3068,
            3834,
            1050,
            3993,
            3378,
            2192,
            392,
            3709,
            590,
            1814,
            907,
            1449,
            3448,
            1837,
            3464,
            2593,
            3484,
            2009,
            2048,
            837,
            854,
            3210,
            2746,
            2334,
            2822,
            1971,
            1410,
            349,
            1418,
            3616,
            880,
            1601,
            1412,
            1692,
            2600,
            1308,
            2413,
            1047,
            1204,
            1799,
            2676,
            2597,
            3029,
            2343,
            2016,
            2213,
            790,
            3182,
            3472,
            2871,
            3187,
            1945,
            488,
            1214,
            2598,
            3242,
            3534,
            3426,
            2488,
            3255,
            2863,
            1496,
            3801,
            666,
            3799,
            1366,
            2962,
            2399,
            2338,
            2700,
            2235,
            3825,
            284,
            1427,
            1324,
            398,
            3685,
            2390,
            278,
            3556,
            2196,
            3799,
            2268,
            2965,
            2341,
            3557,
            1354,
            2147,
            505,
            1394,
            374,
            506,
            3702,
            1078,
            2945,
            2981,
            490,
            536,
            2317,
            3704,
            2126,
            2050,
            2462,
            1260,
            3770,
            1886,
            3323,
            2060,
            3926,
            2177,
            728,
            2478,
            2842,
            1617,
            1472,
            764,
            2337,
            3965,
            3693,
            797,
            1687,
            2940,
            1545,
            3834,
            2505,
            2243,
            3600,
            3971,
            2844,
            2227,
            1665,
            3355,
            1866,
            1287,
            1046,
            1441,
            690,
            2504,
            2677,
            2243,
            2984,
            1362,
            3629,
            3559,
            2571,
            2107,
            1022,
            2844,
            711,
            3117,
            978,
            1869,
            1179,
            3844,
            593,
            588,
            3836,
            2669,
            2958,
            3031,
            3388,
            2377,
            2458,
            2766,
            3054,
            3791,
            625,
            2227,
            3256,
            707,
            1457,
            3755,
            1733,
            720,
            3560,
            2311,
            3846,
            1717,
            3704,
            3800,
            278,
            1767,
            1846,
            829,
            2986,
            2394,
            2169,
            1181,
            575,
            2977,
            532,
            1702,
            1074,
            3352,
            1027,
            2039,
            461,
            527,
            3271,
            606,
            2513,
            1675,
            2698,
            1129,
            468,
            1210,
            3086,
            1595,
            2332,
            2589,
            3760,
            1851,
            417,
            294,
            3576,
            3945,
            927,
            2044,
            2334,
            1477,
            3161,
            3660,
            1604,
            2211,
            3822,
            3922,
            1203,
            2822,
            447,
            1652,
            3608,
            1228,
            666,
            1499,
            3321,
            1206,
            952,
            3927,
            311,
            955,
            1109,
            2969,
            725,
            1655,
            3490,
            1774,
            3615,
            714,
            2768,
            3739,
            610,
            715,
            663,
            1725,
            834,
            3865,
            3059,
            651,
            796,
            785,
            3252,
            1322,
            2832,
            1615,
            2125,
            990,
            1595,
            1213,
            2500,
            2801,
            3451,
            1201,
            3139,
            1938,
            3185,
            2111,
            2369,
            3625,
            1051,
            1036,
            2938,
            2855,
            2727,
            2973,
            2581,
            2690,
            916,
            2563,
            3359,
            941,
            2280,
            827,
            473,
            3865,
            1274,
            364,
            3699,
            2927,
            1470,
            1050,
            2616,
            2915,
            2686,
            1243,
            3264,
            2261,
            2993,
            706,
            2542,
            2523,
            336,
            2457,
            418,
            3917,
            1246,
            2971,
            2324,
            3121,
            3801,
            3333,
            3475,
            2360,
            2895,
            3652,
            3097,
            2101,
            3918,
            3869,
            732,
            3684,
            3969,
            1414,
            1892,
            3131,
            2532,
            3215,
            3323,
            1529,
            2403,
            3419,
            3648,
            1974,
            2806,
            2239,
            582,
            3890,
            1141,
            2511,
            3838,
            2734,
            2330,
            2244,
            1851,
            808,
            1587,
            2492,
            2122,
            1122,
            3709,
            3021,
            624,
            3937,
            1797,
            1458,
            3348,
            1834,
            2775,
            1277,
            457,
            438,
            3210,
            985,
            3701,
            1052,
            1175,
            1070,
            3411,
            1175,
            301,
            489,
            1733,
            2756,
            2612,
            1724,
            2190,
            1900,
            3915,
            1043,
            3817,
            2362,
            1107,
            2982,
            2042,
            1343,
            2610,
            2335,
            1614,
            1138,
            604,
            3652,
            3955,
            258,
            1066,
            1731,
            3860,
            3630,
            1965,
            1459,
            2323,
            3291,
            3272,
            918,
            3488,
            3961,
            590,
            3701,
            463,
            3644,
            1713,
            429,
            2765,
            2084,
            1125,
            2048,
            1223,
            3672,
            3100,
            343,
            1970,
            2728,
            3635,
            2180,
            2488,
            3864,
            1155,
            3566,
            864,
            869,
            591,
            1333,
            2208,
            2610,
            2819,
            1955,
            3416,
            3803,
            574,
            2129,
            1948,
            2978,
            1619,
            3716,
            3454,
            3777,
            2712,
            1434,
            3968,
            3383,
            3278,
            3970,
            2674,
            2776,
            2336,
            2430,
            891,
            728,
            3886,
            1711,
            2856,
            2447,
            814,
            2951,
            3653,
            1891,
            701,
            3139,
            2811,
            1701,
            1853,
            2445,
            2287,
            2003,
            3595,
            1726,
            3952,
            850,
            1446,
            1300,
            719,
            988,
            2606,
            2912,
            2339,
            1028,
            3173,
            1667,
            1897,
            1214,
            3269,
            445,
            1073,
            665,
            3129,
            3501,
            3461,
            3206,
            3717,
            3917,
            861,
            2885,
            2777,
            1017,
            980,
            2775,
            3309,
            3607,
            1257,
            759,
            3720,
            475,
            1122,
            1336,
            1185,
            705,
            3169,
            3483,
            1183,
            3373,
            2801,
            565,
            3153,
            3421,
            316,
            448,
            2366,
            3881,
            462,
            1480,
            1087,
            2711,
            2144,
            1165,
            2998,
            2249,
            752,
            1409,
            3072,
            1305,
            3597,
            3132,
            3856,
            1524,
            3673,
            3293,
            3035,
            584,
            2675,
            2382,
            1391,
            2346,
            606,
            1324,
            1076,
            643,
            783,
            3809,
            2104,
            1762,
            3137,
            1756,
            3663,
            2437,
            1432,
            1620,
            1655,
            2832,
            961,
            2608,
            514,
            2304,
            933,
            1896,
            3655,
            1973,
            3278,
            2183,
            824,
            2598,
            1254,
            3622,
            1704,
            1624,
            3809,
            2574,
            950,
            273,
            446,
            1510,
            1122,
            707,
            751,
            3393,
            3771,
            1851,
            2735,
            2253,
            452,
            1419,
            969,
            3441,
            3786,
            2710,
            3533,
            1735,
            3587,
            2298,
            2027,
            2406,
            2419,
            995,
            2078,
            1145,
            626,
            1509,
            3371,
            297,
            3508,
            2385,
            1301,
            2167,
            3914,
            2312,
            2297,
            411,
            665,
            3226,
            740,
            3878,
            3406,
            2440,
            3959,
            3703,
            3714,
            337,
            3176,
            1205,
            2473,
            1229,
            2896,
            2039,
            520,
            1378,
            3254,
            1803,
            2501,
            3762,
            1796,
            990,
            2630,
            1380,
            3487,
            1201,
            3015,
            3204,
            433,
            1906,
            898,
            2799,
            2020,
            770,
            305,
            2953,
            1671,
            2574,
            3772,
            2870,
            2577,
            2246,
            2405,
            3377,
            3619,
            1184,
            2965,
            3713,
            3798,
            1026,
            1086,
            2236,
            1303,
            3048,
            616,
            3764,
            419,
            2858,
            504,
            392,
            820,
            3387,
            3447,
            1403,
            2295,
            1973,
            3755,
            2073,
            3872,
            302,
            1519,
            2549,
            404,
            595,
            1663,
            2764,
            2614,
            1094,
            2826,
            2496,
            2103,
            966,
            2172,
            905,
            3916,
            3902,
            2487,
            3914,
            363,
            3233,
            2532,
            3836,
            3076,
            1513,
            1724,
            542,
            3481,
            3991,
            1790,
            509,
            282,
            3006,
            1913,
            1333,
            3746,
            3941,
            3970,
            3925,
            424,
            1284,
            277,
            1713,
            485,
            3002,
            418,
            1652,
            2893,
            1406,
            629,
            3724,
            3835,
            1354,
            2376,
            3438,
            2655,
            3361,
            3535,
            2150,
            2587,
            2736,
            676,
            2941,
            3336,
            3796,
            3654,
            2934,
            2889,
            3315,
            1993,
            2509,
            3534,
            2246,
            3165,
            286,
            776,
            3213,
            716,
            424,
            3486,
            3237,
            1944,
            643,
            2317,
            1436,
            3190,
            2338,
            1334,
            3657,
            2080,
            1609,
            3078,
            590,
            2552,
            3355,
            1968,
            3358,
            1539,
            942,
            3669,
            3612,
            2351,
            604,
            1285,
            515,
            654,
            2540,
            3869,
            3412,
            509,
            2547,
            2735,
            1369,
            2942,
            2812,
            3497,
            619,
            3483,
            1475,
            1950,
            3592,
            3573,
            2020,
            1329,
            3568,
            1947,
            1998,
            725,
            3302,
            3884,
            1185,
            1656,
            2870,
            911,
            3892,
            3897,
            656,
            606,
            1653,
            860,
            2197,
            3458,
            2066,
            3676,
            1518,
            1018,
            1360,
            3449,
            2524,
            703,
            1251,
            582,
            1732,
            1252,
            2146,
            1927,
            3328,
            2462,
            1172,
            3598,
            2437,
            3271,
            1511,
            2981,
            3119,
            3647,
            3148,
            2829,
            2373,
            3998,
            316,
            999,
            1808,
            589,
            3317,
            670,
            584,
            2173,
            1561,
            2073,
            1401,
            3764,
            1275,
            1464,
            2885,
            3458,
            3019,
            1083,
            3538,
            2493,
            3726,
            1531,
            601,
            2991,
            3912,
            309,
            323,
            1489,
            1322,
            1846,
            417,
            3269,
            3731,
            667,
            1241,
            3941,
            1018,
            3142,
            1444,
            934,
            3863,
            3506,
            1944,
            348,
            550,
            971,
            3984,
            2973,
            2019,
            1587,
            1870,
            1352,
            570,
            546,
            3624,
            2944,
            1209,
            2752,
            3868,
            3087,
            947,
            1484,
            347,
            3227,
            3299,
            1028,
            714,
            2641,
            1229,
            2622,
            929,
            1211,
            1653,
            511,
            3034,
            1414,
            2438,
            3744,
            1377,
            3520,
            3796,
            1699,
            336,
            1095,
            325,
            1223,
            1649,
            1898,
            2609,
            2265,
            2775,
            1445,
            2614,
            356,
            523,
            3745,
            1229,
            1402,
            2085,
            587,
            2457,
            2030,
            816,
            1667,
            2958,
            3705,
            2800,
            3773,
            2050,
            1754,
            1904,
            2705,
            3537,
            1370,
            1222,
            3102,
            2268,
            2648,
            2173,
            3162,
            1368,
            2746,
            619,
            724,
            912,
            2784,
            793,
            560,
            312,
            545,
            1591,
            2746,
            893,
            2514,
            1032,
            819,
            3672,
            3833,
            1482,
            2043,
            1229,
            1816,
            2166,
            2469,
            2575,
            1375,
            1732,
            673,
            2301,
            2676,
            1781,
            2248,
            921,
            391,
            1719,
            3262,
            1947,
            2693,
            441,
            1886,
            1267,
            687,
            2890,
            1737,
            887,
            398,
            2389,
            3431,
            3601,
            2459,
            3230,
            3646,
            2200,
            3862,
            3077,
            621,
            2372,
            2574,
            2857,
            3143,
            3197,
            2727,
            1519,
            2438,
            2674,
            2107,
            2262,
            815,
            2829,
            3022,
            2573,
            2678,
            1282,
            568,
            2074,
            2227,
            1080,
            2867,
            1075,
            3063,
            2479,
            2394,
            2814,
            2094,
            2315,
            2519,
            735,
            1891,
            1386,
            3260,
            2022,
            1716,
            3317,
            1454,
            2290,
            3940,
            461,
            2219,
            470,
            2495,
            2944,
            2361,
            1783,
            2832,
            931,
            1344,
            3916,
            1882,
            2865,
            757,
            3469,
            2399,
            3300,
            2719,
            3012,
            2836,
            2385,
            594,
            2968,
            3762,
            892,
            467,
            345,
            3033,
            2110,
            3105,
            874,
            3681,
            3865,
            737,
            3153,
            2800,
            315,
            1412,
            2286,
            2392,
            1857,
            1419,
            3305,
            959,
            2635,
            1754,
            2563,
            1936,
            1869,
            3637,
            896,
            2349,
            774,
            3397,
            485,
            3964,
            1887,
            2977,
            2114,
            1542,
            3038,
            2481,
            841,
            3565,
            1958,
            2454,
            3392,
            262,
            530,
            3703,
            2562,
            574,
            2632,
            259,
            688,
            2554,
            3859,
            2977,
            571,
            402,
            878,
            2130,
            3260,
            454,
            2638,
            2819,
            1464,
            1824,
            876,
            2706,
            659,
            455,
            2378,
            2278,
            3436,
            3424,
            401,
            681,
            3212,
            3105,
            2036,
            3153,
            1642,
            509,
            3465,
            3900,
            1105,
            2064,
            3117,
            1142,
            1060,
            2381,
            3366,
            1339,
            2560,
            285,
            2476,
            1862,
            3488,
            3178,
            733,
            756,
            3622,
            1277,
            3703,
            1905,
            2636,
            2014,
            3056,
            465,
            3869,
            1108,
            1906,
            851,
            3142,
            1300,
            1430,
            362,
            2456,
            2360,
            3700,
            2620,
            437,
            3774,
            1218,
            696,
            3407,
            3902,
            2086,
            1356,
            1704,
            3571,
            2132,
            3209,
            2096,
            1273,
            3623,
            368,
            854,
            3646,
            3130,
            3784,
            3885,
            1494,
            2973,
            1471,
            2001,
            1486,
            1321,
            1877,
            3499,
            2124,
            1221,
            2316,
            1068,
            1930,
            1316,
            1742,
            3265,
            2700,
            2115,
            3915,
            3144,
            1571,
            2253,
            1433,
            2583,
            1817,
            2915,
            3340,
            2790,
            577,
            3220,
            3488,
            1277,
            3593,
            2736,
            2311,
            1464,
            1355,
            2382,
            1886,
            832,
            751,
            436,
            722,
            1075,
            2802,
            1780,
            1247,
            3066,
            403,
            2518,
            1069,
            2651,
            2974,
            657,
            3372,
            1464,
            1326,
            3393,
            323,
            1628,
            2610,
            3721,
            763,
            792,
            1568,
            1276,
            3472,
            2520,
            3423,
            258,
            1868,
            2171,
            780,
            712,
            1012,
            318,
            3948,
            629,
            1161,
            1403,
            3707,
            1305,
            2926,
            2674,
            471,
            1724,
            1220,
            2754,
            594,
            1005,
            2439,
            2302,
            1083,
            2219,
            1571,
            2107,
            593,
            857,
            3518,
            1284,
            545,
            3566,
            2821,
            3442,
            1309,
            1179,
            1713,
            1663,
            1812,
            1384,
            1348,
            3957,
            1212,
            3925,
            348,
            1382,
            980,
            838,
            378,
            909,
            3632,
            3396,
            1861,
            348,
            2528,
            2190,
            663,
            961,
            1280,
            2189,
            531,
            2331,
            794,
            2644,
            1666,
            2157,
            1196,
            2072,
            1590,
            2916,
            3707,
            726,
            3577,
            2627,
            685,
            1693,
            2137,
            3696,
            3771,
            1342,
            1379,
            1164,
            908,
            1019,
            3685,
            1470,
            2299,
            999,
            3011,
            3142,
            1543,
            3692,
            2753,
            2156,
            2637,
            331,
            1214,
            2248,
            3073,
            422,
            2293,
            3958,
            432,
            1152,
            1303,
            797,
            298,
            3847,
            2110,
            1921,
            2423,
            1658,
            2361,
            1380,
            3892,
            315,
            2289,
            2182,
            2184,
            2872,
            2367,
            1919,
            3296,
            462,
            1591,
            1593,
            2456,
            2933,
            3154,
            3615,
            3563,
            1440,
            1907,
            324,
            1404,
            2738,
            2722,
            2294,
            1424,
            1133,
            555,
            3785,
            3675,
            435,
            3183,
            3495,
            1853,
            2648,
            2647,
            702,
            1836,
            3894,
            2944,
            3424,
            2678,
            2424,
            2671,
            3078,
            2304,
            1146,
            2335,
            1422,
            2096,
            1303,
            1115,
            456,
            3396,
            2750,
            941,
            2278,
            2273,
            2355,
            2328,
            2638,
            558,
            1345,
            425,
            2192,
            3397,
            1208,
            267,
            3518,
            3122,
            1050,
            3358,
            2929,
            624,
            1598,
            2052,
            1249,
            412,
            595,
            263,
            2577,
            639,
            3206,
            1550,
            1646,
            1930,
            3695,
            3235,
            2334,
            1998,
            2258,
            1869,
            1825,
            3946,
            436,
            2678,
            3887,
            1813,
            817,
            3614,
            2013,
            543,
            3400,
            2623,
            2700,
            1400,
            3127,
            2803,
            2518,
            3564,
            2515,
            3654,
            3930,
            3476,
            3242,
            2666,
            1695,
            3510,
            3438,
            2033,
            1017,
            1686,
            862,
            957,
            3494,
            3083,
            1656,
            313,
            3282,
            3629,
            1687,
            1149,
            3899,
            1783,
            1629,
            2528,
            3256,
            2341,
            2785,
            3761,
            1513,
            2240,
            820,
            1875,
            920,
            3615,
            1412,
            361,
            1540,
            1077,
            3610,
            512,
            3024,
            1610,
            2201,
            3338,
            3473,
            2203,
            940,
            1155,
            3872,
            1882,
            1798,
            2598,
            1567,
            1491,
            2003,
            3905,
            291,
            1693,
            1788,
            3322,
            1751,
            1641,
            2854,
            3115,
            3259,
            1612,
            2319,
            1258,
            1543,
            3991,
            3555,
            3411,
            524,
            3541,
            1998,
            3437,
            1637,
            3064,
            2855,
            1393,
            1633,
            2392,
            3284,
            464,
            1234,
            292,
            2521,
            2658,
            587,
            2190,
            879,
            1743,
            1181,
            3330,
            3564,
            562,
            1659,
            1816,
            1338,
            2992,
            315,
            3249,
            3242,
            2725,
            586,
            2454,
            804,
            2460,
            1295,
            2625,
            1999,
            2698,
            3742,
            1193,
            3511,
            885,
            1044,
            2548,
            2940,
            3966,
            3484,
            1246,
            2757,
            2378,
            1794,
            3933,
            1637,
            783,
            3094,
            1774,
            256,
            1365,
            2339,
            2049,
            3241,
            867,
            2175,
            3497,
            609,
            3285,
            3684,
            3586,
            2322,
            354,
            3204,
            1400,
            2308,
            1082,
            3104,
            2847,
            2256,
            1113,
            3210,
            3990,
            984,
            3117,
            1622,
            2967,
            1037,
            2908,
            1167,
            2715,
            1734,
            2259,
            2921,
            1244,
            3132,
            2488,
            1052,
            2927,
            2090,
            2131,
            3362,
            1053,
            1799,
            3124,
            1672,
            1552,
            1938,
            3202,
            1302,
            2296,
            575,
            2359,
            3314,
            440,
            2002,
            3167,
            3480,
            595,
            2466,
            1337,
            1462,
            503,
            2066,
            708,
            2029,
            308,
            552,
            2082,
            3314,
            3996,
            1795,
            594,
            2473,
            690,
            3845,
            2004,
            2083,
            2661,
            1903,
            2466,
            3779,
            3904,
            265,
            1417,
            1769,
            2948,
            2572,
            3267,
            1459,
            1633,
            2052,
            1086,
            2402,
            3376,
            1507,
            641,
            3451,
            1607,
            3521,
            1241,
            881,
            2563,
            3033,
            769,
            3454,
            819,
            435,
            586,
            650,
            3038,
            3736,
            2948,
            739,
            686,
            841,
            1183,
            3943,
            1037,
            2633,
            3911,
            1958,
            3297,
            3141,
            3351,
            2355,
            1308,
            991,
            3248,
            1519,
            493,
            1747,
            2517,
            1389,
            2522,
            1885,
            704,
            953,
            3206,
            928,
            2177,
            2610,
            489,
            3839,
            1861,
            299,
            340,
            3606,
            799,
            2367,
            445,
            3927,
            3756,
            770,
            1641,
            2343,
            2787,
            2495,
            3208,
            1234,
            1069,
            3568,
            3585,
            1974,
            2419,
            340,
            2282,
            298,
            968,
            2391,
            1534,
            696,
            2782,
            755,
            3746,
            2651,
            1341,
            2651,
            1824,
            1905,
            1456,
            3155,
            773,
            1829,
            3883,
            1675,
            1601,
            1958,
            2376,
            1647,
            3548,
            2694,
            2195,
            2506,
            2073,
            3767,
            1150,
            841,
            1675,
            1301,
            1817,
            2321,
            1974,
            3433,
            1205,
            872,
            2052,
            3654,
            1503,
            3104,
            343,
            3430,
            2470,
            1396,
            2668,
            1457,
            2452,
            1871,
            1112,
            967,
            2498,
            387,
            635,
            2711,
            2023,
            2215,
            824,
            3851,
            2156,
            2885,
            495,
            2358,
            1744,
            1832,
            2016,
            1300,
            2313,
            3248,
            2216,
            3142,
            1850,
            1424,
            1620,
            2592,
            296,
            3579,
            1180,
            3962,
            3978,
            668,
            2032,
            3057,
            869,
            3919
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sc_h",
           "values": [
            9,
            17,
            11,
            16,
            8,
            17,
            13,
            16,
            17,
            19,
            5,
            14,
            18,
            7,
            14,
            17,
            10,
            10,
            19,
            11,
            17,
            17,
            11,
            8,
            11,
            18,
            16,
            17,
            10,
            12,
            17,
            5,
            18,
            7,
            12,
            10,
            13,
            6,
            6,
            18,
            18,
            18,
            15,
            15,
            9,
            8,
            10,
            16,
            15,
            16,
            5,
            19,
            12,
            17,
            6,
            14,
            14,
            9,
            7,
            15,
            14,
            15,
            7,
            14,
            10,
            18,
            9,
            16,
            11,
            7,
            13,
            9,
            15,
            11,
            9,
            5,
            10,
            11,
            17,
            8,
            19,
            14,
            9,
            10,
            11,
            15,
            16,
            13,
            9,
            10,
            19,
            18,
            16,
            16,
            19,
            13,
            19,
            18,
            14,
            6,
            16,
            16,
            17,
            14,
            11,
            5,
            12,
            18,
            19,
            19,
            16,
            18,
            13,
            8,
            9,
            12,
            12,
            15,
            5,
            5,
            12,
            18,
            10,
            5,
            13,
            19,
            9,
            6,
            12,
            12,
            17,
            7,
            12,
            10,
            12,
            12,
            19,
            12,
            12,
            9,
            17,
            17,
            18,
            12,
            19,
            18,
            12,
            16,
            13,
            13,
            10,
            16,
            16,
            19,
            12,
            16,
            18,
            10,
            17,
            8,
            10,
            15,
            7,
            17,
            16,
            17,
            18,
            7,
            12,
            9,
            5,
            17,
            9,
            19,
            16,
            7,
            14,
            14,
            16,
            17,
            14,
            19,
            6,
            16,
            17,
            13,
            11,
            13,
            9,
            17,
            15,
            12,
            17,
            18,
            7,
            18,
            17,
            15,
            11,
            13,
            5,
            7,
            14,
            6,
            19,
            11,
            13,
            5,
            19,
            18,
            11,
            15,
            11,
            14,
            9,
            18,
            5,
            13,
            16,
            11,
            19,
            6,
            12,
            12,
            14,
            11,
            6,
            18,
            19,
            7,
            14,
            11,
            17,
            6,
            9,
            19,
            18,
            7,
            9,
            12,
            11,
            7,
            10,
            7,
            8,
            8,
            16,
            18,
            12,
            9,
            7,
            17,
            6,
            13,
            15,
            6,
            12,
            17,
            14,
            9,
            6,
            5,
            7,
            10,
            14,
            9,
            10,
            18,
            9,
            12,
            19,
            19,
            17,
            8,
            17,
            19,
            9,
            8,
            8,
            13,
            17,
            11,
            19,
            15,
            11,
            12,
            8,
            7,
            14,
            7,
            14,
            11,
            14,
            15,
            15,
            5,
            15,
            8,
            19,
            12,
            5,
            7,
            19,
            5,
            18,
            18,
            13,
            8,
            11,
            7,
            16,
            12,
            13,
            18,
            19,
            14,
            8,
            18,
            13,
            14,
            16,
            12,
            6,
            9,
            13,
            10,
            16,
            15,
            7,
            11,
            17,
            6,
            14,
            5,
            17,
            18,
            15,
            14,
            14,
            15,
            8,
            18,
            7,
            17,
            13,
            17,
            6,
            5,
            13,
            12,
            15,
            18,
            13,
            19,
            10,
            8,
            17,
            15,
            11,
            10,
            18,
            11,
            17,
            14,
            13,
            6,
            18,
            8,
            11,
            19,
            16,
            7,
            5,
            12,
            10,
            17,
            13,
            11,
            9,
            13,
            8,
            12,
            10,
            18,
            10,
            17,
            19,
            17,
            7,
            16,
            5,
            10,
            9,
            19,
            14,
            9,
            12,
            12,
            10,
            8,
            10,
            18,
            13,
            13,
            9,
            7,
            9,
            12,
            7,
            13,
            15,
            17,
            5,
            6,
            9,
            10,
            19,
            14,
            15,
            11,
            6,
            16,
            15,
            12,
            14,
            12,
            9,
            8,
            19,
            5,
            18,
            5,
            13,
            13,
            17,
            17,
            15,
            12,
            15,
            8,
            16,
            8,
            12,
            12,
            13,
            15,
            5,
            11,
            8,
            5,
            16,
            16,
            9,
            6,
            8,
            17,
            7,
            14,
            17,
            5,
            12,
            15,
            17,
            9,
            16,
            14,
            19,
            11,
            11,
            9,
            7,
            9,
            6,
            6,
            9,
            11,
            18,
            9,
            16,
            16,
            11,
            15,
            14,
            15,
            16,
            9,
            13,
            7,
            17,
            6,
            8,
            5,
            17,
            11,
            16,
            6,
            7,
            7,
            6,
            13,
            7,
            17,
            9,
            19,
            9,
            15,
            12,
            18,
            19,
            9,
            14,
            7,
            5,
            13,
            13,
            18,
            7,
            15,
            13,
            7,
            7,
            5,
            16,
            16,
            8,
            16,
            14,
            19,
            10,
            7,
            18,
            13,
            14,
            7,
            14,
            16,
            11,
            7,
            7,
            18,
            17,
            6,
            7,
            11,
            17,
            6,
            15,
            8,
            7,
            16,
            8,
            5,
            17,
            8,
            14,
            11,
            13,
            11,
            19,
            14,
            9,
            17,
            6,
            9,
            8,
            15,
            6,
            18,
            17,
            13,
            6,
            12,
            11,
            15,
            7,
            19,
            9,
            12,
            13,
            14,
            15,
            12,
            16,
            9,
            15,
            7,
            9,
            8,
            11,
            11,
            14,
            10,
            14,
            11,
            6,
            18,
            9,
            16,
            15,
            18,
            16,
            14,
            6,
            6,
            17,
            8,
            13,
            19,
            5,
            14,
            17,
            16,
            7,
            12,
            14,
            8,
            18,
            15,
            12,
            8,
            12,
            17,
            12,
            9,
            19,
            13,
            6,
            7,
            16,
            17,
            10,
            17,
            7,
            5,
            12,
            6,
            14,
            14,
            12,
            17,
            18,
            5,
            16,
            6,
            7,
            7,
            14,
            14,
            7,
            12,
            6,
            8,
            15,
            13,
            15,
            7,
            17,
            15,
            17,
            10,
            17,
            17,
            16,
            12,
            12,
            19,
            7,
            12,
            15,
            15,
            7,
            17,
            13,
            12,
            15,
            19,
            17,
            13,
            16,
            7,
            14,
            17,
            11,
            13,
            8,
            7,
            5,
            19,
            10,
            15,
            13,
            13,
            13,
            16,
            13,
            11,
            10,
            9,
            14,
            8,
            10,
            6,
            11,
            19,
            5,
            9,
            10,
            17,
            19,
            16,
            6,
            18,
            16,
            5,
            12,
            11,
            10,
            18,
            12,
            5,
            16,
            15,
            16,
            14,
            18,
            10,
            15,
            12,
            6,
            18,
            10,
            17,
            16,
            5,
            6,
            12,
            11,
            9,
            6,
            12,
            17,
            16,
            9,
            8,
            15,
            5,
            14,
            6,
            15,
            8,
            19,
            13,
            18,
            8,
            12,
            10,
            15,
            10,
            7,
            6,
            14,
            10,
            12,
            11,
            14,
            15,
            15,
            10,
            17,
            7,
            9,
            18,
            18,
            10,
            18,
            18,
            14,
            9,
            7,
            10,
            18,
            13,
            19,
            5,
            17,
            19,
            8,
            11,
            13,
            17,
            9,
            13,
            16,
            15,
            7,
            11,
            11,
            11,
            10,
            15,
            19,
            19,
            10,
            6,
            12,
            18,
            10,
            6,
            13,
            7,
            5,
            17,
            11,
            10,
            12,
            6,
            11,
            17,
            8,
            14,
            5,
            16,
            15,
            15,
            5,
            17,
            12,
            16,
            17,
            9,
            7,
            9,
            15,
            13,
            6,
            10,
            18,
            8,
            5,
            17,
            9,
            17,
            18,
            14,
            5,
            14,
            7,
            15,
            11,
            10,
            12,
            19,
            16,
            14,
            6,
            16,
            5,
            16,
            18,
            17,
            16,
            16,
            10,
            13,
            19,
            12,
            12,
            7,
            19,
            16,
            9,
            13,
            8,
            10,
            5,
            5,
            16,
            17,
            17,
            13,
            12,
            10,
            16,
            9,
            11,
            10,
            9,
            5,
            17,
            11,
            11,
            7,
            17,
            6,
            6,
            6,
            5,
            12,
            11,
            11,
            17,
            15,
            5,
            19,
            14,
            18,
            12,
            14,
            7,
            11,
            14,
            16,
            7,
            12,
            12,
            16,
            17,
            15,
            5,
            13,
            10,
            13,
            6,
            8,
            6,
            11,
            17,
            10,
            15,
            12,
            9,
            19,
            14,
            17,
            7,
            9,
            18,
            18,
            11,
            11,
            6,
            19,
            16,
            12,
            18,
            8,
            16,
            11,
            12,
            10,
            16,
            19,
            14,
            13,
            17,
            13,
            9,
            11,
            6,
            18,
            14,
            8,
            12,
            7,
            8,
            11,
            8,
            9,
            10,
            9,
            7,
            8,
            19,
            17,
            17,
            12,
            19,
            17,
            19,
            7,
            16,
            17,
            14,
            9,
            14,
            12,
            11,
            17,
            17,
            10,
            9,
            17,
            5,
            7,
            12,
            12,
            8,
            19,
            15,
            5,
            16,
            11,
            9,
            17,
            12,
            13,
            8,
            18,
            8,
            12,
            16,
            13,
            15,
            7,
            9,
            13,
            6,
            13,
            11,
            5,
            7,
            17,
            17,
            14,
            14,
            9,
            19,
            6,
            12,
            12,
            5,
            9,
            17,
            5,
            5,
            5,
            17,
            6,
            14,
            13,
            8,
            19,
            7,
            16,
            10,
            17,
            6,
            11,
            11,
            7,
            17,
            12,
            17,
            16,
            16,
            13,
            6,
            7,
            7,
            7,
            19,
            14,
            10,
            7,
            17,
            16,
            12,
            7,
            9,
            5,
            12,
            19,
            15,
            9,
            16,
            10,
            8,
            15,
            14,
            15,
            7,
            16,
            17,
            10,
            18,
            13,
            15,
            6,
            10,
            15,
            17,
            18,
            16,
            14,
            5,
            19,
            16,
            15,
            8,
            15,
            19,
            17,
            6,
            17,
            8,
            18,
            16,
            7,
            15,
            14,
            14,
            6,
            15,
            18,
            17,
            17,
            11,
            14,
            14,
            17,
            17,
            14,
            15,
            9,
            14,
            9,
            13,
            16,
            16,
            13,
            13,
            16,
            16,
            11,
            12,
            8,
            9,
            10,
            5,
            12,
            17,
            9,
            19,
            13,
            8,
            10,
            13,
            7,
            19,
            7,
            14,
            14,
            7,
            7,
            11,
            13,
            17,
            17,
            15,
            6,
            10,
            8,
            16,
            17,
            13,
            11,
            17,
            13,
            14,
            11,
            12,
            5,
            18,
            12,
            19,
            14,
            6,
            11,
            12,
            9,
            12,
            15,
            17,
            10,
            9,
            9,
            14,
            15,
            19,
            8,
            14,
            8,
            8,
            14,
            13,
            19,
            17,
            8,
            10,
            12,
            17,
            17,
            17,
            18,
            7,
            8,
            11,
            17,
            10,
            15,
            7,
            10,
            13,
            8,
            6,
            10,
            17,
            15,
            11,
            18,
            12,
            9,
            14,
            12,
            14,
            8,
            13,
            5,
            10,
            7,
            12,
            10,
            19,
            11,
            15,
            8,
            14,
            16,
            8,
            13,
            11,
            19,
            16,
            7,
            11,
            16,
            7,
            14,
            9,
            17,
            15,
            16,
            19,
            5,
            19,
            15,
            7,
            5,
            16,
            9,
            15,
            16,
            18,
            7,
            13,
            19,
            13,
            12,
            14,
            7,
            17,
            7,
            19,
            17,
            16,
            6,
            9,
            15,
            14,
            17,
            14,
            13,
            16,
            12,
            11,
            18,
            12,
            16,
            12,
            14,
            8,
            11,
            14,
            8,
            16,
            13,
            7,
            10,
            7,
            14,
            11,
            15,
            14,
            12,
            9,
            10,
            15,
            6,
            16,
            18,
            13,
            13,
            19,
            9,
            5,
            8,
            10,
            12,
            13,
            11,
            18,
            11,
            13,
            11,
            15,
            7,
            18,
            11,
            6,
            18,
            19,
            6,
            13,
            8,
            18,
            14,
            13,
            8,
            9,
            16,
            17,
            10,
            6,
            15,
            6,
            17,
            8,
            19,
            14,
            17,
            14,
            9,
            14,
            12,
            17,
            7,
            6,
            6,
            16,
            7,
            19,
            10,
            6,
            7,
            8,
            12,
            19,
            11,
            11,
            19,
            18,
            14,
            15,
            7,
            18,
            15,
            7,
            6,
            17,
            9,
            8,
            16,
            17,
            17,
            14,
            19,
            10,
            8,
            13,
            14,
            16,
            19,
            11,
            17,
            7,
            11,
            18,
            16,
            11,
            16,
            19,
            6,
            17,
            7,
            8,
            12,
            9,
            19,
            17,
            12,
            6,
            12,
            16,
            9,
            12,
            18,
            9,
            12,
            16,
            17,
            19,
            9,
            13,
            18,
            12,
            10,
            16,
            9,
            5,
            19,
            11,
            17,
            12,
            8,
            17,
            12,
            13,
            14,
            11,
            18,
            17,
            5,
            13,
            7,
            11,
            16,
            12,
            10,
            8,
            7,
            18,
            18,
            11,
            7,
            15,
            8,
            10,
            10,
            12,
            16,
            5,
            11,
            17,
            18,
            15,
            6,
            11,
            9,
            10,
            16,
            17,
            15,
            18,
            19,
            13,
            12,
            19,
            6,
            7,
            13,
            7,
            13,
            13,
            13,
            8,
            11,
            12,
            19,
            19,
            7,
            5,
            12,
            6,
            8,
            8,
            6,
            8,
            8,
            18,
            9,
            17,
            15,
            17,
            14,
            5,
            7,
            10,
            7,
            13,
            7,
            5,
            12,
            17,
            18,
            11,
            19,
            7,
            16,
            14,
            5,
            17,
            17,
            15,
            14,
            10,
            18,
            12,
            8,
            18,
            14,
            19,
            13,
            12,
            9,
            5,
            17,
            15,
            12,
            14,
            19,
            12,
            13,
            19,
            13,
            9,
            12,
            15,
            15,
            18,
            6,
            6,
            5,
            15,
            17,
            16,
            12,
            14,
            6,
            15,
            13,
            8,
            17,
            17,
            8,
            8,
            16,
            7,
            15,
            19,
            14,
            12,
            7,
            7,
            11,
            17,
            10,
            9,
            7,
            17,
            16,
            17,
            19,
            6,
            17,
            6,
            10,
            7,
            12,
            17,
            6,
            6,
            8,
            17,
            10,
            12,
            7,
            12,
            10,
            6,
            8,
            16,
            14,
            15,
            11,
            8,
            16,
            17,
            9,
            11,
            10,
            19,
            18,
            7,
            14,
            13,
            17,
            11,
            18,
            5,
            13,
            14,
            14,
            16,
            11,
            8,
            7,
            6,
            13,
            8,
            8,
            15,
            16,
            19,
            15,
            8,
            5,
            13,
            8,
            11,
            9,
            10,
            15,
            13,
            15,
            10,
            19,
            16,
            16,
            18,
            16,
            6,
            12,
            6,
            13,
            5,
            6,
            11,
            17,
            18,
            7,
            7,
            14,
            19,
            10,
            16,
            16,
            17,
            15,
            16,
            12,
            6,
            19,
            11,
            13,
            18,
            12,
            14,
            10,
            7,
            10,
            17,
            5,
            17,
            18,
            12,
            5,
            17,
            9,
            18,
            9,
            11,
            17,
            7,
            16,
            9,
            18,
            10,
            15,
            7,
            19,
            8,
            6,
            15,
            6,
            18,
            16,
            7,
            16,
            14,
            12,
            17,
            11,
            12,
            16,
            15,
            16,
            19,
            10,
            15,
            19,
            9,
            17,
            10,
            18,
            15,
            15,
            11,
            13,
            16,
            16,
            11,
            8,
            6,
            13,
            17,
            11,
            12,
            17,
            18,
            18,
            7,
            7,
            6,
            12,
            12,
            18,
            9,
            19,
            12,
            14,
            8,
            14,
            18,
            11,
            11,
            15,
            19,
            19,
            10,
            18,
            17,
            11,
            15,
            17,
            9,
            6,
            14,
            10,
            14,
            17,
            6,
            18,
            7,
            15,
            12,
            5,
            15,
            17,
            14,
            8,
            14,
            17,
            8,
            11,
            15,
            5,
            7,
            13,
            14,
            16,
            5,
            17,
            10,
            19,
            13,
            13,
            12,
            15,
            13,
            6,
            14,
            17,
            14,
            17,
            10,
            5,
            12,
            19,
            7,
            9,
            17,
            10,
            19,
            6,
            15,
            9,
            12,
            7,
            16,
            10,
            5,
            15,
            11,
            19,
            9,
            8,
            8,
            9,
            5,
            8,
            10,
            10,
            12,
            16,
            17,
            15,
            15,
            5,
            14,
            12,
            7,
            5,
            9,
            5,
            15,
            15,
            9,
            6,
            18,
            14,
            19,
            17,
            10,
            12,
            10,
            17,
            18,
            14,
            8,
            18,
            8,
            13,
            5,
            13,
            6,
            16,
            13,
            7,
            7,
            6,
            12,
            14,
            5,
            14,
            8,
            13,
            18,
            7,
            16,
            17,
            10,
            10,
            14,
            18,
            16,
            12,
            12,
            10,
            9,
            17,
            8,
            6,
            16,
            10,
            7,
            11,
            10,
            8,
            17,
            14,
            7,
            17,
            10,
            8,
            11,
            7,
            7,
            16,
            10,
            14,
            15,
            17,
            5,
            10,
            6,
            12,
            6,
            14,
            7,
            18,
            5,
            17,
            19,
            15,
            13,
            16,
            9,
            11,
            17,
            18,
            12,
            14,
            10,
            7,
            19,
            12,
            14,
            18,
            5,
            5,
            9,
            15,
            7,
            9,
            13,
            8,
            10,
            7,
            17,
            6,
            12,
            7,
            9,
            9,
            10,
            6,
            19,
            7,
            11,
            15,
            12,
            14,
            19,
            13,
            8,
            10,
            13,
            14,
            17,
            15,
            17,
            9,
            19,
            10,
            7,
            8,
            15,
            10,
            6,
            15,
            13,
            10,
            14,
            13,
            9,
            16,
            9,
            15,
            9,
            12,
            5,
            19,
            6,
            15,
            17,
            13,
            11,
            9,
            18,
            19
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sc_w",
           "values": [
            7,
            3,
            2,
            8,
            2,
            1,
            8,
            3,
            1,
            10,
            2,
            9,
            0,
            1,
            9,
            15,
            9,
            2,
            13,
            1,
            15,
            1,
            5,
            7,
            0,
            9,
            1,
            8,
            1,
            7,
            11,
            1,
            5,
            0,
            1,
            4,
            7,
            5,
            1,
            12,
            2,
            10,
            0,
            12,
            3,
            3,
            5,
            12,
            3,
            13,
            0,
            12,
            5,
            8,
            0,
            7,
            11,
            4,
            5,
            13,
            4,
            6,
            0,
            2,
            4,
            8,
            4,
            11,
            1,
            1,
            11,
            7,
            6,
            0,
            2,
            4,
            7,
            7,
            13,
            7,
            10,
            11,
            6,
            3,
            5,
            9,
            0,
            3,
            5,
            1,
            3,
            7,
            10,
            10,
            17,
            5,
            4,
            17,
            3,
            2,
            8,
            7,
            14,
            10,
            4,
            2,
            6,
            1,
            17,
            13,
            4,
            13,
            6,
            4,
            1,
            1,
            9,
            4,
            3,
            0,
            9,
            11,
            4,
            4,
            5,
            14,
            3,
            4,
            0,
            10,
            7,
            0,
            9,
            5,
            8,
            3,
            4,
            11,
            9,
            1,
            0,
            0,
            17,
            11,
            0,
            12,
            6,
            1,
            5,
            10,
            1,
            11,
            7,
            1,
            2,
            10,
            4,
            0,
            0,
            0,
            5,
            6,
            1,
            14,
            4,
            16,
            9,
            6,
            8,
            2,
            3,
            4,
            1,
            6,
            12,
            5,
            8,
            8,
            12,
            4,
            7,
            12,
            1,
            1,
            14,
            2,
            0,
            9,
            5,
            7,
            4,
            10,
            9,
            12,
            5,
            10,
            9,
            2,
            10,
            4,
            4,
            4,
            6,
            4,
            15,
            7,
            4,
            0,
            15,
            17,
            0,
            9,
            3,
            9,
            5,
            5,
            3,
            1,
            7,
            8,
            17,
            5,
            11,
            10,
            11,
            6,
            4,
            8,
            16,
            2,
            10,
            10,
            6,
            5,
            8,
            8,
            12,
            1,
            0,
            2,
            1,
            6,
            1,
            1,
            6,
            3,
            3,
            15,
            5,
            0,
            1,
            9,
            5,
            5,
            9,
            4,
            8,
            0,
            8,
            1,
            3,
            1,
            3,
            0,
            0,
            3,
            8,
            4,
            6,
            1,
            13,
            15,
            6,
            4,
            13,
            11,
            5,
            2,
            4,
            9,
            16,
            7,
            10,
            5,
            5,
            0,
            4,
            5,
            3,
            4,
            3,
            1,
            12,
            5,
            13,
            0,
            1,
            6,
            10,
            8,
            4,
            1,
            3,
            3,
            5,
            11,
            1,
            6,
            10,
            6,
            12,
            0,
            0,
            2,
            2,
            1,
            1,
            5,
            12,
            5,
            0,
            10,
            5,
            6,
            3,
            7,
            0,
            11,
            1,
            6,
            15,
            1,
            0,
            0,
            1,
            15,
            1,
            12,
            7,
            11,
            4,
            7,
            0,
            1,
            10,
            4,
            0,
            0,
            3,
            5,
            11,
            16,
            10,
            17,
            0,
            4,
            14,
            9,
            1,
            7,
            7,
            1,
            11,
            13,
            0,
            4,
            8,
            2,
            7,
            18,
            1,
            6,
            1,
            9,
            1,
            16,
            11,
            7,
            7,
            11,
            1,
            8,
            6,
            2,
            6,
            15,
            3,
            16,
            6,
            1,
            0,
            7,
            2,
            18,
            1,
            7,
            8,
            1,
            0,
            3,
            5,
            0,
            12,
            8,
            2,
            5,
            0,
            10,
            2,
            9,
            7,
            10,
            2,
            3,
            2,
            1,
            5,
            9,
            13,
            7,
            5,
            14,
            1,
            6,
            2,
            2,
            3,
            1,
            5,
            1,
            16,
            2,
            8,
            6,
            10,
            0,
            7,
            11,
            8,
            1,
            0,
            1,
            11,
            9,
            5,
            11,
            3,
            3,
            2,
            1,
            12,
            8,
            8,
            2,
            7,
            2,
            3,
            9,
            15,
            1,
            3,
            7,
            12,
            8,
            14,
            10,
            4,
            9,
            1,
            5,
            5,
            7,
            2,
            1,
            8,
            2,
            0,
            1,
            2,
            14,
            3,
            9,
            6,
            10,
            4,
            4,
            12,
            1,
            10,
            0,
            0,
            1,
            5,
            8,
            8,
            0,
            4,
            4,
            1,
            3,
            1,
            6,
            8,
            13,
            2,
            6,
            11,
            14,
            17,
            4,
            6,
            3,
            3,
            9,
            1,
            7,
            1,
            13,
            8,
            2,
            2,
            0,
            12,
            5,
            6,
            8,
            8,
            17,
            4,
            4,
            3,
            5,
            10,
            3,
            3,
            2,
            7,
            0,
            1,
            3,
            15,
            1,
            6,
            4,
            14,
            1,
            1,
            7,
            5,
            5,
            6,
            1,
            2,
            7,
            10,
            5,
            6,
            10,
            9,
            6,
            4,
            6,
            2,
            8,
            7,
            14,
            4,
            5,
            6,
            11,
            3,
            6,
            0,
            5,
            4,
            7,
            2,
            7,
            9,
            4,
            3,
            10,
            10,
            3,
            7,
            6,
            3,
            5,
            2,
            3,
            9,
            1,
            3,
            2,
            1,
            12,
            4,
            2,
            14,
            17,
            8,
            10,
            3,
            1,
            16,
            2,
            12,
            6,
            4,
            3,
            8,
            5,
            3,
            8,
            5,
            7,
            4,
            10,
            1,
            4,
            7,
            14,
            11,
            0,
            13,
            12,
            0,
            3,
            8,
            1,
            8,
            16,
            4,
            3,
            1,
            3,
            10,
            0,
            9,
            15,
            0,
            0,
            6,
            0,
            4,
            1,
            0,
            10,
            3,
            9,
            3,
            3,
            5,
            5,
            9,
            0,
            13,
            13,
            12,
            1,
            9,
            6,
            13,
            10,
            9,
            2,
            3,
            7,
            6,
            3,
            2,
            4,
            6,
            11,
            1,
            1,
            12,
            1,
            14,
            0,
            11,
            5,
            5,
            10,
            7,
            6,
            3,
            1,
            4,
            11,
            9,
            1,
            8,
            9,
            12,
            10,
            3,
            1,
            6,
            2,
            0,
            4,
            10,
            16,
            1,
            5,
            4,
            6,
            3,
            3,
            2,
            7,
            11,
            3,
            11,
            6,
            2,
            15,
            0,
            4,
            12,
            9,
            6,
            11,
            8,
            1,
            6,
            3,
            1,
            13,
            7,
            13,
            8,
            3,
            0,
            1,
            8,
            5,
            1,
            4,
            8,
            0,
            5,
            1,
            0,
            1,
            8,
            2,
            7,
            3,
            10,
            5,
            14,
            7,
            3,
            3,
            6,
            7,
            4,
            2,
            4,
            5,
            6,
            3,
            11,
            8,
            11,
            2,
            14,
            1,
            8,
            5,
            6,
            8,
            5,
            4,
            0,
            3,
            0,
            5,
            0,
            6,
            8,
            0,
            14,
            10,
            1,
            9,
            12,
            13,
            5,
            3,
            14,
            3,
            4,
            8,
            2,
            1,
            8,
            2,
            12,
            1,
            9,
            4,
            7,
            0,
            0,
            2,
            1,
            6,
            3,
            13,
            2,
            3,
            7,
            5,
            6,
            4,
            5,
            12,
            3,
            15,
            8,
            13,
            1,
            12,
            2,
            10,
            0,
            5,
            5,
            1,
            0,
            12,
            4,
            5,
            8,
            6,
            2,
            7,
            0,
            2,
            17,
            11,
            0,
            4,
            6,
            14,
            4,
            5,
            8,
            12,
            6,
            2,
            0,
            1,
            2,
            1,
            7,
            3,
            11,
            11,
            1,
            2,
            8,
            5,
            7,
            3,
            12,
            12,
            1,
            5,
            7,
            4,
            0,
            0,
            13,
            2,
            5,
            8,
            3,
            3,
            15,
            6,
            5,
            1,
            7,
            3,
            4,
            4,
            2,
            6,
            3,
            0,
            1,
            2,
            4,
            7,
            3,
            0,
            0,
            4,
            3,
            16,
            2,
            7,
            10,
            10,
            2,
            7,
            1,
            2,
            5,
            9,
            8,
            11,
            10,
            12,
            2,
            11,
            8,
            4,
            4,
            5,
            4,
            10,
            4,
            9,
            7,
            11,
            2,
            2,
            12,
            8,
            3,
            4,
            11,
            7,
            5,
            9,
            3,
            10,
            12,
            8,
            6,
            0,
            3,
            9,
            3,
            5,
            7,
            14,
            9,
            7,
            4,
            11,
            4,
            6,
            4,
            1,
            12,
            5,
            0,
            5,
            7,
            2,
            4,
            3,
            9,
            2,
            0,
            0,
            6,
            13,
            8,
            3,
            6,
            1,
            3,
            4,
            10,
            12,
            5,
            4,
            9,
            8,
            10,
            14,
            8,
            4,
            4,
            2,
            1,
            6,
            6,
            11,
            6,
            12,
            2,
            1,
            4,
            4,
            7,
            0,
            6,
            4,
            4,
            2,
            2,
            0,
            3,
            6,
            10,
            5,
            0,
            6,
            1,
            11,
            1,
            3,
            4,
            10,
            10,
            8,
            12,
            1,
            12,
            1,
            4,
            11,
            3,
            4,
            16,
            4,
            3,
            0,
            1,
            0,
            10,
            5,
            3,
            13,
            0,
            2,
            0,
            1,
            4,
            6,
            9,
            2,
            11,
            8,
            3,
            13,
            3,
            2,
            0,
            5,
            5,
            3,
            18,
            5,
            4,
            4,
            3,
            9,
            6,
            3,
            1,
            2,
            0,
            0,
            5,
            6,
            12,
            0,
            0,
            2,
            13,
            7,
            5,
            11,
            16,
            1,
            6,
            10,
            3,
            2,
            3,
            9,
            8,
            3,
            8,
            13,
            2,
            3,
            15,
            13,
            3,
            1,
            11,
            4,
            0,
            1,
            5,
            6,
            8,
            3,
            6,
            4,
            1,
            1,
            4,
            14,
            10,
            3,
            4,
            12,
            3,
            14,
            12,
            9,
            8,
            4,
            2,
            3,
            12,
            15,
            6,
            10,
            5,
            12,
            5,
            9,
            11,
            1,
            2,
            0,
            4,
            11,
            13,
            2,
            11,
            3,
            0,
            1,
            0,
            6,
            13,
            1,
            12,
            11,
            5,
            1,
            3,
            0,
            11,
            4,
            6,
            0,
            0,
            7,
            11,
            3,
            7,
            5,
            10,
            1,
            1,
            6,
            2,
            4,
            15,
            10,
            2,
            4,
            5,
            7,
            0,
            0,
            10,
            13,
            11,
            6,
            8,
            5,
            8,
            3,
            3,
            3,
            4,
            2,
            6,
            5,
            1,
            8,
            15,
            4,
            6,
            7,
            7,
            16,
            7,
            4,
            0,
            5,
            7,
            6,
            1,
            4,
            6,
            6,
            8,
            5,
            1,
            5,
            9,
            7,
            9,
            15,
            11,
            6,
            0,
            8,
            4,
            4,
            1,
            1,
            2,
            1,
            6,
            7,
            1,
            10,
            12,
            7,
            7,
            5,
            3,
            1,
            8,
            16,
            13,
            3,
            9,
            0,
            5,
            9,
            2,
            11,
            3,
            13,
            2,
            0,
            18,
            7,
            1,
            1,
            15,
            4,
            12,
            11,
            16,
            6,
            7,
            6,
            9,
            10,
            1,
            3,
            0,
            3,
            7,
            1,
            9,
            4,
            7,
            0,
            5,
            2,
            9,
            10,
            3,
            2,
            9,
            16,
            8,
            14,
            1,
            5,
            4,
            4,
            7,
            6,
            10,
            0,
            4,
            0,
            5,
            7,
            5,
            10,
            12,
            1,
            3,
            0,
            9,
            5,
            1,
            15,
            11,
            6,
            0,
            4,
            1,
            2,
            9,
            4,
            4,
            0,
            10,
            1,
            4,
            10,
            3,
            2,
            2,
            7,
            0,
            12,
            2,
            0,
            12,
            1,
            12,
            7,
            7,
            7,
            7,
            2,
            16,
            8,
            1,
            2,
            1,
            1,
            0,
            11,
            12,
            2,
            3,
            6,
            9,
            5,
            9,
            6,
            1,
            4,
            0,
            3,
            11,
            8,
            5,
            3,
            0,
            10,
            16,
            2,
            2,
            12,
            6,
            8,
            12,
            3,
            13,
            9,
            4,
            5,
            3,
            3,
            3,
            11,
            1,
            12,
            0,
            4,
            4,
            2,
            3,
            8,
            7,
            8,
            10,
            5,
            4,
            10,
            11,
            15,
            8,
            7,
            3,
            5,
            13,
            2,
            4,
            10,
            3,
            10,
            8,
            2,
            3,
            10,
            11,
            3,
            3,
            5,
            4,
            4,
            12,
            7,
            18,
            5,
            5,
            16,
            1,
            3,
            0,
            3,
            0,
            9,
            6,
            15,
            8,
            1,
            11,
            7,
            7,
            2,
            6,
            4,
            8,
            3,
            10,
            5,
            8,
            10,
            4,
            7,
            1,
            5,
            10,
            12,
            5,
            2,
            4,
            6,
            6,
            8,
            0,
            11,
            1,
            4,
            7,
            3,
            11,
            2,
            8,
            0,
            3,
            11,
            1,
            12,
            13,
            9,
            1,
            0,
            9,
            2,
            4,
            0,
            5,
            0,
            7,
            6,
            4,
            8,
            5,
            3,
            1,
            6,
            1,
            2,
            1,
            3,
            4,
            1,
            2,
            0,
            9,
            2,
            11,
            9,
            8,
            5,
            1,
            6,
            2,
            3,
            9,
            2,
            3,
            10,
            6,
            7,
            9,
            11,
            4,
            3,
            11,
            2,
            10,
            15,
            14,
            10,
            3,
            16,
            6,
            7,
            4,
            3,
            10,
            10,
            0,
            2,
            0,
            4,
            10,
            0,
            13,
            15,
            11,
            10,
            17,
            11,
            4,
            5,
            10,
            2,
            4,
            1,
            4,
            0,
            3,
            16,
            2,
            8,
            2,
            1,
            8,
            4,
            4,
            16,
            10,
            4,
            5,
            0,
            0,
            13,
            15,
            13,
            10,
            3,
            0,
            9,
            11,
            4,
            7,
            0,
            6,
            2,
            11,
            0,
            5,
            16,
            0,
            9,
            2,
            0,
            7,
            5,
            1,
            7,
            4,
            1,
            2,
            1,
            5,
            6,
            0,
            6,
            13,
            1,
            12,
            7,
            7,
            6,
            16,
            3,
            0,
            7,
            9,
            17,
            2,
            8,
            9,
            1,
            8,
            11,
            4,
            6,
            4,
            3,
            9,
            3,
            1,
            0,
            5,
            3,
            4,
            4,
            1,
            2,
            17,
            5,
            7,
            4,
            12,
            6,
            8,
            6,
            9,
            10,
            1,
            6,
            6,
            18,
            8,
            2,
            8,
            9,
            1,
            8,
            1,
            7,
            2,
            0,
            0,
            16,
            13,
            3,
            5,
            10,
            14,
            2,
            6,
            7,
            2,
            2,
            2,
            4,
            2,
            9,
            4,
            11,
            7,
            10,
            8,
            7,
            0,
            1,
            9,
            4,
            14,
            3,
            4,
            2,
            16,
            3,
            17,
            2,
            1,
            3,
            6,
            13,
            4,
            4,
            4,
            2,
            5,
            13,
            0,
            5,
            0,
            3,
            1,
            2,
            1,
            0,
            4,
            1,
            14,
            10,
            7,
            8,
            3,
            7,
            0,
            1,
            10,
            8,
            8,
            14,
            5,
            15,
            11,
            9,
            8,
            3,
            13,
            14,
            1,
            7,
            1,
            0,
            15,
            10,
            2,
            12,
            3,
            7,
            0,
            3,
            1,
            4,
            8,
            7,
            3,
            2,
            8,
            11,
            3,
            4,
            10,
            3,
            5,
            8,
            9,
            8,
            4,
            1,
            7,
            10,
            5,
            7,
            2,
            1,
            1,
            3,
            9,
            4,
            0,
            17,
            3,
            3,
            4,
            3,
            9,
            8,
            12,
            5,
            10,
            0,
            4,
            9,
            6,
            1,
            2,
            10,
            11,
            9,
            0,
            13,
            8,
            8,
            1,
            7,
            0,
            8,
            6,
            5,
            10,
            8,
            8,
            16,
            0,
            3,
            7,
            18,
            3,
            0,
            5,
            5,
            17,
            1,
            10,
            7,
            9,
            3,
            6,
            4,
            2,
            1,
            8,
            15,
            7,
            4,
            0,
            3,
            1,
            0,
            4,
            1,
            3,
            5,
            9,
            3,
            10,
            3,
            7,
            10,
            0,
            0,
            1,
            2,
            0,
            11,
            0,
            3,
            14,
            11,
            13,
            11,
            3,
            9,
            9,
            0,
            0,
            8,
            3,
            1,
            6,
            10,
            3,
            0,
            4,
            0,
            6,
            5,
            5,
            2,
            9,
            7,
            0,
            8,
            6,
            0,
            9,
            0,
            8,
            8,
            5,
            9,
            5,
            11,
            15,
            10,
            2,
            1,
            4,
            3,
            5,
            2,
            5,
            0,
            3,
            7,
            8,
            3,
            5,
            1,
            2,
            12,
            8,
            7,
            9,
            4,
            6,
            14,
            1,
            1,
            5,
            13,
            2,
            1,
            2,
            6,
            2,
            7,
            0,
            3,
            0,
            7,
            1,
            5,
            3,
            3,
            3,
            10,
            7,
            17,
            2,
            12,
            9,
            5,
            6,
            2,
            4,
            3,
            4,
            3,
            5,
            9,
            2,
            1,
            4,
            6,
            0,
            5,
            5,
            5,
            7,
            5,
            3,
            1,
            0,
            5,
            14,
            5,
            1,
            8,
            5,
            10,
            17,
            5,
            2,
            1,
            6,
            5,
            7,
            7,
            13,
            3,
            18,
            3,
            3,
            4,
            6,
            5,
            3,
            8,
            10,
            0,
            1,
            3,
            5,
            6,
            4,
            12,
            2,
            8,
            3,
            8,
            3,
            11,
            16,
            4,
            10,
            1,
            10,
            4
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "talk_time",
           "values": [
            19,
            7,
            9,
            11,
            15,
            10,
            18,
            5,
            20,
            12,
            7,
            13,
            2,
            4,
            3,
            11,
            19,
            18,
            16,
            18,
            3,
            15,
            20,
            20,
            12,
            7,
            4,
            12,
            10,
            10,
            12,
            6,
            3,
            12,
            15,
            15,
            2,
            4,
            2,
            11,
            11,
            19,
            4,
            14,
            12,
            20,
            3,
            12,
            5,
            10,
            15,
            16,
            10,
            12,
            18,
            13,
            17,
            20,
            4,
            9,
            20,
            6,
            15,
            5,
            19,
            13,
            4,
            8,
            4,
            7,
            2,
            13,
            4,
            12,
            6,
            13,
            17,
            6,
            10,
            16,
            6,
            16,
            11,
            11,
            17,
            4,
            7,
            4,
            8,
            11,
            16,
            14,
            5,
            2,
            12,
            12,
            8,
            14,
            18,
            7,
            20,
            11,
            15,
            18,
            8,
            11,
            11,
            14,
            9,
            14,
            18,
            7,
            3,
            5,
            20,
            13,
            4,
            10,
            9,
            14,
            6,
            4,
            6,
            17,
            15,
            9,
            18,
            7,
            7,
            13,
            7,
            20,
            4,
            7,
            20,
            9,
            4,
            2,
            4,
            9,
            17,
            5,
            17,
            5,
            10,
            16,
            19,
            4,
            4,
            15,
            20,
            12,
            19,
            11,
            3,
            6,
            8,
            11,
            19,
            10,
            16,
            7,
            4,
            4,
            7,
            11,
            7,
            12,
            10,
            17,
            13,
            20,
            7,
            12,
            20,
            13,
            4,
            20,
            2,
            19,
            7,
            15,
            17,
            6,
            16,
            15,
            18,
            2,
            10,
            13,
            15,
            6,
            7,
            10,
            18,
            3,
            12,
            4,
            14,
            16,
            13,
            13,
            6,
            15,
            8,
            4,
            17,
            12,
            6,
            6,
            13,
            10,
            13,
            10,
            10,
            8,
            7,
            16,
            20,
            4,
            10,
            18,
            11,
            13,
            2,
            6,
            16,
            13,
            3,
            7,
            19,
            6,
            13,
            20,
            6,
            19,
            20,
            8,
            11,
            11,
            16,
            7,
            3,
            16,
            11,
            14,
            11,
            11,
            7,
            10,
            6,
            7,
            7,
            15,
            12,
            2,
            10,
            5,
            15,
            3,
            5,
            12,
            8,
            15,
            15,
            4,
            16,
            14,
            17,
            6,
            6,
            4,
            5,
            9,
            14,
            15,
            13,
            9,
            9,
            11,
            9,
            18,
            18,
            2,
            4,
            6,
            3,
            5,
            9,
            18,
            18,
            8,
            18,
            11,
            12,
            20,
            16,
            6,
            5,
            19,
            14,
            8,
            16,
            15,
            19,
            7,
            19,
            17,
            19,
            15,
            13,
            10,
            6,
            13,
            12,
            13,
            6,
            20,
            7,
            9,
            5,
            18,
            10,
            7,
            14,
            5,
            8,
            7,
            15,
            13,
            15,
            2,
            12,
            16,
            11,
            12,
            11,
            4,
            14,
            16,
            15,
            2,
            18,
            12,
            15,
            13,
            5,
            19,
            8,
            18,
            15,
            9,
            12,
            19,
            14,
            9,
            4,
            6,
            20,
            11,
            3,
            7,
            13,
            11,
            14,
            8,
            10,
            10,
            18,
            5,
            13,
            16,
            20,
            16,
            8,
            9,
            4,
            17,
            16,
            8,
            2,
            12,
            16,
            16,
            2,
            7,
            3,
            18,
            16,
            19,
            18,
            2,
            8,
            10,
            14,
            18,
            6,
            7,
            15,
            7,
            4,
            20,
            6,
            3,
            9,
            5,
            16,
            12,
            7,
            13,
            4,
            10,
            6,
            17,
            7,
            16,
            20,
            19,
            2,
            5,
            20,
            10,
            2,
            16,
            10,
            18,
            19,
            7,
            4,
            15,
            16,
            20,
            14,
            19,
            16,
            2,
            2,
            7,
            8,
            3,
            8,
            14,
            15,
            6,
            13,
            8,
            8,
            4,
            16,
            14,
            4,
            19,
            2,
            11,
            9,
            19,
            5,
            12,
            19,
            9,
            10,
            7,
            3,
            10,
            11,
            9,
            18,
            4,
            16,
            6,
            16,
            17,
            6,
            19,
            7,
            13,
            17,
            12,
            3,
            14,
            6,
            8,
            11,
            6,
            10,
            14,
            10,
            18,
            9,
            18,
            18,
            3,
            13,
            18,
            15,
            17,
            18,
            16,
            14,
            11,
            20,
            13,
            2,
            9,
            9,
            11,
            10,
            15,
            9,
            15,
            8,
            16,
            19,
            12,
            18,
            18,
            10,
            11,
            20,
            13,
            6,
            11,
            3,
            2,
            2,
            18,
            10,
            14,
            7,
            6,
            9,
            8,
            7,
            9,
            20,
            3,
            19,
            10,
            15,
            12,
            15,
            12,
            7,
            4,
            12,
            6,
            8,
            14,
            19,
            19,
            3,
            4,
            8,
            18,
            10,
            9,
            12,
            15,
            17,
            8,
            17,
            17,
            15,
            15,
            10,
            14,
            8,
            5,
            2,
            3,
            10,
            16,
            10,
            18,
            6,
            14,
            14,
            9,
            17,
            17,
            3,
            6,
            16,
            14,
            4,
            6,
            10,
            15,
            20,
            6,
            6,
            15,
            17,
            16,
            5,
            9,
            12,
            5,
            17,
            14,
            2,
            9,
            5,
            17,
            14,
            3,
            12,
            4,
            13,
            4,
            17,
            3,
            5,
            3,
            9,
            6,
            18,
            7,
            20,
            7,
            19,
            13,
            15,
            19,
            12,
            2,
            18,
            12,
            4,
            8,
            20,
            5,
            16,
            6,
            11,
            17,
            14,
            9,
            9,
            12,
            9,
            8,
            5,
            2,
            13,
            18,
            5,
            20,
            20,
            16,
            13,
            11,
            13,
            8,
            9,
            20,
            2,
            9,
            17,
            5,
            9,
            3,
            15,
            17,
            2,
            5,
            3,
            8,
            12,
            16,
            9,
            7,
            14,
            18,
            7,
            5,
            12,
            8,
            15,
            13,
            20,
            2,
            11,
            20,
            4,
            18,
            11,
            7,
            4,
            18,
            9,
            15,
            5,
            19,
            19,
            7,
            8,
            18,
            8,
            2,
            4,
            5,
            18,
            4,
            11,
            10,
            17,
            20,
            10,
            5,
            2,
            14,
            2,
            4,
            16,
            19,
            18,
            4,
            16,
            7,
            4,
            12,
            13,
            6,
            9,
            13,
            11,
            18,
            15,
            6,
            6,
            13,
            13,
            15,
            16,
            20,
            16,
            13,
            9,
            14,
            18,
            17,
            8,
            3,
            16,
            6,
            7,
            2,
            14,
            8,
            8,
            20,
            13,
            12,
            20,
            2,
            19,
            4,
            3,
            18,
            10,
            8,
            11,
            3,
            19,
            2,
            16,
            8,
            8,
            10,
            11,
            8,
            8,
            4,
            19,
            15,
            17,
            7,
            14,
            15,
            2,
            5,
            7,
            13,
            3,
            12,
            12,
            3,
            16,
            13,
            17,
            4,
            15,
            14,
            18,
            7,
            19,
            14,
            8,
            10,
            15,
            3,
            14,
            3,
            6,
            2,
            5,
            13,
            11,
            7,
            15,
            2,
            12,
            4,
            7,
            7,
            17,
            14,
            9,
            20,
            11,
            20,
            11,
            13,
            19,
            13,
            4,
            13,
            16,
            7,
            18,
            8,
            4,
            17,
            14,
            13,
            15,
            14,
            11,
            14,
            5,
            11,
            11,
            15,
            13,
            17,
            8,
            15,
            14,
            4,
            18,
            9,
            12,
            19,
            12,
            15,
            20,
            15,
            2,
            19,
            12,
            3,
            18,
            6,
            10,
            13,
            9,
            19,
            19,
            14,
            10,
            4,
            10,
            8,
            9,
            18,
            6,
            15,
            14,
            16,
            19,
            5,
            13,
            6,
            16,
            17,
            11,
            18,
            16,
            7,
            6,
            6,
            20,
            18,
            7,
            18,
            6,
            15,
            20,
            20,
            20,
            6,
            17,
            10,
            9,
            19,
            10,
            18,
            2,
            17,
            14,
            9,
            11,
            17,
            5,
            8,
            15,
            11,
            14,
            7,
            7,
            18,
            15,
            13,
            19,
            2,
            6,
            10,
            17,
            17,
            12,
            2,
            8,
            9,
            14,
            15,
            9,
            6,
            13,
            20,
            13,
            4,
            9,
            13,
            4,
            19,
            8,
            6,
            8,
            17,
            9,
            11,
            10,
            12,
            12,
            5,
            9,
            19,
            6,
            15,
            13,
            15,
            7,
            4,
            4,
            19,
            10,
            3,
            3,
            15,
            3,
            6,
            8,
            15,
            14,
            9,
            13,
            16,
            17,
            20,
            9,
            9,
            12,
            9,
            14,
            6,
            10,
            8,
            15,
            16,
            19,
            18,
            11,
            19,
            7,
            16,
            2,
            12,
            7,
            7,
            4,
            16,
            8,
            12,
            8,
            16,
            17,
            2,
            7,
            14,
            5,
            18,
            7,
            6,
            7,
            8,
            19,
            10,
            7,
            15,
            7,
            12,
            3,
            13,
            13,
            6,
            7,
            14,
            3,
            19,
            20,
            3,
            12,
            15,
            19,
            4,
            4,
            3,
            3,
            8,
            4,
            7,
            6,
            3,
            12,
            19,
            13,
            16,
            6,
            17,
            9,
            13,
            10,
            3,
            19,
            7,
            2,
            4,
            18,
            19,
            3,
            17,
            7,
            8,
            9,
            6,
            20,
            17,
            10,
            8,
            9,
            2,
            10,
            13,
            7,
            8,
            10,
            15,
            17,
            17,
            14,
            2,
            15,
            4,
            7,
            7,
            19,
            11,
            12,
            8,
            7,
            17,
            13,
            18,
            8,
            18,
            16,
            15,
            7,
            11,
            9,
            20,
            20,
            12,
            10,
            5,
            3,
            15,
            13,
            16,
            16,
            9,
            11,
            15,
            9,
            17,
            14,
            2,
            5,
            2,
            20,
            6,
            11,
            5,
            17,
            9,
            12,
            4,
            10,
            3,
            19,
            18,
            13,
            11,
            8,
            5,
            14,
            6,
            15,
            3,
            4,
            15,
            14,
            19,
            10,
            2,
            16,
            14,
            7,
            8,
            6,
            4,
            15,
            8,
            8,
            12,
            20,
            4,
            12,
            16,
            9,
            2,
            19,
            13,
            20,
            20,
            5,
            5,
            14,
            9,
            16,
            6,
            17,
            20,
            9,
            15,
            4,
            16,
            4,
            3,
            5,
            20,
            5,
            14,
            17,
            16,
            5,
            10,
            15,
            7,
            11,
            11,
            12,
            11,
            20,
            13,
            19,
            5,
            16,
            2,
            6,
            4,
            9,
            14,
            9,
            4,
            16,
            14,
            20,
            6,
            2,
            4,
            12,
            11,
            20,
            19,
            15,
            18,
            11,
            16,
            8,
            4,
            8,
            5,
            20,
            8,
            9,
            3,
            18,
            15,
            4,
            15,
            12,
            17,
            5,
            17,
            14,
            17,
            15,
            17,
            6,
            14,
            5,
            20,
            3,
            5,
            18,
            13,
            6,
            2,
            18,
            5,
            13,
            10,
            19,
            7,
            10,
            10,
            5,
            3,
            12,
            6,
            2,
            19,
            6,
            9,
            8,
            4,
            5,
            11,
            14,
            8,
            3,
            10,
            2,
            10,
            10,
            14,
            6,
            17,
            5,
            11,
            18,
            6,
            10,
            6,
            4,
            20,
            7,
            5,
            4,
            19,
            13,
            10,
            6,
            13,
            19,
            7,
            13,
            17,
            18,
            17,
            15,
            20,
            3,
            19,
            16,
            20,
            11,
            19,
            8,
            2,
            6,
            15,
            12,
            10,
            16,
            2,
            4,
            14,
            12,
            20,
            11,
            12,
            19,
            6,
            11,
            19,
            12,
            15,
            6,
            8,
            4,
            8,
            15,
            14,
            8,
            18,
            8,
            14,
            2,
            9,
            17,
            13,
            16,
            6,
            16,
            10,
            12,
            6,
            11,
            16,
            3,
            19,
            18,
            16,
            14,
            7,
            17,
            19,
            8,
            11,
            10,
            2,
            10,
            2,
            16,
            19,
            12,
            3,
            14,
            9,
            19,
            2,
            10,
            20,
            3,
            7,
            7,
            6,
            13,
            4,
            17,
            13,
            19,
            17,
            12,
            11,
            19,
            9,
            6,
            18,
            5,
            10,
            13,
            19,
            15,
            20,
            5,
            6,
            18,
            4,
            9,
            5,
            20,
            8,
            20,
            8,
            16,
            14,
            12,
            10,
            16,
            15,
            14,
            16,
            18,
            6,
            7,
            17,
            16,
            12,
            17,
            5,
            14,
            18,
            19,
            14,
            19,
            2,
            7,
            6,
            12,
            10,
            18,
            8,
            16,
            18,
            9,
            7,
            19,
            7,
            16,
            19,
            16,
            16,
            5,
            19,
            16,
            16,
            13,
            20,
            9,
            11,
            20,
            18,
            5,
            19,
            14,
            18,
            19,
            3,
            4,
            15,
            7,
            5,
            12,
            3,
            9,
            4,
            16,
            19,
            6,
            15,
            18,
            12,
            9,
            11,
            12,
            6,
            19,
            16,
            17,
            6,
            3,
            7,
            16,
            15,
            8,
            15,
            2,
            19,
            10,
            3,
            2,
            11,
            17,
            12,
            6,
            3,
            6,
            20,
            7,
            14,
            4,
            20,
            19,
            17,
            20,
            11,
            15,
            20,
            14,
            11,
            20,
            2,
            7,
            11,
            7,
            9,
            3,
            19,
            5,
            2,
            15,
            12,
            16,
            20,
            13,
            10,
            11,
            14,
            12,
            2,
            4,
            7,
            18,
            19,
            20,
            15,
            18,
            9,
            17,
            9,
            14,
            9,
            12,
            6,
            18,
            17,
            2,
            15,
            3,
            4,
            18,
            18,
            18,
            14,
            20,
            2,
            8,
            2,
            2,
            13,
            13,
            11,
            11,
            9,
            17,
            17,
            16,
            8,
            4,
            3,
            12,
            17,
            15,
            8,
            10,
            4,
            2,
            3,
            8,
            5,
            3,
            10,
            3,
            6,
            15,
            18,
            10,
            14,
            17,
            10,
            7,
            4,
            14,
            15,
            8,
            7,
            20,
            16,
            9,
            16,
            18,
            13,
            16,
            19,
            13,
            11,
            6,
            5,
            2,
            4,
            10,
            18,
            17,
            4,
            20,
            17,
            18,
            20,
            2,
            9,
            8,
            20,
            11,
            14,
            11,
            15,
            19,
            18,
            5,
            5,
            18,
            6,
            10,
            11,
            8,
            5,
            16,
            16,
            12,
            2,
            5,
            3,
            8,
            10,
            13,
            5,
            10,
            3,
            4,
            9,
            6,
            20,
            16,
            12,
            11,
            4,
            15,
            15,
            12,
            7,
            10,
            10,
            3,
            7,
            14,
            3,
            4,
            11,
            18,
            13,
            20,
            20,
            12,
            12,
            2,
            14,
            17,
            4,
            16,
            10,
            19,
            19,
            18,
            4,
            4,
            4,
            2,
            11,
            3,
            7,
            4,
            4,
            15,
            8,
            16,
            18,
            6,
            16,
            19,
            13,
            9,
            8,
            9,
            18,
            15,
            17,
            8,
            11,
            7,
            5,
            17,
            7,
            12,
            20,
            7,
            19,
            13,
            16,
            18,
            17,
            12,
            2,
            6,
            13,
            20,
            15,
            14,
            11,
            5,
            15,
            2,
            16,
            20,
            15,
            4,
            19,
            7,
            6,
            11,
            20,
            10,
            2,
            16,
            10,
            5,
            11,
            3,
            14,
            18,
            5,
            17,
            6,
            8,
            7,
            13,
            9,
            3,
            9,
            10,
            11,
            4,
            19,
            6,
            8,
            12,
            9,
            2,
            16,
            7,
            15,
            11,
            19,
            16,
            14,
            19,
            17,
            16,
            2,
            8,
            11,
            3,
            13,
            6,
            11,
            16,
            7,
            13,
            19,
            10,
            5,
            14,
            13,
            5,
            7,
            3,
            6,
            13,
            5,
            20,
            17,
            19,
            18,
            14,
            13,
            5,
            15,
            17,
            12,
            7,
            11,
            12,
            17,
            17,
            4,
            20,
            9,
            15,
            4,
            9,
            20,
            4,
            14,
            5,
            7,
            7,
            12,
            19,
            16,
            5,
            16,
            19,
            2,
            5,
            6,
            3,
            14,
            7,
            4,
            8,
            17,
            10,
            9,
            16,
            17,
            15,
            17,
            3,
            2,
            4,
            3,
            4,
            10,
            14,
            5,
            3,
            2,
            11,
            11,
            8,
            4,
            15,
            9,
            8,
            2,
            16,
            11,
            19,
            6,
            2,
            7,
            6,
            2,
            10,
            14,
            7,
            8,
            2,
            7,
            4,
            20,
            4,
            9,
            17,
            11,
            2,
            11,
            17,
            10,
            16,
            5,
            9,
            10,
            10,
            7,
            12,
            15,
            13,
            15,
            7,
            16,
            4,
            14,
            7,
            3,
            10,
            18,
            19,
            4,
            17,
            10,
            2,
            3,
            17,
            3,
            17,
            9,
            5,
            4,
            12,
            3,
            14,
            19,
            20,
            7,
            2,
            13,
            5,
            9,
            12,
            7,
            11,
            3,
            5,
            19,
            4,
            11,
            4,
            16,
            7,
            2,
            17,
            12,
            6,
            20,
            13,
            10,
            3,
            4,
            14,
            16,
            13,
            20,
            3,
            19,
            3,
            9,
            19,
            20,
            8,
            4,
            15,
            14,
            8,
            3,
            15,
            8,
            8,
            17,
            7,
            14,
            13,
            2,
            14,
            15,
            10,
            11,
            16,
            2,
            11,
            4,
            6,
            5,
            8,
            7,
            19,
            3,
            7,
            20,
            4,
            5,
            3,
            19,
            16,
            5,
            19,
            2
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "three_g",
           "values": [
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "touch_screen",
           "values": [
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "wifi",
           "values": [
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "price_range",
           "values": [
            1,
            2,
            2,
            2,
            1,
            1,
            3,
            0,
            0,
            0,
            3,
            3,
            1,
            2,
            0,
            0,
            3,
            3,
            1,
            1,
            3,
            3,
            1,
            0,
            1,
            2,
            3,
            3,
            2,
            0,
            3,
            0,
            1,
            3,
            0,
            1,
            1,
            3,
            2,
            2,
            2,
            3,
            1,
            1,
            0,
            1,
            3,
            1,
            0,
            3,
            2,
            2,
            3,
            1,
            2,
            2,
            0,
            3,
            1,
            2,
            1,
            0,
            0,
            2,
            2,
            3,
            3,
            3,
            0,
            3,
            3,
            2,
            0,
            3,
            0,
            1,
            0,
            0,
            3,
            1,
            3,
            2,
            3,
            1,
            1,
            1,
            0,
            3,
            2,
            2,
            3,
            2,
            0,
            0,
            1,
            3,
            0,
            2,
            0,
            1,
            3,
            1,
            1,
            0,
            0,
            1,
            3,
            3,
            3,
            3,
            1,
            1,
            0,
            2,
            3,
            3,
            2,
            1,
            0,
            1,
            2,
            3,
            3,
            3,
            2,
            3,
            2,
            1,
            3,
            0,
            3,
            1,
            2,
            1,
            2,
            2,
            1,
            3,
            0,
            0,
            2,
            0,
            3,
            2,
            0,
            3,
            1,
            3,
            2,
            2,
            1,
            3,
            1,
            2,
            0,
            1,
            0,
            0,
            3,
            1,
            2,
            2,
            0,
            0,
            2,
            3,
            1,
            2,
            3,
            1,
            3,
            1,
            2,
            2,
            3,
            2,
            0,
            3,
            2,
            1,
            0,
            0,
            2,
            3,
            3,
            0,
            2,
            3,
            1,
            3,
            1,
            2,
            3,
            3,
            2,
            2,
            2,
            3,
            1,
            1,
            0,
            1,
            0,
            2,
            2,
            2,
            2,
            1,
            3,
            3,
            2,
            2,
            0,
            2,
            0,
            3,
            0,
            1,
            1,
            3,
            0,
            0,
            3,
            2,
            3,
            3,
            3,
            2,
            2,
            3,
            3,
            3,
            0,
            1,
            2,
            0,
            1,
            3,
            1,
            0,
            2,
            2,
            3,
            2,
            3,
            3,
            0,
            1,
            1,
            0,
            2,
            2,
            1,
            0,
            0,
            2,
            0,
            1,
            0,
            3,
            1,
            1,
            0,
            0,
            3,
            1,
            3,
            1,
            2,
            1,
            0,
            0,
            3,
            1,
            2,
            2,
            3,
            1,
            0,
            0,
            3,
            3,
            1,
            1,
            2,
            1,
            3,
            3,
            1,
            1,
            3,
            3,
            2,
            3,
            0,
            1,
            3,
            1,
            0,
            1,
            3,
            0,
            0,
            3,
            0,
            1,
            1,
            2,
            0,
            2,
            3,
            2,
            3,
            0,
            2,
            3,
            1,
            0,
            0,
            1,
            0,
            3,
            3,
            0,
            0,
            0,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            3,
            0,
            2,
            2,
            2,
            1,
            2,
            3,
            0,
            1,
            2,
            2,
            2,
            2,
            1,
            3,
            0,
            2,
            2,
            1,
            2,
            0,
            0,
            3,
            1,
            0,
            3,
            2,
            0,
            0,
            2,
            3,
            2,
            1,
            3,
            1,
            3,
            0,
            3,
            2,
            0,
            1,
            0,
            3,
            1,
            2,
            2,
            2,
            2,
            3,
            3,
            1,
            2,
            3,
            3,
            1,
            3,
            3,
            1,
            3,
            3,
            0,
            1,
            3,
            2,
            3,
            3,
            2,
            2,
            3,
            3,
            0,
            2,
            2,
            0,
            3,
            1,
            2,
            3,
            2,
            1,
            1,
            1,
            0,
            1,
            2,
            2,
            0,
            2,
            2,
            0,
            3,
            1,
            1,
            3,
            1,
            3,
            1,
            0,
            0,
            3,
            1,
            3,
            0,
            0,
            1,
            3,
            1,
            0,
            0,
            2,
            2,
            2,
            1,
            1,
            1,
            3,
            0,
            3,
            2,
            1,
            3,
            1,
            0,
            2,
            2,
            1,
            0,
            0,
            3,
            3,
            0,
            0,
            1,
            3,
            3,
            1,
            0,
            2,
            2,
            3,
            0,
            3,
            3,
            0,
            3,
            0,
            3,
            1,
            0,
            2,
            2,
            1,
            2,
            0,
            3,
            2,
            0,
            1,
            2,
            3,
            1,
            2,
            3,
            0,
            3,
            1,
            0,
            0,
            1,
            2,
            2,
            2,
            2,
            3,
            3,
            0,
            1,
            1,
            2,
            1,
            3,
            3,
            3,
            3,
            1,
            3,
            3,
            3,
            3,
            2,
            3,
            2,
            2,
            0,
            0,
            3,
            1,
            2,
            2,
            0,
            2,
            2,
            1,
            0,
            3,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            3,
            1,
            0,
            1,
            0,
            0,
            2,
            2,
            2,
            0,
            3,
            1,
            2,
            1,
            2,
            0,
            0,
            1,
            3,
            3,
            3,
            3,
            3,
            3,
            0,
            2,
            2,
            0,
            0,
            3,
            3,
            3,
            0,
            0,
            3,
            0,
            0,
            1,
            2,
            0,
            2,
            3,
            1,
            3,
            2,
            0,
            2,
            2,
            0,
            0,
            1,
            3,
            0,
            1,
            1,
            2,
            2,
            0,
            2,
            2,
            0,
            1,
            3,
            0,
            2,
            2,
            3,
            1,
            3,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            3,
            1,
            1,
            3,
            1,
            3,
            2,
            1,
            1,
            0,
            2,
            0,
            2,
            0,
            2,
            0,
            1,
            3,
            1,
            2,
            2,
            0,
            1,
            0,
            3,
            2,
            1,
            3,
            2,
            0,
            0,
            1,
            2,
            0,
            0,
            0,
            2,
            3,
            1,
            2,
            1,
            0,
            0,
            0,
            2,
            3,
            2,
            3,
            0,
            3,
            2,
            1,
            2,
            1,
            0,
            2,
            0,
            0,
            1,
            3,
            0,
            3,
            1,
            1,
            1,
            3,
            2,
            2,
            0,
            0,
            3,
            0,
            3,
            3,
            2,
            3,
            3,
            3,
            0,
            2,
            1,
            2,
            0,
            3,
            1,
            0,
            1,
            3,
            1,
            3,
            3,
            1,
            0,
            1,
            1,
            2,
            1,
            3,
            3,
            0,
            2,
            0,
            3,
            2,
            0,
            0,
            2,
            1,
            2,
            3,
            3,
            2,
            1,
            2,
            2,
            3,
            1,
            2,
            3,
            3,
            1,
            1,
            2,
            1,
            2,
            0,
            2,
            0,
            3,
            0,
            0,
            0,
            3,
            3,
            0,
            2,
            2,
            3,
            1,
            3,
            0,
            1,
            2,
            0,
            0,
            0,
            3,
            2,
            0,
            2,
            2,
            1,
            0,
            2,
            0,
            3,
            3,
            2,
            3,
            0,
            3,
            2,
            3,
            2,
            2,
            1,
            0,
            3,
            3,
            1,
            0,
            0,
            2,
            1,
            2,
            3,
            3,
            3,
            3,
            0,
            0,
            0,
            1,
            0,
            2,
            0,
            1,
            2,
            0,
            0,
            3,
            3,
            1,
            3,
            3,
            2,
            3,
            3,
            1,
            1,
            3,
            0,
            3,
            3,
            3,
            3,
            2,
            2,
            3,
            2,
            1,
            3,
            2,
            3,
            0,
            0,
            3,
            0,
            0,
            3,
            2,
            1,
            0,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            0,
            2,
            0,
            2,
            2,
            2,
            3,
            0,
            0,
            3,
            3,
            1,
            0,
            1,
            0,
            0,
            3,
            3,
            3,
            0,
            2,
            2,
            1,
            2,
            2,
            2,
            0,
            3,
            1,
            1,
            3,
            3,
            1,
            2,
            3,
            2,
            2,
            0,
            3,
            3,
            0,
            2,
            2,
            0,
            3,
            3,
            0,
            1,
            1,
            0,
            1,
            2,
            1,
            3,
            1,
            0,
            0,
            3,
            2,
            0,
            1,
            0,
            1,
            2,
            1,
            1,
            2,
            2,
            0,
            3,
            2,
            3,
            1,
            2,
            3,
            3,
            2,
            2,
            2,
            3,
            0,
            0,
            1,
            0,
            3,
            0,
            0,
            2,
            1,
            1,
            1,
            3,
            0,
            1,
            3,
            3,
            3,
            1,
            3,
            2,
            3,
            1,
            1,
            2,
            3,
            0,
            0,
            1,
            1,
            1,
            0,
            3,
            2,
            0,
            1,
            3,
            0,
            3,
            1,
            0,
            3,
            3,
            1,
            0,
            0,
            0,
            3,
            3,
            1,
            1,
            1,
            1,
            0,
            0,
            3,
            2,
            0,
            3,
            3,
            2,
            0,
            0,
            0,
            2,
            3,
            0,
            1,
            3,
            1,
            2,
            0,
            0,
            2,
            0,
            2,
            0,
            3,
            3,
            1,
            3,
            3,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            0,
            0,
            3,
            0,
            1,
            1,
            0,
            3,
            2,
            0,
            1,
            2,
            3,
            2,
            3,
            1,
            1,
            1,
            1,
            3,
            1,
            1,
            2,
            2,
            1,
            2,
            3,
            1,
            2,
            0,
            1,
            0,
            3,
            0,
            0,
            0,
            0,
            1,
            3,
            0,
            1,
            1,
            0,
            3,
            3,
            2,
            1,
            0,
            1,
            2,
            2,
            2,
            1,
            1,
            0,
            1,
            2,
            2,
            2,
            0,
            0,
            1,
            3,
            1,
            3,
            0,
            0,
            1,
            0,
            3,
            1,
            0,
            0,
            2,
            3,
            3,
            1,
            3,
            3,
            1,
            3,
            2,
            0,
            2,
            2,
            3,
            3,
            3,
            3,
            1,
            2,
            2,
            1,
            2,
            0,
            2,
            3,
            2,
            2,
            1,
            0,
            1,
            1,
            0,
            2,
            0,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            0,
            1,
            1,
            2,
            1,
            1,
            3,
            0,
            1,
            3,
            0,
            2,
            0,
            2,
            2,
            2,
            2,
            2,
            0,
            0,
            2,
            1,
            2,
            0,
            3,
            1,
            3,
            2,
            2,
            3,
            2,
            0,
            3,
            3,
            1,
            0,
            0,
            3,
            2,
            2,
            0,
            3,
            2,
            0,
            3,
            2,
            0,
            1,
            2,
            2,
            1,
            0,
            2,
            0,
            2,
            1,
            2,
            1,
            1,
            3,
            0,
            1,
            1,
            2,
            0,
            3,
            1,
            2,
            2,
            1,
            3,
            2,
            0,
            2,
            2,
            2,
            3,
            0,
            0,
            3,
            2,
            1,
            2,
            0,
            0,
            2,
            3,
            2,
            0,
            0,
            0,
            2,
            3,
            0,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            0,
            2,
            1,
            3,
            3,
            0,
            0,
            2,
            3,
            1,
            3,
            1,
            0,
            3,
            3,
            0,
            1,
            3,
            1,
            1,
            2,
            3,
            0,
            3,
            0,
            1,
            1,
            3,
            2,
            0,
            0,
            3,
            1,
            3,
            1,
            2,
            2,
            3,
            0,
            3,
            1,
            1,
            0,
            3,
            0,
            2,
            0,
            2,
            2,
            3,
            2,
            0,
            3,
            1,
            0,
            3,
            3,
            1,
            1,
            0,
            3,
            1,
            3,
            1,
            1,
            2,
            0,
            1,
            3,
            3,
            3,
            3,
            1,
            3,
            0,
            1,
            1,
            0,
            1,
            3,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            3,
            3,
            2,
            1,
            1,
            2,
            1,
            2,
            3,
            2,
            0,
            3,
            3,
            1,
            2,
            3,
            2,
            1,
            1,
            2,
            1,
            1,
            0,
            0,
            0,
            0,
            2,
            1,
            1,
            2,
            0,
            2,
            0,
            2,
            2,
            0,
            3,
            1,
            0,
            3,
            0,
            1,
            3,
            3,
            0,
            0,
            1,
            1,
            3,
            2,
            3,
            0,
            2,
            1,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            0,
            3,
            0,
            2,
            2,
            0,
            1,
            1,
            2,
            0,
            0,
            1,
            1,
            0,
            2,
            2,
            2,
            0,
            1,
            3,
            0,
            0,
            2,
            2,
            3,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            3,
            0,
            3,
            0,
            1,
            0,
            0,
            0,
            0,
            3,
            2,
            1,
            0,
            2,
            1,
            0,
            0,
            1,
            2,
            0,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            3,
            3,
            0,
            3,
            2,
            0,
            1,
            2,
            3,
            3,
            0,
            1,
            1,
            1,
            1,
            3,
            1,
            2,
            0,
            2,
            3,
            1,
            3,
            1,
            1,
            2,
            0,
            1,
            2,
            3,
            0,
            1,
            3,
            0,
            0,
            0,
            0,
            0,
            3,
            1,
            2,
            2,
            2,
            1,
            1,
            3,
            0,
            1,
            2,
            2,
            3,
            1,
            1,
            3,
            0,
            1,
            0,
            1,
            3,
            3,
            3,
            3,
            1,
            1,
            0,
            1,
            2,
            2,
            3,
            1,
            1,
            0,
            3,
            3,
            0,
            2,
            2,
            1,
            2,
            2,
            0,
            2,
            3,
            3,
            3,
            2,
            2,
            3,
            2,
            2,
            1,
            1,
            1,
            2,
            0,
            1,
            0,
            3,
            2,
            0,
            2,
            2,
            2,
            2,
            1,
            0,
            0,
            0,
            1,
            2,
            1,
            0,
            3,
            2,
            0,
            3,
            2,
            0,
            1,
            2,
            1,
            0,
            0,
            0,
            2,
            0,
            3,
            1,
            1,
            2,
            3,
            3,
            2,
            1,
            2,
            1,
            2,
            3,
            0,
            2,
            3,
            1,
            0,
            3,
            1,
            0,
            2,
            1,
            2,
            1,
            2,
            3,
            2,
            3,
            2,
            3,
            3,
            3,
            3,
            2,
            2,
            2,
            3,
            1,
            1,
            1,
            0,
            0,
            3,
            2,
            1,
            0,
            3,
            3,
            1,
            0,
            3,
            2,
            1,
            2,
            3,
            2,
            2,
            3,
            1,
            1,
            1,
            1,
            0,
            2,
            1,
            0,
            1,
            0,
            3,
            0,
            3,
            1,
            1,
            3,
            3,
            1,
            1,
            1,
            3,
            2,
            1,
            3,
            1,
            1,
            2,
            3,
            0,
            1,
            1,
            3,
            2,
            1,
            2,
            2,
            3,
            2,
            1,
            1,
            1,
            3,
            3,
            3,
            0,
            3,
            2,
            3,
            1,
            3,
            2,
            1,
            2,
            3,
            3,
            0,
            0,
            0,
            2,
            2,
            0,
            2,
            0,
            2,
            1,
            3,
            3,
            0,
            1,
            2,
            0,
            2,
            0,
            2,
            2,
            2,
            0,
            2,
            0,
            2,
            0,
            2,
            1,
            2,
            3,
            1,
            3,
            0,
            0,
            2,
            2,
            3,
            3,
            1,
            2,
            1,
            1,
            3,
            0,
            0,
            2,
            1,
            0,
            1,
            2,
            1,
            3,
            0,
            1,
            3,
            0,
            2,
            3,
            3,
            2,
            0,
            3,
            1,
            1,
            0,
            2,
            2,
            2,
            0,
            3,
            3,
            0,
            3,
            2,
            3,
            0,
            2,
            0,
            2,
            1,
            3,
            3,
            0,
            3,
            2,
            0,
            2,
            1,
            2,
            3,
            1,
            1,
            3,
            0,
            1,
            1,
            3,
            0,
            1,
            0,
            1,
            2,
            0,
            1,
            3,
            3,
            1,
            2,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            3,
            3,
            1,
            0,
            2,
            0,
            3,
            1,
            2,
            2,
            1,
            2,
            3,
            3,
            0,
            0,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            0,
            2,
            3,
            0,
            0,
            3,
            0,
            3,
            0,
            0,
            3,
            3,
            0,
            3,
            0,
            0,
            0,
            0,
            3,
            3,
            2,
            0,
            0,
            0,
            0,
            3,
            0,
            2,
            3,
            2,
            3,
            3,
            2,
            3,
            1,
            0,
            2,
            0,
            0,
            1,
            1,
            1,
            2,
            2,
            0,
            0,
            3,
            0,
            1,
            2,
            0,
            3,
            1,
            0,
            0,
            3,
            0,
            1,
            0,
            3,
            3,
            0,
            1,
            2,
            2,
            2,
            3,
            0,
            1,
            3,
            3,
            1,
            3,
            0,
            1,
            0,
            0,
            2,
            1,
            0,
            2,
            0,
            3,
            1,
            0,
            2,
            1,
            2,
            1,
            3,
            0,
            2,
            3,
            1,
            1,
            1,
            2,
            1,
            3,
            2,
            2,
            2,
            2,
            3,
            1,
            0,
            0,
            0,
            0,
            2,
            1,
            2,
            1,
            0,
            1,
            3,
            0,
            3,
            0,
            3,
            3,
            1,
            2,
            0,
            2,
            1,
            1,
            0,
            2,
            1,
            0,
            2,
            2,
            1,
            0,
            3,
            1,
            3,
            0,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            3,
            1,
            0,
            1,
            1,
            0,
            3,
            0,
            3,
            3,
            0,
            2,
            3,
            0,
            3
           ]
          }
         ],
         "hovertemplate": "%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<br>price_range=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           1,
           2,
           2,
           2,
           1,
           1,
           3,
           0,
           0,
           0,
           3,
           3,
           1,
           2,
           0,
           0,
           3,
           3,
           1,
           1,
           3,
           3,
           1,
           0,
           1,
           2,
           3,
           3,
           2,
           0,
           3,
           0,
           1,
           3,
           0,
           1,
           1,
           3,
           2,
           2,
           2,
           3,
           1,
           1,
           0,
           1,
           3,
           1,
           0,
           3,
           2,
           2,
           3,
           1,
           2,
           2,
           0,
           3,
           1,
           2,
           1,
           0,
           0,
           2,
           2,
           3,
           3,
           3,
           0,
           3,
           3,
           2,
           0,
           3,
           0,
           1,
           0,
           0,
           3,
           1,
           3,
           2,
           3,
           1,
           1,
           1,
           0,
           3,
           2,
           2,
           3,
           2,
           0,
           0,
           1,
           3,
           0,
           2,
           0,
           1,
           3,
           1,
           1,
           0,
           0,
           1,
           3,
           3,
           3,
           3,
           1,
           1,
           0,
           2,
           3,
           3,
           2,
           1,
           0,
           1,
           2,
           3,
           3,
           3,
           2,
           3,
           2,
           1,
           3,
           0,
           3,
           1,
           2,
           1,
           2,
           2,
           1,
           3,
           0,
           0,
           2,
           0,
           3,
           2,
           0,
           3,
           1,
           3,
           2,
           2,
           1,
           3,
           1,
           2,
           0,
           1,
           0,
           0,
           3,
           1,
           2,
           2,
           0,
           0,
           2,
           3,
           1,
           2,
           3,
           1,
           3,
           1,
           2,
           2,
           3,
           2,
           0,
           3,
           2,
           1,
           0,
           0,
           2,
           3,
           3,
           0,
           2,
           3,
           1,
           3,
           1,
           2,
           3,
           3,
           2,
           2,
           2,
           3,
           1,
           1,
           0,
           1,
           0,
           2,
           2,
           2,
           2,
           1,
           3,
           3,
           2,
           2,
           0,
           2,
           0,
           3,
           0,
           1,
           1,
           3,
           0,
           0,
           3,
           2,
           3,
           3,
           3,
           2,
           2,
           3,
           3,
           3,
           0,
           1,
           2,
           0,
           1,
           3,
           1,
           0,
           2,
           2,
           3,
           2,
           3,
           3,
           0,
           1,
           1,
           0,
           2,
           2,
           1,
           0,
           0,
           2,
           0,
           1,
           0,
           3,
           1,
           1,
           0,
           0,
           3,
           1,
           3,
           1,
           2,
           1,
           0,
           0,
           3,
           1,
           2,
           2,
           3,
           1,
           0,
           0,
           3,
           3,
           1,
           1,
           2,
           1,
           3,
           3,
           1,
           1,
           3,
           3,
           2,
           3,
           0,
           1,
           3,
           1,
           0,
           1,
           3,
           0,
           0,
           3,
           0,
           1,
           1,
           2,
           0,
           2,
           3,
           2,
           3,
           0,
           2,
           3,
           1,
           0,
           0,
           1,
           0,
           3,
           3,
           0,
           0,
           0,
           2,
           1,
           2,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           3,
           0,
           2,
           2,
           2,
           1,
           2,
           3,
           0,
           1,
           2,
           2,
           2,
           2,
           1,
           3,
           0,
           2,
           2,
           1,
           2,
           0,
           0,
           3,
           1,
           0,
           3,
           2,
           0,
           0,
           2,
           3,
           2,
           1,
           3,
           1,
           3,
           0,
           3,
           2,
           0,
           1,
           0,
           3,
           1,
           2,
           2,
           2,
           2,
           3,
           3,
           1,
           2,
           3,
           3,
           1,
           3,
           3,
           1,
           3,
           3,
           0,
           1,
           3,
           2,
           3,
           3,
           2,
           2,
           3,
           3,
           0,
           2,
           2,
           0,
           3,
           1,
           2,
           3,
           2,
           1,
           1,
           1,
           0,
           1,
           2,
           2,
           0,
           2,
           2,
           0,
           3,
           1,
           1,
           3,
           1,
           3,
           1,
           0,
           0,
           3,
           1,
           3,
           0,
           0,
           1,
           3,
           1,
           0,
           0,
           2,
           2,
           2,
           1,
           1,
           1,
           3,
           0,
           3,
           2,
           1,
           3,
           1,
           0,
           2,
           2,
           1,
           0,
           0,
           3,
           3,
           0,
           0,
           1,
           3,
           3,
           1,
           0,
           2,
           2,
           3,
           0,
           3,
           3,
           0,
           3,
           0,
           3,
           1,
           0,
           2,
           2,
           1,
           2,
           0,
           3,
           2,
           0,
           1,
           2,
           3,
           1,
           2,
           3,
           0,
           3,
           1,
           0,
           0,
           1,
           2,
           2,
           2,
           2,
           3,
           3,
           0,
           1,
           1,
           2,
           1,
           3,
           3,
           3,
           3,
           1,
           3,
           3,
           3,
           3,
           2,
           3,
           2,
           2,
           0,
           0,
           3,
           1,
           2,
           2,
           0,
           2,
           2,
           1,
           0,
           3,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           1,
           3,
           1,
           0,
           1,
           0,
           0,
           2,
           2,
           2,
           0,
           3,
           1,
           2,
           1,
           2,
           0,
           0,
           1,
           3,
           3,
           3,
           3,
           3,
           3,
           0,
           2,
           2,
           0,
           0,
           3,
           3,
           3,
           0,
           0,
           3,
           0,
           0,
           1,
           2,
           0,
           2,
           3,
           1,
           3,
           2,
           0,
           2,
           2,
           0,
           0,
           1,
           3,
           0,
           1,
           1,
           2,
           2,
           0,
           2,
           2,
           0,
           1,
           3,
           0,
           2,
           2,
           3,
           1,
           3,
           2,
           2,
           1,
           2,
           2,
           1,
           1,
           0,
           1,
           0,
           1,
           0,
           3,
           1,
           1,
           3,
           1,
           3,
           2,
           1,
           1,
           0,
           2,
           0,
           2,
           0,
           2,
           0,
           1,
           3,
           1,
           2,
           2,
           0,
           1,
           0,
           3,
           2,
           1,
           3,
           2,
           0,
           0,
           1,
           2,
           0,
           0,
           0,
           2,
           3,
           1,
           2,
           1,
           0,
           0,
           0,
           2,
           3,
           2,
           3,
           0,
           3,
           2,
           1,
           2,
           1,
           0,
           2,
           0,
           0,
           1,
           3,
           0,
           3,
           1,
           1,
           1,
           3,
           2,
           2,
           0,
           0,
           3,
           0,
           3,
           3,
           2,
           3,
           3,
           3,
           0,
           2,
           1,
           2,
           0,
           3,
           1,
           0,
           1,
           3,
           1,
           3,
           3,
           1,
           0,
           1,
           1,
           2,
           1,
           3,
           3,
           0,
           2,
           0,
           3,
           2,
           0,
           0,
           2,
           1,
           2,
           3,
           3,
           2,
           1,
           2,
           2,
           3,
           1,
           2,
           3,
           3,
           1,
           1,
           2,
           1,
           2,
           0,
           2,
           0,
           3,
           0,
           0,
           0,
           3,
           3,
           0,
           2,
           2,
           3,
           1,
           3,
           0,
           1,
           2,
           0,
           0,
           0,
           3,
           2,
           0,
           2,
           2,
           1,
           0,
           2,
           0,
           3,
           3,
           2,
           3,
           0,
           3,
           2,
           3,
           2,
           2,
           1,
           0,
           3,
           3,
           1,
           0,
           0,
           2,
           1,
           2,
           3,
           3,
           3,
           3,
           0,
           0,
           0,
           1,
           0,
           2,
           0,
           1,
           2,
           0,
           0,
           3,
           3,
           1,
           3,
           3,
           2,
           3,
           3,
           1,
           1,
           3,
           0,
           3,
           3,
           3,
           3,
           2,
           2,
           3,
           2,
           1,
           3,
           2,
           3,
           0,
           0,
           3,
           0,
           0,
           3,
           2,
           1,
           0,
           2,
           1,
           2,
           2,
           1,
           2,
           2,
           0,
           2,
           0,
           2,
           2,
           2,
           3,
           0,
           0,
           3,
           3,
           1,
           0,
           1,
           0,
           0,
           3,
           3,
           3,
           0,
           2,
           2,
           1,
           2,
           2,
           2,
           0,
           3,
           1,
           1,
           3,
           3,
           1,
           2,
           3,
           2,
           2,
           0,
           3,
           3,
           0,
           2,
           2,
           0,
           3,
           3,
           0,
           1,
           1,
           0,
           1,
           2,
           1,
           3,
           1,
           0,
           0,
           3,
           2,
           0,
           1,
           0,
           1,
           2,
           1,
           1,
           2,
           2,
           0,
           3,
           2,
           3,
           1,
           2,
           3,
           3,
           2,
           2,
           2,
           3,
           0,
           0,
           1,
           0,
           3,
           0,
           0,
           2,
           1,
           1,
           1,
           3,
           0,
           1,
           3,
           3,
           3,
           1,
           3,
           2,
           3,
           1,
           1,
           2,
           3,
           0,
           0,
           1,
           1,
           1,
           0,
           3,
           2,
           0,
           1,
           3,
           0,
           3,
           1,
           0,
           3,
           3,
           1,
           0,
           0,
           0,
           3,
           3,
           1,
           1,
           1,
           1,
           0,
           0,
           3,
           2,
           0,
           3,
           3,
           2,
           0,
           0,
           0,
           2,
           3,
           0,
           1,
           3,
           1,
           2,
           0,
           0,
           2,
           0,
           2,
           0,
           3,
           3,
           1,
           3,
           3,
           1,
           0,
           0,
           0,
           0,
           1,
           1,
           2,
           2,
           2,
           1,
           2,
           0,
           0,
           3,
           0,
           1,
           1,
           0,
           3,
           2,
           0,
           1,
           2,
           3,
           2,
           3,
           1,
           1,
           1,
           1,
           3,
           1,
           1,
           2,
           2,
           1,
           2,
           3,
           1,
           2,
           0,
           1,
           0,
           3,
           0,
           0,
           0,
           0,
           1,
           3,
           0,
           1,
           1,
           0,
           3,
           3,
           2,
           1,
           0,
           1,
           2,
           2,
           2,
           1,
           1,
           0,
           1,
           2,
           2,
           2,
           0,
           0,
           1,
           3,
           1,
           3,
           0,
           0,
           1,
           0,
           3,
           1,
           0,
           0,
           2,
           3,
           3,
           1,
           3,
           3,
           1,
           3,
           2,
           0,
           2,
           2,
           3,
           3,
           3,
           3,
           1,
           2,
           2,
           1,
           2,
           0,
           2,
           3,
           2,
           2,
           1,
           0,
           1,
           1,
           0,
           2,
           0,
           2,
           2,
           2,
           2,
           1,
           2,
           2,
           0,
           1,
           1,
           2,
           1,
           1,
           3,
           0,
           1,
           3,
           0,
           2,
           0,
           2,
           2,
           2,
           2,
           2,
           0,
           0,
           2,
           1,
           2,
           0,
           3,
           1,
           3,
           2,
           2,
           3,
           2,
           0,
           3,
           3,
           1,
           0,
           0,
           3,
           2,
           2,
           0,
           3,
           2,
           0,
           3,
           2,
           0,
           1,
           2,
           2,
           1,
           0,
           2,
           0,
           2,
           1,
           2,
           1,
           1,
           3,
           0,
           1,
           1,
           2,
           0,
           3,
           1,
           2,
           2,
           1,
           3,
           2,
           0,
           2,
           2,
           2,
           3,
           0,
           0,
           3,
           2,
           1,
           2,
           0,
           0,
           2,
           3,
           2,
           0,
           0,
           0,
           2,
           3,
           0,
           2,
           2,
           1,
           1,
           1,
           2,
           1,
           0,
           2,
           1,
           3,
           3,
           0,
           0,
           2,
           3,
           1,
           3,
           1,
           0,
           3,
           3,
           0,
           1,
           3,
           1,
           1,
           2,
           3,
           0,
           3,
           0,
           1,
           1,
           3,
           2,
           0,
           0,
           3,
           1,
           3,
           1,
           2,
           2,
           3,
           0,
           3,
           1,
           1,
           0,
           3,
           0,
           2,
           0,
           2,
           2,
           3,
           2,
           0,
           3,
           1,
           0,
           3,
           3,
           1,
           1,
           0,
           3,
           1,
           3,
           1,
           1,
           2,
           0,
           1,
           3,
           3,
           3,
           3,
           1,
           3,
           0,
           1,
           1,
           0,
           1,
           3,
           1,
           1,
           2,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           3,
           3,
           2,
           1,
           1,
           2,
           1,
           2,
           3,
           2,
           0,
           3,
           3,
           1,
           2,
           3,
           2,
           1,
           1,
           2,
           1,
           1,
           0,
           0,
           0,
           0,
           2,
           1,
           1,
           2,
           0,
           2,
           0,
           2,
           2,
           0,
           3,
           1,
           0,
           3,
           0,
           1,
           3,
           3,
           0,
           0,
           1,
           1,
           3,
           2,
           3,
           0,
           2,
           1,
           0,
           0,
           0,
           0,
           3,
           0,
           0,
           0,
           3,
           0,
           2,
           2,
           0,
           1,
           1,
           2,
           0,
           0,
           1,
           1,
           0,
           2,
           2,
           2,
           0,
           1,
           3,
           0,
           0,
           2,
           2,
           3,
           1,
           1,
           1,
           1,
           2,
           1,
           1,
           3,
           0,
           3,
           0,
           1,
           0,
           0,
           0,
           0,
           3,
           2,
           1,
           0,
           2,
           1,
           0,
           0,
           1,
           2,
           0,
           1,
           1,
           2,
           1,
           1,
           1,
           2,
           1,
           3,
           3,
           0,
           3,
           2,
           0,
           1,
           2,
           3,
           3,
           0,
           1,
           1,
           1,
           1,
           3,
           1,
           2,
           0,
           2,
           3,
           1,
           3,
           1,
           1,
           2,
           0,
           1,
           2,
           3,
           0,
           1,
           3,
           0,
           0,
           0,
           0,
           0,
           3,
           1,
           2,
           2,
           2,
           1,
           1,
           3,
           0,
           1,
           2,
           2,
           3,
           1,
           1,
           3,
           0,
           1,
           0,
           1,
           3,
           3,
           3,
           3,
           1,
           1,
           0,
           1,
           2,
           2,
           3,
           1,
           1,
           0,
           3,
           3,
           0,
           2,
           2,
           1,
           2,
           2,
           0,
           2,
           3,
           3,
           3,
           2,
           2,
           3,
           2,
           2,
           1,
           1,
           1,
           2,
           0,
           1,
           0,
           3,
           2,
           0,
           2,
           2,
           2,
           2,
           1,
           0,
           0,
           0,
           1,
           2,
           1,
           0,
           3,
           2,
           0,
           3,
           2,
           0,
           1,
           2,
           1,
           0,
           0,
           0,
           2,
           0,
           3,
           1,
           1,
           2,
           3,
           3,
           2,
           1,
           2,
           1,
           2,
           3,
           0,
           2,
           3,
           1,
           0,
           3,
           1,
           0,
           2,
           1,
           2,
           1,
           2,
           3,
           2,
           3,
           2,
           3,
           3,
           3,
           3,
           2,
           2,
           2,
           3,
           1,
           1,
           1,
           0,
           0,
           3,
           2,
           1,
           0,
           3,
           3,
           1,
           0,
           3,
           2,
           1,
           2,
           3,
           2,
           2,
           3,
           1,
           1,
           1,
           1,
           0,
           2,
           1,
           0,
           1,
           0,
           3,
           0,
           3,
           1,
           1,
           3,
           3,
           1,
           1,
           1,
           3,
           2,
           1,
           3,
           1,
           1,
           2,
           3,
           0,
           1,
           1,
           3,
           2,
           1,
           2,
           2,
           3,
           2,
           1,
           1,
           1,
           3,
           3,
           3,
           0,
           3,
           2,
           3,
           1,
           3,
           2,
           1,
           2,
           3,
           3,
           0,
           0,
           0,
           2,
           2,
           0,
           2,
           0,
           2,
           1,
           3,
           3,
           0,
           1,
           2,
           0,
           2,
           0,
           2,
           2,
           2,
           0,
           2,
           0,
           2,
           0,
           2,
           1,
           2,
           3,
           1,
           3,
           0,
           0,
           2,
           2,
           3,
           3,
           1,
           2,
           1,
           1,
           3,
           0,
           0,
           2,
           1,
           0,
           1,
           2,
           1,
           3,
           0,
           1,
           3,
           0,
           2,
           3,
           3,
           2,
           0,
           3,
           1,
           1,
           0,
           2,
           2,
           2,
           0,
           3,
           3,
           0,
           3,
           2,
           3,
           0,
           2,
           0,
           2,
           1,
           3,
           3,
           0,
           3,
           2,
           0,
           2,
           1,
           2,
           3,
           1,
           1,
           3,
           0,
           1,
           1,
           3,
           0,
           1,
           0,
           1,
           2,
           0,
           1,
           3,
           3,
           1,
           2,
           1,
           0,
           0,
           1,
           0,
           1,
           0,
           0,
           1,
           3,
           3,
           1,
           0,
           2,
           0,
           3,
           1,
           2,
           2,
           1,
           2,
           3,
           3,
           0,
           0,
           1,
           2,
           2,
           2,
           1,
           2,
           1,
           0,
           2,
           3,
           0,
           0,
           3,
           0,
           3,
           0,
           0,
           3,
           3,
           0,
           3,
           0,
           0,
           0,
           0,
           3,
           3,
           2,
           0,
           0,
           0,
           0,
           3,
           0,
           2,
           3,
           2,
           3,
           3,
           2,
           3,
           1,
           0,
           2,
           0,
           0,
           1,
           1,
           1,
           2,
           2,
           0,
           0,
           3,
           0,
           1,
           2,
           0,
           3,
           1,
           0,
           0,
           3,
           0,
           1,
           0,
           3,
           3,
           0,
           1,
           2,
           2,
           2,
           3,
           0,
           1,
           3,
           3,
           1,
           3,
           0,
           1,
           0,
           0,
           2,
           1,
           0,
           2,
           0,
           3,
           1,
           0,
           2,
           1,
           2,
           1,
           3,
           0,
           2,
           3,
           1,
           1,
           1,
           2,
           1,
           3,
           2,
           2,
           2,
           2,
           3,
           1,
           0,
           0,
           0,
           0,
           2,
           1,
           2,
           1,
           0,
           1,
           3,
           0,
           3,
           0,
           3,
           3,
           1,
           2,
           0,
           2,
           1,
           1,
           0,
           2,
           1,
           0,
           2,
           2,
           1,
           0,
           3,
           1,
           3,
           0,
           2,
           1,
           1,
           1,
           1,
           2,
           2,
           1,
           3,
           1,
           0,
           1,
           1,
           0,
           3,
           0,
           3,
           3,
           0,
           2,
           3,
           0,
           3
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "name": "",
         "showlegend": false,
         "type": "splom"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "price_range"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "dragmode": "select",
        "height": 2000,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 2000
       }
      },
      "text/html": [
       "<div>                            <div id=\"5aafa0d3-4df8-4d01-bd8a-b67026e66e7c\" class=\"plotly-graph-div\" style=\"height:2000px; width:2000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5aafa0d3-4df8-4d01-bd8a-b67026e66e7c\")) {                    Plotly.newPlot(                        \"5aafa0d3-4df8-4d01-bd8a-b67026e66e7c\",                        [{\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"battery_power\", \"values\": [842, 1021, 563, 615, 1821, 1859, 1821, 1954, 1445, 509, 769, 1520, 1815, 803, 1866, 775, 838, 595, 1131, 682, 772, 1709, 1949, 1602, 503, 961, 519, 956, 1453, 851, 1579, 1568, 1319, 1310, 644, 725, 589, 1725, 790, 560, 1347, 1646, 1253, 1656, 1195, 1514, 1723, 1054, 578, 596, 1547, 1760, 1654, 1457, 1073, 1936, 823, 987, 1757, 1063, 1484, 799, 1156, 1720, 702, 616, 1358, 1866, 1242, 1166, 1448, 1407, 605, 1038, 797, 819, 1114, 1234, 1199, 1103, 1589, 999, 1510, 1008, 1127, 1412, 1496, 1083, 668, 1309, 1724, 1977, 885, 879, 1322, 1137, 1355, 1665, 657, 593, 1883, 1732, 543, 1939, 553, 832, 1661, 1657, 1135, 1775, 783, 617, 867, 1310, 1804, 1900, 909, 1084, 1308, 1778, 1701, 772, 825, 1379, 1166, 1659, 826, 1564, 1957, 1571, 1414, 1790, 645, 668, 1652, 1213, 1272, 866, 536, 523, 1753, 1218, 1537, 786, 1678, 1814, 1101, 1461, 1216, 506, 843, 742, 1692, 1485, 1547, 1692, 637, 1224, 1356, 1840, 1481, 961, 1296, 1193, 1441, 517, 748, 1126, 1572, 1569, 1270, 1854, 625, 1577, 534, 946, 685, 1949, 947, 801, 703, 1165, 1082, 959, 1502, 1380, 1266, 1934, 1905, 1831, 596, 1562, 1490, 1523, 640, 1526, 1989, 1308, 609, 1905, 1703, 1445, 1087, 671, 1472, 765, 1642, 1265, 664, 1277, 1395, 1539, 1049, 1827, 903, 1987, 1154, 1336, 1886, 1551, 850, 660, 1225, 1452, 1686, 1634, 1708, 1151, 1578, 1689, 1488, 1313, 1715, 1442, 528, 1523, 1133, 1718, 1330, 1799, 633, 724, 822, 1918, 1891, 1373, 862, 1273, 957, 1617, 893, 1210, 708, 835, 1450, 973, 601, 507, 564, 1559, 754, 728, 1431, 767, 1722, 1992, 1876, 1054, 1283, 1281, 1066, 730, 1053, 1611, 793, 1220, 1662, 1281, 538, 823, 1733, 1429, 1839, 659, 1953, 1172, 612, 1563, 1191, 615, 1893, 1563, 1995, 1517, 832, 1162, 1595, 1329, 928, 1656, 1937, 977, 768, 1464, 1177, 1348, 1956, 1751, 1530, 1997, 1414, 1707, 894, 645, 1163, 1126, 1648, 1170, 508, 1097, 728, 1980, 1504, 1379, 1698, 920, 1144, 1995, 1430, 972, 662, 1867, 1730, 1723, 1882, 803, 580, 668, 1391, 1560, 821, 811, 1989, 1034, 618, 654, 576, 667, 869, 635, 609, 1557, 1604, 1182, 848, 1610, 1828, 880, 1394, 1960, 809, 899, 1976, 879, 916, 763, 508, 825, 1864, 1725, 1108, 1011, 1703, 1067, 1334, 775, 1899, 930, 1058, 1187, 1874, 1482, 1809, 864, 625, 1880, 1138, 560, 1117, 1712, 1836, 1036, 1860, 1375, 1945, 1896, 788, 880, 1323, 1868, 1266, 1042, 1479, 1476, 1552, 1454, 1007, 652, 1504, 1726, 1029, 1582, 1478, 1178, 707, 755, 912, 651, 1483, 914, 1456, 1178, 1503, 1681, 715, 1876, 1190, 1755, 1197, 1048, 1887, 1772, 1421, 1464, 728, 954, 685, 1472, 853, 1469, 1310, 1654, 902, 1448, 1631, 1608, 1991, 1349, 1589, 1844, 712, 972, 1406, 1289, 932, 1747, 1172, 1128, 664, 513, 1742, 1512, 1986, 965, 1067, 1583, 1653, 1433, 765, 1845, 752, 1948, 1077, 932, 1968, 1122, 1588, 601, 1615, 1589, 1417, 1697, 1330, 1476, 1579, 1277, 1089, 1663, 1949, 1138, 685, 1889, 857, 1902, 1225, 1066, 1554, 1337, 1926, 869, 1278, 1773, 1661, 1438, 1661, 1846, 1260, 512, 581, 1872, 687, 1062, 1678, 1417, 1074, 1832, 1039, 1059, 1606, 1928, 1875, 1128, 1748, 1413, 825, 1589, 535, 1780, 1671, 1821, 1076, 532, 777, 1217, 1656, 1185, 577, 737, 704, 525, 504, 793, 569, 1590, 707, 767, 1350, 1117, 641, 1002, 1408, 1900, 726, 1544, 1454, 1230, 1552, 1519, 723, 1191, 1168, 1229, 1758, 1290, 574, 1271, 1170, 1269, 1288, 1366, 1572, 1627, 1701, 1900, 1974, 1197, 587, 934, 1195, 1853, 1164, 1512, 1232, 1813, 946, 739, 704, 1663, 1966, 1219, 612, 1658, 1263, 1395, 621, 652, 1175, 561, 1137, 1835, 1170, 1595, 1719, 1770, 1312, 1878, 1871, 922, 1975, 1212, 1430, 1958, 1836, 557, 539, 880, 1369, 1109, 843, 598, 972, 1944, 1225, 644, 1919, 501, 1620, 1227, 1359, 1914, 1645, 1063, 946, 1231, 1397, 701, 570, 1993, 955, 924, 600, 518, 1063, 920, 1715, 1841, 543, 1762, 1112, 709, 1315, 1762, 1462, 571, 666, 1994, 1583, 1778, 1926, 966, 568, 1897, 1695, 1623, 914, 1721, 890, 600, 1677, 723, 638, 852, 1979, 710, 1034, 600, 1027, 1260, 1793, 675, 658, 1694, 804, 1713, 706, 1362, 1527, 768, 1314, 705, 1403, 1486, 781, 986, 1310, 560, 1348, 1567, 1940, 1979, 561, 1717, 535, 1413, 1358, 1519, 1254, 1591, 977, 1640, 663, 1744, 1624, 1108, 1188, 817, 863, 1397, 1108, 1068, 1702, 808, 1156, 1271, 696, 1981, 614, 1590, 1945, 1135, 1049, 1807, 984, 720, 1536, 771, 503, 675, 1936, 1303, 1004, 1972, 1822, 1159, 1782, 894, 1884, 1648, 798, 802, 1276, 1331, 1620, 1996, 1092, 1018, 545, 554, 1264, 1030, 1432, 558, 829, 1741, 1849, 733, 1872, 1853, 1149, 891, 1911, 550, 576, 1760, 969, 1160, 827, 1786, 774, 819, 1042, 1368, 1592, 1067, 1892, 915, 774, 1848, 1501, 1614, 911, 1520, 1647, 1347, 967, 1442, 1204, 1320, 1800, 1567, 1439, 1422, 1591, 880, 1929, 1642, 1511, 1312, 1852, 972, 691, 807, 1113, 721, 1188, 1512, 805, 918, 1320, 1236, 1387, 1883, 839, 1593, 1722, 1954, 1788, 1628, 1965, 833, 571, 1808, 1860, 1368, 1161, 1224, 902, 787, 854, 1184, 1973, 510, 966, 1438, 1986, 1907, 1489, 1843, 825, 1286, 840, 757, 814, 1195, 767, 1068, 994, 1550, 1878, 623, 1829, 525, 1065, 1425, 1296, 642, 1664, 1498, 981, 1236, 1673, 712, 569, 603, 1332, 1413, 1064, 1444, 1366, 906, 1554, 1187, 1156, 1918, 841, 730, 878, 1059, 1578, 1829, 1132, 1652, 912, 1181, 1497, 1790, 1742, 1549, 1372, 1112, 1005, 783, 1698, 899, 1062, 989, 1983, 1800, 1254, 868, 1205, 1284, 626, 1763, 694, 1265, 816, 904, 979, 1531, 934, 1624, 874, 1027, 1966, 1048, 1496, 959, 1349, 1689, 1558, 582, 1485, 1991, 553, 1231, 805, 764, 894, 1589, 1456, 743, 1061, 1428, 674, 832, 1625, 907, 1631, 1584, 1382, 633, 1444, 852, 922, 1745, 862, 1754, 1008, 1569, 1596, 1807, 1660, 1861, 648, 1379, 1910, 1807, 1923, 1345, 904, 1239, 1330, 516, 672, 892, 1778, 1130, 1359, 1866, 1597, 1046, 1035, 1175, 1261, 1068, 1713, 1688, 1413, 559, 1483, 860, 1090, 686, 818, 1456, 774, 1068, 1373, 1777, 594, 1524, 511, 1402, 965, 1270, 919, 1963, 1977, 1881, 683, 772, 536, 1694, 1633, 1371, 551, 1221, 1000, 1986, 1119, 1699, 1099, 1576, 950, 1081, 606, 502, 957, 1658, 1681, 1983, 1753, 1632, 1949, 1696, 688, 1104, 1122, 504, 1154, 1727, 1674, 1948, 1218, 531, 1057, 576, 1960, 673, 718, 864, 547, 623, 1793, 1720, 812, 980, 643, 1925, 1408, 1522, 587, 1959, 1872, 1043, 1809, 1703, 1095, 1414, 1659, 1540, 1342, 1189, 518, 1449, 516, 1569, 1312, 1893, 1076, 530, 1044, 1330, 1836, 1741, 1869, 1331, 1655, 605, 1245, 1709, 846, 1403, 688, 775, 1154, 1375, 1827, 1697, 741, 1672, 514, 1375, 989, 1510, 1266, 1396, 808, 1542, 745, 1735, 586, 1341, 781, 501, 1224, 1970, 618, 1537, 1018, 1545, 1871, 1729, 680, 1106, 635, 618, 1671, 1562, 1975, 831, 1524, 1528, 1447, 1344, 564, 921, 1413, 831, 802, 1923, 1193, 904, 1285, 1635, 1923, 1851, 727, 1396, 877, 696, 1009, 1526, 987, 1414, 1732, 1587, 1097, 504, 1159, 1910, 865, 1930, 860, 1268, 999, 622, 1250, 763, 1100, 815, 618, 553, 673, 563, 1590, 1031, 909, 1880, 1479, 1271, 709, 1744, 1314, 1136, 1039, 1300, 732, 511, 1092, 921, 1030, 1130, 1299, 1009, 1735, 1129, 708, 894, 530, 1481, 1068, 1487, 1524, 1093, 1816, 627, 989, 1895, 805, 713, 864, 1177, 582, 1898, 1158, 1451, 1820, 984, 580, 1263, 1237, 514, 1515, 721, 603, 1426, 1820, 1215, 697, 717, 1348, 1164, 578, 1971, 1794, 1558, 1597, 1240, 1994, 1221, 989, 1180, 718, 1670, 534, 1566, 1935, 504, 913, 1317, 917, 712, 1083, 1039, 1747, 1449, 1872, 1796, 1097, 1562, 1433, 740, 676, 503, 1020, 896, 1824, 1512, 1053, 1944, 1174, 627, 1602, 1733, 586, 1528, 875, 1602, 1426, 1370, 609, 840, 991, 1724, 584, 860, 1541, 615, 912, 1278, 1365, 1702, 589, 1347, 1644, 956, 1089, 871, 664, 1874, 1928, 888, 1077, 1023, 1426, 831, 1496, 1433, 1095, 643, 1142, 730, 1901, 1510, 1924, 1825, 1275, 987, 1538, 1965, 808, 712, 1507, 912, 769, 948, 507, 1384, 1766, 1407, 614, 1972, 1039, 511, 1811, 1159, 1848, 1988, 1469, 1423, 1974, 835, 1429, 947, 1446, 1332, 1498, 1576, 1398, 1885, 798, 1436, 1998, 1321, 1021, 1339, 1210, 1949, 882, 1549, 1991, 796, 1012, 1318, 1708, 598, 541, 818, 1414, 601, 561, 1616, 1263, 1604, 539, 1071, 826, 771, 1811, 1842, 1420, 1763, 1163, 1805, 610, 1533, 1924, 1801, 1726, 794, 1686, 1444, 1004, 1242, 539, 717, 1540, 672, 1325, 950, 948, 1328, 1447, 1973, 1731, 1617, 1791, 851, 856, 714, 951, 1303, 1550, 1759, 1448, 1987, 908, 820, 904, 667, 1109, 1333, 1352, 1600, 1454, 1489, 1823, 1581, 1672, 1283, 630, 659, 1811, 688, 514, 1933, 915, 1006, 1134, 599, 973, 1180, 1237, 659, 1142, 1002, 1392, 602, 1249, 531, 1450, 1342, 832, 867, 1404, 840, 1368, 1927, 1714, 1201, 1796, 1147, 776, 510, 1045, 1497, 1425, 726, 1576, 714, 1595, 541, 1617, 1154, 1638, 1150, 1254, 1806, 603, 1834, 520, 565, 1689, 742, 1143, 761, 959, 772, 1015, 1824, 1130, 1183, 574, 1472, 1868, 1375, 881, 1742, 1225, 1970, 1186, 1762, 1731, 852, 848, 1575, 1554, 1972, 827, 1063, 1695, 1343, 834, 595, 1081, 911, 844, 1335, 1883, 1128, 826, 1650, 1162, 1517, 963, 1837, 1028, 1831, 571, 1770, 970, 642, 622, 600, 1412, 733, 1070, 875, 1994, 823, 1908, 790, 1330, 1660, 1776, 1611, 1410, 1772, 1280, 1712, 1562, 891, 1957, 1110, 875, 1211, 769, 671, 1872, 1076, 1325, 911, 1273, 1062, 1317, 940, 729, 1494, 1546, 1253, 895, 793, 1628, 625, 1110, 999, 1856, 1715, 1897, 1202, 1171, 964, 1973, 1992, 546, 1093, 1880, 1765, 1640, 1830, 826, 1864, 586, 1206, 832, 848, 1851, 1166, 1776, 1254, 1494, 984, 1179, 513, 557, 744, 1129, 1148, 1793, 1310, 1604, 1676, 1001, 1086, 1986, 634, 502, 1641, 623, 962, 1057, 1862, 555, 1000, 841, 865, 999, 1232, 1194, 612, 1362, 1469, 986, 1843, 1561, 1695, 667, 1768, 1269, 1109, 936, 1713, 1175, 1940, 855, 771, 645, 1307, 1948, 1509, 626, 1190, 1727, 1670, 1150, 1208, 1812, 770, 1559, 1902, 1751, 1416, 1288, 1003, 1715, 1967, 962, 1487, 1569, 1146, 1379, 1504, 1945, 1630, 1125, 614, 854, 1470, 1826, 807, 1996, 1083, 1035, 1521, 1314, 535, 673, 1219, 1606, 1603, 1958, 695, 1441, 1470, 1890, 1152, 1619, 1002, 742, 1306, 1424, 942, 1934, 1290, 930, 1699, 1849, 1922, 548, 916, 1783, 1083, 1698, 1969, 869, 1337, 637, 833, 1248, 1174, 1831, 1002, 1010, 1117, 990, 1564, 1065, 1982, 1607, 1066, 681, 1163, 1285, 753, 1779, 987, 511, 1044, 1855, 946, 657, 1673, 731, 926, 1261, 618, 936, 1485, 1339, 508, 1938, 995, 1086, 1322, 1864, 1944, 581, 1020, 1936, 1329, 808, 994, 1811, 765, 993, 1136, 719, 733, 1946, 1230, 1577, 1514, 579, 574, 1072, 843, 1492, 1807, 909, 1876, 1436, 1251, 636, 1354, 930, 1830, 1424, 583, 648, 1203, 1777, 1299, 1494, 527, 793, 873, 722, 1250, 1358, 1035, 1289, 1445, 1702, 534, 628, 713, 1207, 683, 594, 537, 1565, 1052, 713, 1766, 584, 1541, 837, 935, 665, 680, 877, 1493, 1762, 842, 925, 1692, 1576, 1065, 958, 623, 897, 1110, 616, 1680, 1715, 983, 720, 1660, 1564, 592, 969, 1356, 504, 1546, 689, 1685, 1792, 1786, 1944, 1077, 648, 1702, 1571, 856, 1786, 914, 1615, 649, 1646, 1189, 1043, 1920, 1220, 1748, 907, 1193, 1766, 1278, 1234, 1961, 1745, 1442, 1083, 868, 864, 1142, 608, 983, 1720, 1802, 591, 1205, 1369, 713, 1082, 1719, 1544, 1090, 1991, 1367, 1179, 721, 1549, 1349, 1799, 1075, 684, 1837, 1619, 965, 730, 1361, 1494, 1027, 1807, 709, 689, 955, 1872, 1259, 695, 1969, 759, 936, 1176, 1802, 772, 1318, 591, 569, 764, 1056, 1982, 1830, 1122, 1023, 1283, 1602, 732, 1854, 718, 897, 1405, 798, 1035, 1396, 1749, 1588, 1600, 1522, 1279, 719, 544, 1007, 590, 1788, 748, 757, 1561, 1327, 797, 727, 686, 1515, 555, 1589, 1976, 1884, 1063, 672, 635, 1906, 1753, 659, 1583, 1783, 1667, 640, 1913, 538, 1191, 816, 915, 1157, 1201, 1379, 1483, 1614, 930, 1454, 1784, 1262, 797, 1829, 1139, 618, 1547, 586, 1617, 1882, 674, 1467, 858, 794, 1965, 1911, 1512, 510]}, {\"axis\": {\"matches\": true}, \"label\": \"blue\", \"values\": [0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"clock_speed\", \"values\": [2.2, 0.5, 0.5, 2.5, 1.2, 0.5, 1.7, 0.5, 0.5, 0.6, 2.9, 2.2, 2.8, 2.1, 0.5, 1.0, 0.5, 0.9, 0.5, 0.5, 1.1, 2.1, 2.6, 2.8, 1.2, 1.4, 1.6, 0.5, 1.6, 0.5, 0.5, 0.5, 0.9, 2.2, 2.7, 1.3, 2.3, 1.6, 2.0, 0.5, 2.9, 2.3, 0.5, 1.0, 2.8, 2.9, 1.1, 1.8, 2.6, 2.1, 3.0, 1.4, 1.5, 1.9, 0.5, 2.1, 2.7, 1.3, 0.5, 1.4, 3.0, 2.3, 1.2, 2.0, 2.6, 1.9, 0.5, 1.4, 1.1, 1.5, 0.5, 2.4, 1.0, 1.2, 2.9, 0.6, 2.8, 1.6, 2.5, 1.0, 0.6, 2.9, 0.9, 0.8, 2.9, 2.4, 2.0, 2.9, 0.5, 1.1, 2.0, 2.0, 2.3, 2.5, 1.7, 1.0, 2.3, 0.5, 2.5, 0.5, 0.5, 1.1, 0.5, 2.8, 2.4, 0.7, 1.9, 2.4, 2.0, 1.5, 1.8, 2.3, 1.4, 1.1, 0.5, 2.3, 1.4, 2.6, 1.3, 0.5, 1.6, 2.4, 0.5, 1.1, 2.0, 2.8, 2.4, 2.3, 0.9, 1.4, 2.0, 2.5, 0.5, 2.9, 1.1, 0.5, 2.5, 0.5, 2.4, 2.6, 0.5, 1.8, 2.5, 2.2, 2.1, 1.4, 1.8, 2.4, 3.0, 1.6, 0.5, 2.2, 2.1, 1.0, 2.9, 2.3, 2.3, 0.5, 2.8, 0.5, 0.7, 1.5, 0.9, 0.9, 2.1, 1.4, 1.7, 2.4, 0.6, 2.8, 0.6, 3.0, 1.2, 0.5, 0.5, 1.7, 0.5, 1.3, 0.8, 2.2, 2.7, 2.4, 0.8, 2.6, 1.2, 2.1, 0.5, 2.6, 0.5, 1.4, 1.4, 1.3, 0.5, 2.4, 0.6, 2.1, 2.5, 1.9, 0.5, 0.6, 0.5, 2.4, 1.3, 0.9, 2.3, 0.5, 0.5, 1.5, 1.1, 0.5, 1.8, 2.6, 2.2, 1.7, 1.0, 0.5, 1.2, 0.9, 0.5, 1.1, 1.6, 0.5, 2.1, 0.5, 2.8, 1.4, 2.4, 2.9, 1.9, 1.8, 0.5, 1.8, 1.0, 1.0, 1.7, 1.8, 1.4, 2.5, 0.5, 0.5, 2.2, 2.7, 0.5, 1.9, 2.8, 1.0, 0.8, 1.0, 2.5, 1.3, 2.5, 0.5, 2.7, 1.1, 2.7, 1.5, 1.4, 0.5, 1.8, 1.6, 0.5, 2.7, 0.5, 1.5, 2.6, 1.6, 0.5, 1.1, 1.6, 2.3, 0.6, 2.1, 0.8, 0.5, 1.0, 2.9, 0.5, 2.8, 0.8, 0.5, 2.8, 1.5, 1.2, 2.7, 2.1, 1.9, 1.9, 2.4, 1.3, 0.5, 2.1, 1.7, 1.9, 0.5, 1.6, 2.0, 2.6, 1.0, 0.5, 0.5, 1.7, 1.1, 0.9, 0.5, 0.9, 2.0, 2.2, 0.5, 2.0, 1.8, 1.5, 1.4, 0.5, 1.2, 2.8, 0.7, 0.7, 0.9, 0.8, 2.1, 1.7, 0.5, 2.3, 2.1, 2.1, 2.1, 1.5, 0.5, 0.5, 0.5, 0.5, 2.3, 2.0, 1.0, 1.3, 0.8, 1.2, 2.3, 1.4, 2.3, 1.8, 2.4, 2.9, 2.7, 2.1, 1.3, 2.8, 0.6, 1.0, 0.5, 3.0, 2.8, 1.7, 0.5, 2.1, 2.9, 2.1, 2.0, 0.9, 2.3, 0.8, 0.5, 0.7, 1.5, 2.6, 1.7, 1.6, 2.9, 1.4, 0.5, 2.9, 1.1, 1.5, 1.7, 2.4, 2.6, 0.6, 1.3, 1.5, 2.2, 2.7, 0.5, 1.7, 3.0, 1.9, 1.8, 1.4, 0.5, 0.5, 1.4, 1.7, 2.4, 2.3, 1.1, 1.6, 0.5, 2.5, 1.7, 2.5, 2.0, 1.9, 2.7, 0.5, 1.2, 2.2, 0.5, 2.0, 2.4, 1.5, 0.9, 1.9, 2.8, 0.8, 1.3, 2.1, 2.8, 2.0, 0.8, 2.1, 0.9, 0.5, 2.2, 0.7, 2.5, 2.3, 2.8, 2.2, 1.3, 1.3, 0.9, 3.0, 0.5, 2.8, 0.5, 2.3, 0.5, 0.5, 2.8, 2.0, 2.1, 2.3, 1.8, 0.7, 1.4, 0.9, 1.8, 2.7, 2.0, 2.5, 2.3, 0.5, 1.7, 0.5, 0.9, 1.3, 0.9, 2.1, 1.4, 1.2, 0.5, 2.3, 1.7, 0.5, 1.5, 2.4, 1.6, 0.5, 1.6, 2.9, 0.5, 0.5, 2.8, 2.5, 0.5, 0.9, 1.2, 0.5, 2.1, 0.5, 2.2, 2.7, 0.7, 1.3, 0.5, 1.5, 3.0, 0.9, 2.5, 1.8, 1.1, 2.7, 2.8, 2.0, 2.8, 2.4, 1.6, 0.5, 0.5, 1.7, 1.3, 0.5, 2.5, 0.8, 2.2, 1.4, 0.6, 2.0, 0.5, 2.2, 2.3, 1.3, 2.5, 2.3, 0.5, 2.4, 0.7, 1.3, 2.4, 1.4, 1.8, 0.5, 1.1, 1.7, 0.5, 0.5, 1.4, 0.5, 0.5, 0.6, 0.9, 2.6, 0.8, 2.6, 0.5, 1.6, 1.9, 2.4, 2.4, 2.2, 0.5, 0.5, 1.2, 0.6, 1.9, 0.5, 1.9, 0.5, 0.6, 1.1, 0.8, 2.1, 2.0, 2.9, 2.3, 1.6, 2.2, 1.2, 2.1, 1.9, 2.4, 1.3, 0.5, 0.5, 0.5, 2.8, 0.5, 1.2, 1.3, 2.8, 0.7, 0.8, 0.9, 0.5, 0.5, 1.0, 0.5, 2.3, 1.1, 2.0, 1.0, 1.0, 0.5, 2.9, 0.6, 2.6, 2.3, 2.7, 0.5, 1.4, 1.0, 0.7, 1.4, 0.5, 2.8, 2.7, 0.5, 0.6, 1.0, 2.2, 2.3, 2.9, 0.8, 2.0, 2.3, 0.5, 1.5, 2.0, 1.5, 1.5, 2.6, 0.9, 1.9, 2.1, 1.6, 0.6, 0.5, 1.8, 2.7, 1.8, 2.3, 2.7, 0.5, 0.7, 1.3, 1.5, 2.3, 2.2, 1.1, 0.5, 0.7, 1.8, 1.1, 2.1, 1.2, 2.9, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.8, 2.5, 0.5, 1.9, 1.3, 0.5, 2.7, 0.5, 2.0, 2.6, 2.5, 1.7, 1.6, 0.5, 1.5, 2.1, 1.4, 1.1, 0.6, 2.0, 3.0, 0.5, 1.2, 0.5, 0.5, 2.2, 0.5, 1.8, 0.5, 1.7, 1.0, 1.6, 0.5, 2.6, 2.5, 0.5, 0.6, 1.8, 2.3, 2.7, 0.6, 0.8, 2.5, 0.5, 0.5, 2.7, 0.5, 2.9, 0.5, 2.7, 2.6, 1.1, 1.5, 1.4, 2.0, 1.3, 0.5, 0.9, 2.9, 2.1, 1.3, 1.5, 0.5, 2.5, 2.9, 1.2, 2.2, 2.8, 0.7, 0.8, 1.0, 1.6, 1.3, 1.4, 1.3, 1.6, 2.4, 1.0, 2.0, 2.3, 0.5, 0.5, 0.5, 0.5, 1.9, 1.7, 0.6, 1.6, 2.8, 0.6, 2.1, 0.5, 1.4, 0.8, 0.5, 2.5, 2.4, 1.9, 1.0, 2.9, 1.2, 1.2, 2.8, 2.8, 2.1, 2.1, 0.5, 2.4, 2.4, 2.9, 1.6, 2.6, 2.5, 2.7, 0.7, 0.5, 1.3, 2.9, 1.6, 1.7, 2.8, 0.5, 1.8, 2.3, 2.1, 0.9, 0.5, 2.2, 0.5, 2.0, 0.5, 0.8, 1.9, 2.4, 1.1, 1.1, 0.5, 0.6, 2.5, 2.2, 3.0, 2.1, 1.8, 0.7, 1.1, 2.8, 1.4, 1.1, 1.9, 2.9, 0.5, 1.4, 0.9, 1.6, 2.2, 2.6, 2.1, 0.7, 2.1, 0.9, 1.0, 2.2, 1.7, 0.5, 1.6, 2.5, 1.3, 1.4, 1.3, 2.7, 0.5, 0.6, 1.3, 1.5, 0.5, 0.7, 0.9, 0.5, 0.9, 0.5, 2.7, 2.0, 1.0, 1.0, 0.6, 0.5, 0.5, 0.5, 2.2, 0.6, 0.5, 0.5, 1.6, 1.3, 1.6, 0.6, 0.5, 0.5, 1.5, 1.6, 1.7, 2.0, 1.8, 0.5, 2.0, 2.4, 1.8, 1.5, 2.3, 0.6, 2.3, 1.1, 1.1, 0.8, 1.3, 2.8, 2.5, 0.5, 2.0, 0.5, 2.4, 0.5, 1.2, 1.4, 2.8, 2.9, 0.7, 1.9, 2.8, 2.8, 1.6, 0.5, 0.7, 0.8, 0.6, 2.9, 0.6, 1.7, 2.1, 2.7, 2.3, 2.7, 2.8, 0.5, 1.3, 1.3, 0.5, 2.9, 1.7, 1.0, 0.6, 0.5, 2.3, 0.7, 2.3, 1.6, 0.5, 2.7, 0.5, 0.5, 1.7, 2.5, 2.7, 1.5, 2.0, 1.2, 3.0, 1.7, 1.7, 0.5, 2.8, 0.7, 2.9, 1.8, 0.6, 0.5, 1.8, 0.5, 2.0, 2.0, 2.6, 1.3, 2.2, 0.8, 1.5, 1.9, 1.7, 2.2, 2.4, 2.4, 0.5, 2.8, 1.8, 0.5, 0.7, 0.8, 1.2, 0.9, 2.6, 1.6, 0.5, 2.1, 1.6, 2.8, 1.1, 0.5, 1.4, 0.5, 1.3, 0.5, 1.4, 2.1, 1.6, 0.7, 1.5, 2.3, 2.1, 2.5, 0.5, 2.9, 0.5, 2.8, 0.9, 1.9, 0.5, 1.7, 2.0, 0.5, 3.0, 1.6, 1.2, 0.9, 0.7, 0.5, 0.5, 0.5, 2.5, 0.6, 2.5, 2.3, 2.8, 0.6, 1.3, 0.5, 0.7, 3.0, 2.5, 0.5, 0.5, 2.6, 1.6, 1.3, 0.5, 1.3, 1.6, 0.5, 0.5, 1.9, 3.0, 0.8, 2.6, 0.6, 0.6, 0.5, 2.3, 0.5, 1.0, 1.3, 2.2, 0.7, 2.6, 1.4, 1.8, 1.7, 0.5, 2.8, 0.5, 0.5, 2.7, 0.5, 0.5, 0.5, 2.6, 0.5, 2.5, 2.5, 1.5, 0.7, 1.4, 2.5, 0.5, 1.2, 1.4, 1.4, 1.7, 2.5, 1.7, 2.8, 0.5, 0.5, 2.1, 2.8, 2.8, 2.8, 1.1, 2.1, 1.2, 2.1, 1.7, 0.7, 1.7, 1.9, 0.9, 0.7, 0.7, 0.5, 0.5, 1.0, 3.0, 2.5, 0.7, 2.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 2.6, 2.5, 0.7, 1.0, 2.3, 2.4, 0.8, 1.1, 0.8, 0.8, 0.5, 2.5, 2.6, 2.8, 1.1, 1.1, 0.5, 2.6, 0.5, 0.5, 1.5, 0.6, 0.8, 2.6, 2.5, 1.5, 2.5, 2.0, 0.5, 0.8, 0.5, 1.5, 0.5, 0.7, 1.3, 1.8, 2.5, 2.6, 2.4, 2.3, 2.3, 0.6, 1.9, 0.5, 2.1, 1.3, 0.5, 1.8, 1.6, 2.2, 2.9, 2.2, 1.3, 2.1, 2.1, 2.1, 1.9, 0.6, 0.6, 0.5, 1.2, 1.9, 0.7, 1.8, 1.8, 2.5, 1.5, 3.0, 0.6, 1.7, 2.4, 0.5, 1.3, 3.0, 1.2, 2.4, 2.6, 1.0, 1.9, 1.5, 1.5, 1.3, 2.4, 1.0, 1.1, 2.0, 2.2, 0.8, 0.5, 0.8, 2.8, 2.7, 1.4, 1.2, 2.0, 1.3, 2.9, 1.8, 0.7, 1.4, 0.5, 1.6, 2.0, 0.5, 2.2, 2.2, 2.0, 1.5, 2.1, 0.5, 0.5, 1.8, 1.5, 1.9, 2.8, 2.2, 0.5, 2.7, 0.5, 2.3, 0.7, 0.5, 2.2, 0.5, 1.8, 2.8, 2.8, 1.8, 2.8, 1.9, 0.7, 2.4, 2.0, 2.7, 0.5, 2.5, 1.9, 1.6, 1.8, 0.7, 0.5, 1.7, 1.6, 2.5, 0.9, 0.5, 1.8, 0.7, 0.5, 1.4, 0.5, 2.1, 2.0, 0.5, 1.6, 2.6, 0.5, 2.5, 2.5, 1.7, 1.0, 2.7, 0.5, 2.0, 2.6, 0.5, 0.5, 0.5, 2.5, 0.9, 1.6, 0.8, 2.3, 1.6, 1.2, 2.8, 1.6, 2.5, 0.5, 0.5, 2.8, 1.7, 1.8, 0.5, 2.5, 1.0, 0.6, 0.5, 0.5, 0.5, 1.6, 1.4, 0.5, 2.3, 1.8, 2.2, 1.8, 1.5, 1.0, 0.8, 0.6, 2.9, 1.8, 0.7, 1.6, 2.7, 1.5, 1.4, 2.4, 0.7, 0.6, 1.6, 1.4, 0.5, 1.2, 2.0, 1.8, 1.8, 1.3, 2.8, 0.5, 2.8, 1.2, 0.6, 1.0, 0.5, 1.5, 2.8, 1.9, 1.4, 0.6, 1.5, 0.6, 2.6, 2.6, 1.7, 2.3, 0.8, 1.7, 1.3, 2.3, 2.6, 0.7, 1.1, 0.5, 0.5, 0.6, 2.6, 1.1, 0.5, 2.3, 2.3, 0.5, 1.9, 0.5, 0.9, 0.7, 1.2, 2.3, 1.9, 2.8, 1.3, 1.8, 2.9, 2.9, 0.5, 3.0, 2.5, 1.5, 1.3, 0.5, 0.5, 0.6, 0.5, 0.8, 0.6, 0.5, 2.8, 1.3, 1.2, 0.9, 1.6, 2.4, 0.5, 2.5, 1.2, 1.3, 1.3, 2.3, 1.6, 0.5, 0.7, 1.5, 2.2, 0.5, 0.5, 1.9, 0.8, 0.8, 2.3, 0.5, 0.5, 1.7, 2.8, 2.3, 1.4, 1.9, 2.0, 2.6, 0.5, 2.9, 2.9, 2.2, 2.8, 0.5, 1.9, 0.5, 1.6, 1.1, 0.5, 0.5, 1.9, 2.5, 0.5, 2.1, 1.6, 2.4, 1.5, 1.1, 0.7, 0.5, 0.5, 0.5, 1.5, 0.5, 0.5, 0.5, 2.3, 1.5, 0.5, 2.3, 1.9, 0.5, 0.5, 2.1, 2.7, 1.4, 0.5, 1.9, 2.1, 0.5, 2.7, 1.3, 2.1, 1.4, 2.4, 2.5, 1.4, 2.1, 2.8, 1.0, 1.7, 0.7, 2.0, 2.2, 0.5, 2.5, 2.9, 2.8, 2.0, 2.2, 0.9, 2.9, 0.5, 2.5, 2.5, 0.5, 1.4, 2.7, 2.5, 0.9, 0.5, 0.5, 2.1, 2.8, 0.5, 1.5, 2.1, 0.9, 0.5, 0.9, 0.5, 0.5, 2.3, 0.8, 1.7, 2.6, 1.2, 0.9, 0.6, 2.2, 2.0, 2.1, 1.9, 1.0, 2.6, 1.1, 1.0, 2.7, 2.3, 2.0, 1.5, 2.1, 2.3, 0.6, 0.5, 2.3, 1.2, 0.5, 1.3, 1.2, 0.8, 1.9, 0.5, 0.5, 2.0, 3.0, 0.5, 2.9, 1.0, 0.5, 0.7, 0.5, 0.5, 0.7, 1.4, 1.8, 1.5, 2.5, 2.8, 1.6, 0.6, 1.5, 2.3, 2.4, 0.5, 2.6, 1.9, 1.4, 2.2, 3.0, 1.6, 0.5, 2.4, 2.3, 1.0, 2.5, 0.5, 1.0, 2.8, 2.2, 2.0, 2.3, 1.3, 0.5, 1.9, 1.8, 0.6, 0.8, 3.0, 0.7, 0.7, 1.0, 0.5, 0.5, 1.8, 0.5, 2.2, 0.5, 0.5, 1.6, 1.3, 0.9, 0.7, 1.9, 1.2, 0.6, 1.4, 1.2, 0.5, 2.9, 0.5, 2.3, 1.9, 2.2, 0.7, 0.5, 1.5, 2.9, 2.0, 2.4, 1.6, 0.7, 0.5, 1.9, 1.8, 2.7, 1.5, 1.7, 0.5, 2.6, 1.9, 2.4, 1.7, 2.0, 2.4, 0.6, 2.6, 0.5, 2.0, 1.1, 0.5, 0.5, 0.6, 2.2, 0.6, 3.0, 0.5, 2.8, 2.9, 0.5, 2.1, 1.2, 1.6, 2.2, 0.8, 1.9, 0.5, 1.7, 0.8, 2.3, 2.7, 2.3, 1.2, 1.6, 2.8, 2.7, 0.5, 0.8, 0.8, 1.1, 1.4, 1.1, 2.7, 1.5, 3.0, 2.7, 0.5, 2.9, 2.6, 1.2, 2.3, 0.5, 2.9, 1.7, 0.8, 2.8, 0.5, 0.5, 2.6, 2.0, 2.8, 2.8, 0.5, 2.0, 1.7, 1.0, 0.5, 0.5, 2.5, 1.8, 2.9, 0.6, 2.8, 2.0, 2.1, 3.0, 0.5, 2.9, 1.3, 1.5, 1.3, 0.5, 2.5, 1.8, 0.5, 2.9, 0.5, 0.5, 1.0, 1.5, 1.6, 3.0, 1.5, 1.9, 1.2, 2.8, 2.7, 0.7, 1.0, 1.7, 0.5, 0.5, 2.8, 2.0, 0.5, 1.6, 2.8, 1.9, 1.9, 1.6, 0.5, 0.5, 0.5, 0.5, 0.6, 2.2, 1.1, 2.1, 0.9, 1.8, 0.5, 2.1, 2.9, 1.4, 2.7, 1.4, 2.1, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 1.4, 2.8, 0.8, 1.9, 2.7, 0.8, 0.6, 2.4, 0.9, 2.3, 2.6, 2.0, 0.5, 2.7, 1.4, 2.8, 1.1, 1.1, 3.0, 0.5, 2.1, 0.8, 0.5, 2.6, 1.9, 0.9, 1.8, 0.5, 1.4, 1.9, 1.6, 0.5, 0.8, 0.5, 1.1, 2.3, 0.9, 0.5, 1.3, 0.5, 1.3, 1.7, 2.3, 1.9, 0.5, 0.6, 0.7, 1.3, 1.7, 0.5, 1.2, 2.0, 2.5, 2.3, 1.6, 0.5, 0.6, 2.7, 1.6, 1.0, 1.4, 2.5, 3.0, 2.2, 1.3, 1.5, 2.9, 0.7, 1.3, 2.4, 0.5, 0.5, 0.7, 0.5, 1.0, 0.5, 0.9, 0.5, 0.5, 2.2, 2.8, 1.9, 0.7, 2.8, 0.5, 1.1, 1.5, 1.7, 1.9, 0.8, 2.2, 2.3, 0.5, 1.8, 1.4, 0.5, 2.1, 2.6, 2.0, 0.5, 0.9, 1.6, 0.5, 1.7, 1.5, 2.4, 0.5, 0.7, 2.4, 1.5, 1.5, 0.6, 1.8, 2.1, 2.1, 2.2, 1.7, 1.6, 0.8, 1.4, 1.5, 2.2, 1.2, 1.3, 0.6, 0.9, 0.6, 1.5, 0.8, 1.8, 0.5, 1.0, 0.5, 2.9, 0.6, 1.0, 1.2, 0.6, 2.5, 1.8, 0.5, 2.9, 1.1, 2.6, 0.7, 2.8, 1.4, 2.5, 1.9, 2.7, 0.6, 2.8, 0.5, 2.6, 2.3, 0.5, 1.0, 1.7, 1.5, 2.9, 0.6, 2.9, 0.9, 1.6, 1.2, 0.5, 1.8, 1.6, 1.2, 2.1, 2.7, 1.7, 0.9, 2.3, 2.1, 2.4, 2.3, 2.8, 2.9, 0.5, 2.7, 1.8, 2.1, 1.0, 0.5, 0.9, 0.5, 1.9, 1.4, 1.5, 1.4, 1.6, 0.8, 0.8, 2.4, 1.4, 0.5, 1.9, 1.2, 1.2, 1.2, 2.5, 1.0, 2.1, 2.7, 1.0, 2.4, 0.5, 2.5, 1.0, 2.1, 1.6, 0.5, 0.5, 0.7, 2.4, 1.6, 1.3, 0.9, 1.6, 2.0, 1.7, 0.6, 2.7, 0.6, 0.5, 2.5, 0.5, 2.2, 2.5, 2.9, 0.5, 2.9, 1.2, 0.5, 1.4, 0.9, 1.6, 2.8, 1.5, 2.5, 2.7, 2.1, 0.5, 0.6, 2.6, 1.0, 2.0, 0.6, 0.5, 1.9, 0.7, 1.4, 1.2, 2.9, 1.0, 1.3, 1.8, 1.1, 0.8, 3.0, 0.5, 0.8, 0.5, 1.1, 2.2, 1.2, 1.0, 2.6, 1.6, 1.8, 2.2, 2.1, 0.9, 1.0, 2.9, 2.8, 2.4, 2.0, 2.9, 0.5, 2.2, 0.5, 2.6, 0.9, 0.9, 2.0]}, {\"axis\": {\"matches\": true}, \"label\": \"dual_sim\", \"values\": [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"fc\", \"values\": [1, 0, 2, 0, 13, 3, 4, 0, 0, 2, 0, 5, 2, 7, 13, 3, 1, 7, 11, 4, 12, 1, 4, 4, 5, 0, 7, 1, 12, 3, 0, 16, 3, 0, 0, 16, 1, 6, 16, 15, 5, 8, 5, 5, 1, 0, 1, 3, 2, 9, 2, 5, 0, 1, 0, 10, 13, 0, 8, 2, 3, 1, 0, 15, 2, 13, 11, 0, 0, 0, 6, 1, 8, 3, 4, 8, 4, 1, 15, 6, 0, 11, 2, 11, 5, 5, 4, 1, 0, 0, 2, 7, 0, 11, 6, 18, 10, 3, 0, 6, 3, 4, 0, 0, 1, 2, 5, 3, 0, 3, 0, 0, 0, 13, 9, 1, 0, 3, 8, 0, 3, 1, 7, 5, 3, 5, 0, 4, 11, 0, 2, 9, 1, 7, 1, 0, 0, 5, 12, 1, 0, 1, 4, 0, 3, 1, 0, 3, 4, 6, 1, 4, 3, 2, 6, 4, 3, 16, 0, 12, 4, 10, 0, 11, 11, 3, 0, 2, 0, 17, 2, 2, 5, 0, 0, 8, 2, 0, 0, 2, 12, 11, 0, 15, 0, 1, 0, 0, 3, 2, 8, 1, 4, 5, 0, 1, 0, 0, 3, 9, 0, 7, 0, 10, 6, 0, 16, 7, 1, 0, 0, 0, 9, 4, 0, 3, 0, 5, 0, 4, 6, 5, 8, 1, 3, 1, 18, 0, 2, 17, 1, 4, 4, 0, 12, 6, 3, 14, 0, 0, 0, 0, 0, 2, 7, 2, 3, 3, 4, 3, 0, 0, 0, 2, 9, 5, 4, 1, 0, 6, 7, 0, 0, 4, 8, 5, 10, 10, 4, 0, 2, 0, 7, 11, 5, 3, 0, 3, 12, 0, 3, 0, 9, 6, 7, 0, 6, 0, 16, 0, 8, 10, 6, 6, 1, 0, 7, 1, 11, 8, 17, 2, 16, 11, 1, 18, 0, 6, 11, 1, 5, 0, 1, 6, 0, 3, 6, 6, 7, 6, 0, 2, 1, 14, 5, 5, 0, 6, 1, 5, 5, 0, 0, 1, 8, 0, 0, 2, 2, 0, 5, 5, 8, 6, 2, 0, 2, 6, 7, 2, 15, 16, 2, 7, 10, 7, 0, 1, 10, 1, 10, 13, 4, 0, 0, 8, 0, 0, 8, 8, 10, 4, 17, 0, 5, 0, 0, 1, 2, 0, 10, 2, 3, 0, 12, 4, 12, 14, 1, 1, 3, 0, 15, 3, 0, 8, 4, 0, 10, 0, 0, 16, 8, 0, 6, 1, 0, 9, 12, 2, 5, 2, 12, 0, 2, 5, 1, 2, 1, 2, 7, 6, 10, 2, 4, 3, 9, 0, 3, 10, 6, 3, 0, 0, 4, 0, 8, 1, 10, 0, 6, 1, 3, 13, 2, 7, 5, 15, 5, 1, 3, 0, 2, 10, 1, 6, 4, 12, 0, 6, 9, 12, 3, 5, 3, 5, 2, 7, 0, 10, 1, 8, 0, 1, 0, 9, 0, 4, 14, 2, 2, 1, 14, 0, 0, 0, 12, 13, 0, 5, 7, 5, 3, 6, 0, 5, 11, 13, 1, 3, 1, 16, 1, 0, 0, 1, 2, 7, 0, 6, 9, 6, 11, 10, 2, 2, 0, 8, 3, 9, 11, 9, 6, 5, 0, 3, 8, 6, 7, 9, 3, 3, 3, 3, 0, 0, 8, 7, 4, 5, 5, 1, 2, 3, 0, 16, 0, 0, 0, 0, 10, 1, 0, 12, 6, 6, 10, 0, 4, 2, 9, 15, 11, 0, 16, 12, 4, 14, 15, 0, 7, 9, 13, 0, 4, 9, 4, 7, 1, 2, 4, 8, 1, 0, 17, 2, 8, 0, 2, 0, 6, 0, 12, 0, 0, 1, 10, 0, 6, 4, 2, 0, 0, 0, 0, 5, 7, 1, 6, 1, 3, 0, 0, 0, 9, 1, 6, 1, 0, 7, 1, 5, 0, 7, 5, 12, 0, 0, 1, 1, 1, 0, 0, 5, 13, 7, 2, 10, 4, 0, 0, 0, 0, 2, 4, 6, 4, 13, 1, 0, 7, 10, 1, 8, 7, 2, 11, 3, 13, 9, 1, 1, 6, 6, 0, 4, 4, 1, 4, 0, 11, 4, 0, 6, 2, 8, 2, 5, 10, 10, 0, 0, 12, 1, 13, 3, 10, 0, 8, 5, 2, 7, 2, 1, 0, 6, 0, 3, 4, 0, 7, 1, 1, 4, 0, 2, 2, 3, 2, 10, 0, 1, 8, 14, 0, 0, 5, 5, 3, 1, 12, 0, 6, 2, 6, 13, 0, 0, 4, 9, 8, 2, 13, 0, 12, 3, 3, 1, 13, 0, 0, 0, 0, 5, 6, 16, 7, 1, 5, 1, 2, 0, 6, 5, 8, 7, 0, 0, 2, 3, 6, 1, 0, 0, 7, 5, 0, 1, 4, 9, 7, 16, 0, 1, 0, 1, 2, 9, 2, 0, 15, 10, 6, 11, 5, 6, 7, 2, 12, 8, 0, 1, 3, 8, 4, 0, 0, 0, 2, 3, 12, 15, 4, 2, 0, 0, 3, 5, 4, 8, 4, 9, 0, 10, 0, 1, 0, 1, 7, 2, 0, 6, 0, 0, 7, 8, 15, 5, 6, 4, 1, 1, 2, 4, 0, 9, 0, 15, 2, 1, 5, 3, 8, 3, 7, 13, 8, 13, 11, 10, 0, 5, 2, 2, 7, 6, 0, 6, 9, 2, 0, 11, 0, 9, 3, 3, 9, 3, 0, 6, 7, 6, 10, 0, 2, 0, 1, 2, 5, 7, 3, 0, 1, 1, 8, 6, 13, 0, 8, 0, 0, 4, 0, 12, 5, 3, 3, 4, 7, 0, 0, 9, 10, 3, 3, 4, 6, 5, 0, 0, 3, 0, 2, 1, 15, 4, 4, 3, 0, 0, 2, 0, 5, 0, 15, 2, 13, 15, 5, 0, 0, 3, 3, 11, 2, 2, 1, 1, 5, 9, 9, 0, 3, 0, 0, 0, 2, 3, 2, 6, 7, 0, 9, 6, 0, 4, 9, 1, 3, 3, 7, 2, 4, 4, 4, 1, 0, 0, 7, 7, 5, 5, 6, 1, 0, 0, 0, 4, 4, 3, 11, 0, 2, 2, 0, 9, 5, 0, 4, 10, 13, 13, 5, 11, 14, 5, 2, 0, 1, 3, 2, 5, 12, 3, 15, 0, 11, 16, 2, 4, 5, 7, 0, 0, 3, 4, 0, 1, 12, 3, 0, 1, 13, 7, 9, 13, 1, 7, 0, 0, 6, 3, 8, 0, 7, 1, 0, 1, 0, 2, 12, 2, 0, 5, 3, 10, 4, 1, 1, 3, 0, 3, 1, 1, 11, 0, 1, 0, 9, 0, 3, 4, 16, 5, 3, 7, 6, 5, 4, 1, 2, 1, 0, 13, 0, 3, 0, 8, 0, 1, 3, 1, 7, 11, 0, 1, 0, 8, 6, 9, 2, 8, 0, 3, 0, 1, 6, 2, 4, 0, 1, 13, 4, 2, 4, 11, 1, 0, 3, 1, 5, 0, 1, 0, 1, 14, 2, 11, 0, 11, 8, 15, 4, 1, 9, 4, 1, 8, 13, 0, 2, 2, 0, 3, 2, 1, 1, 1, 2, 10, 3, 3, 10, 10, 1, 0, 5, 3, 0, 0, 9, 11, 2, 2, 1, 0, 2, 0, 10, 0, 13, 0, 8, 11, 8, 9, 0, 0, 8, 9, 12, 4, 2, 0, 2, 3, 12, 2, 9, 0, 0, 0, 11, 0, 0, 0, 11, 5, 7, 1, 10, 12, 4, 0, 2, 7, 16, 0, 0, 2, 1, 0, 0, 6, 11, 2, 0, 2, 13, 2, 0, 7, 10, 1, 8, 3, 1, 5, 5, 3, 0, 0, 2, 7, 0, 6, 3, 2, 0, 4, 5, 4, 5, 0, 9, 0, 5, 2, 4, 0, 2, 7, 11, 5, 10, 0, 1, 0, 0, 2, 1, 2, 1, 1, 6, 5, 4, 0, 14, 5, 0, 1, 8, 1, 1, 1, 13, 3, 4, 1, 7, 0, 1, 3, 4, 4, 10, 2, 2, 12, 4, 5, 4, 5, 2, 0, 0, 1, 10, 7, 6, 10, 0, 7, 1, 10, 0, 1, 10, 2, 5, 1, 5, 2, 1, 5, 2, 7, 0, 2, 0, 3, 0, 3, 5, 1, 0, 13, 3, 9, 6, 4, 7, 6, 0, 3, 2, 14, 0, 15, 1, 5, 3, 9, 8, 5, 4, 3, 7, 0, 3, 12, 9, 2, 8, 1, 0, 5, 0, 0, 8, 12, 7, 0, 8, 0, 7, 0, 9, 2, 9, 0, 1, 2, 4, 0, 8, 8, 4, 2, 0, 3, 2, 3, 1, 0, 1, 0, 0, 2, 0, 7, 1, 12, 6, 0, 9, 18, 3, 6, 1, 15, 4, 0, 0, 8, 6, 2, 0, 7, 0, 6, 6, 5, 3, 3, 18, 1, 1, 7, 0, 1, 6, 6, 2, 10, 18, 0, 1, 2, 4, 4, 1, 3, 5, 1, 8, 0, 11, 9, 1, 0, 9, 0, 0, 9, 5, 1, 8, 4, 2, 0, 5, 0, 7, 7, 6, 16, 0, 9, 3, 0, 1, 1, 5, 0, 16, 6, 10, 3, 7, 1, 4, 11, 0, 0, 3, 4, 0, 0, 0, 5, 1, 0, 14, 2, 6, 0, 0, 2, 8, 7, 0, 1, 11, 0, 1, 9, 0, 7, 5, 9, 1, 3, 0, 4, 2, 2, 1, 5, 6, 0, 2, 7, 4, 5, 4, 0, 7, 0, 9, 7, 1, 5, 2, 0, 13, 3, 1, 3, 9, 6, 14, 0, 0, 2, 1, 5, 1, 4, 5, 0, 5, 0, 12, 9, 9, 7, 9, 1, 5, 9, 9, 7, 1, 2, 2, 4, 0, 17, 0, 2, 0, 1, 18, 1, 1, 0, 12, 7, 5, 0, 2, 1, 1, 3, 6, 0, 0, 6, 2, 0, 6, 0, 11, 2, 2, 6, 0, 9, 2, 0, 2, 0, 6, 11, 5, 1, 11, 3, 1, 0, 3, 0, 16, 10, 1, 1, 0, 7, 10, 2, 4, 0, 1, 8, 3, 0, 9, 0, 12, 3, 1, 2, 1, 0, 7, 5, 7, 6, 0, 3, 3, 1, 5, 1, 6, 5, 11, 12, 6, 1, 0, 11, 13, 1, 10, 2, 3, 0, 16, 6, 14, 9, 1, 0, 2, 0, 10, 2, 4, 5, 3, 0, 10, 14, 4, 5, 4, 6, 10, 10, 1, 0, 3, 16, 1, 0, 14, 5, 4, 5, 6, 7, 3, 0, 10, 4, 5, 6, 5, 0, 0, 3, 14, 5, 13, 5, 13, 5, 0, 1, 6, 18, 0, 4, 4, 0, 3, 1, 0, 2, 0, 8, 8, 19, 0, 16, 1, 15, 0, 0, 8, 2, 10, 5, 0, 0, 2, 0, 6, 7, 3, 7, 2, 2, 3, 5, 0, 12, 6, 6, 12, 0, 4, 11, 2, 4, 15, 4, 5, 9, 2, 2, 0, 6, 3, 11, 5, 0, 10, 1, 5, 1, 0, 1, 2, 1, 5, 3, 2, 5, 3, 4, 3, 3, 0, 15, 0, 0, 8, 0, 0, 3, 3, 1, 6, 1, 5, 0, 7, 9, 3, 10, 8, 3, 3, 7, 16, 1, 9, 0, 0, 0, 2, 4, 1, 6, 12, 14, 0, 8, 3, 2, 12, 4, 5, 4, 0, 7, 1, 1, 0, 1, 4, 1, 1, 6, 1, 3, 0, 13, 0, 13, 3, 4, 9, 4, 8, 4, 9, 6, 1, 4, 3, 6, 0, 0, 12, 2, 1, 1, 2, 0, 8, 3, 3, 14, 3, 11, 9, 13, 1, 8, 1, 14, 6, 1, 3, 4, 3, 5, 3, 3, 1, 11, 7, 7, 1, 7, 8, 10, 0, 1, 1, 5, 0, 0, 0, 0, 18, 8, 18, 5, 3, 0, 11, 0, 18, 1, 3, 4, 7, 1, 0, 3, 1, 2, 3, 9, 2, 1, 9, 1, 3, 7, 0, 1, 2, 2, 5, 0, 1, 9, 3, 1, 2, 0, 0, 1, 1, 0, 9, 8, 2, 5, 0, 2, 8, 0, 5, 0, 1, 3, 0, 6, 1, 0, 7, 12, 1, 4, 5, 7, 2, 5, 0, 0, 0, 4, 1, 10, 4, 0, 0, 4, 2, 0, 6, 3, 0, 13, 9, 3, 4, 2, 0, 2, 0, 5, 0, 0, 6, 2, 9, 7, 2, 1, 3, 1, 4, 8, 4, 12, 0, 8, 6, 9, 2, 2, 8, 11, 1, 0, 1, 0, 0, 1, 4, 5]}, {\"axis\": {\"matches\": true}, \"label\": \"four_g\", \"values\": [0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"int_memory\", \"values\": [7, 53, 41, 10, 44, 22, 10, 24, 53, 9, 9, 33, 33, 17, 52, 46, 13, 23, 49, 19, 39, 13, 47, 38, 8, 57, 51, 41, 52, 21, 5, 33, 41, 51, 22, 60, 61, 6, 11, 50, 44, 41, 5, 34, 20, 27, 42, 40, 57, 64, 14, 63, 43, 16, 51, 46, 60, 61, 49, 48, 12, 63, 50, 55, 9, 44, 36, 30, 10, 43, 45, 22, 9, 43, 38, 42, 9, 33, 16, 29, 58, 64, 45, 61, 57, 25, 42, 64, 3, 33, 57, 54, 15, 14, 7, 7, 23, 60, 37, 31, 10, 12, 57, 55, 8, 39, 23, 42, 46, 41, 43, 32, 4, 7, 22, 18, 53, 40, 46, 15, 10, 10, 52, 36, 25, 16, 58, 25, 31, 19, 54, 2, 41, 30, 36, 16, 22, 46, 3, 14, 31, 14, 43, 33, 64, 9, 31, 40, 47, 41, 56, 56, 61, 32, 45, 46, 60, 6, 29, 34, 37, 51, 14, 26, 3, 33, 27, 10, 58, 44, 32, 8, 10, 42, 16, 25, 29, 23, 22, 63, 30, 41, 38, 20, 54, 4, 5, 46, 6, 43, 46, 7, 64, 57, 20, 23, 41, 61, 26, 36, 22, 20, 16, 30, 61, 47, 8, 49, 51, 35, 23, 41, 63, 39, 50, 53, 49, 17, 24, 51, 29, 40, 13, 25, 38, 17, 49, 31, 37, 24, 39, 9, 31, 27, 6, 11, 42, 6, 38, 49, 49, 44, 51, 10, 37, 50, 23, 48, 7, 30, 55, 63, 63, 7, 19, 57, 4, 32, 51, 6, 59, 25, 56, 28, 18, 57, 40, 38, 47, 28, 7, 4, 45, 3, 30, 62, 61, 8, 2, 39, 36, 40, 54, 6, 56, 27, 16, 24, 26, 42, 63, 16, 9, 48, 34, 56, 12, 52, 56, 38, 58, 57, 56, 12, 29, 52, 37, 8, 42, 40, 44, 41, 50, 41, 49, 49, 64, 14, 42, 64, 5, 29, 64, 63, 18, 25, 21, 26, 27, 13, 20, 9, 20, 7, 47, 18, 47, 48, 36, 22, 54, 2, 17, 37, 6, 52, 10, 3, 57, 42, 44, 2, 33, 8, 19, 64, 17, 50, 62, 61, 21, 57, 32, 21, 36, 2, 9, 34, 46, 52, 52, 36, 55, 58, 42, 30, 26, 14, 19, 46, 8, 48, 59, 54, 33, 18, 44, 23, 12, 63, 26, 39, 23, 9, 57, 7, 57, 33, 28, 42, 7, 50, 19, 55, 13, 34, 45, 17, 34, 51, 5, 44, 48, 49, 25, 51, 54, 63, 54, 4, 7, 57, 43, 11, 48, 56, 47, 24, 20, 28, 17, 64, 57, 17, 23, 50, 13, 36, 3, 8, 57, 11, 3, 26, 3, 28, 64, 62, 51, 51, 35, 54, 34, 25, 64, 48, 41, 53, 28, 44, 43, 6, 51, 13, 40, 42, 37, 4, 18, 61, 48, 16, 45, 60, 22, 2, 61, 26, 15, 58, 56, 34, 3, 43, 27, 41, 2, 27, 20, 48, 17, 31, 17, 50, 15, 39, 5, 31, 33, 34, 54, 32, 6, 32, 29, 19, 23, 15, 59, 44, 50, 10, 7, 45, 9, 2, 12, 61, 57, 19, 55, 46, 14, 51, 23, 26, 54, 27, 61, 12, 38, 8, 30, 17, 16, 31, 37, 49, 53, 51, 16, 38, 39, 21, 16, 19, 50, 31, 7, 35, 45, 41, 43, 45, 21, 26, 53, 32, 30, 13, 61, 14, 13, 3, 38, 32, 30, 52, 50, 34, 8, 32, 46, 55, 24, 20, 6, 24, 23, 23, 19, 18, 24, 5, 5, 58, 29, 40, 40, 17, 35, 39, 64, 55, 54, 58, 25, 11, 3, 11, 40, 44, 12, 7, 28, 56, 20, 35, 16, 46, 56, 4, 30, 24, 21, 44, 34, 39, 16, 21, 5, 36, 2, 30, 48, 54, 30, 34, 58, 35, 21, 26, 19, 54, 9, 15, 9, 27, 43, 47, 14, 3, 31, 46, 3, 14, 60, 25, 53, 35, 64, 15, 25, 35, 54, 7, 14, 45, 50, 50, 36, 21, 46, 61, 26, 33, 44, 48, 11, 3, 27, 62, 32, 32, 45, 22, 64, 13, 12, 60, 12, 34, 41, 27, 31, 25, 60, 19, 41, 57, 26, 52, 38, 23, 33, 57, 43, 22, 17, 19, 2, 62, 48, 39, 20, 30, 50, 49, 35, 21, 25, 20, 28, 2, 9, 11, 6, 3, 49, 37, 52, 8, 61, 54, 51, 45, 58, 7, 21, 43, 2, 49, 42, 61, 13, 25, 57, 42, 34, 15, 35, 37, 8, 18, 58, 16, 43, 17, 4, 45, 53, 16, 5, 51, 45, 63, 11, 12, 34, 58, 39, 49, 56, 14, 15, 7, 16, 27, 30, 25, 64, 23, 27, 50, 45, 63, 18, 58, 8, 11, 11, 50, 50, 14, 47, 56, 55, 42, 2, 55, 46, 24, 38, 53, 25, 56, 10, 41, 8, 22, 20, 3, 52, 6, 19, 5, 21, 10, 10, 44, 33, 8, 25, 16, 54, 52, 64, 8, 48, 57, 61, 30, 14, 52, 25, 7, 2, 33, 4, 27, 43, 31, 55, 9, 21, 33, 63, 11, 30, 48, 39, 35, 34, 16, 51, 55, 32, 16, 38, 16, 63, 58, 41, 8, 10, 52, 56, 21, 29, 6, 15, 35, 14, 35, 11, 34, 15, 8, 2, 32, 9, 9, 4, 64, 11, 13, 34, 48, 53, 18, 47, 21, 30, 22, 39, 6, 31, 50, 3, 36, 8, 40, 58, 61, 32, 49, 49, 8, 34, 12, 46, 27, 26, 53, 45, 17, 27, 53, 45, 30, 12, 10, 23, 53, 36, 49, 14, 35, 5, 60, 30, 58, 8, 63, 48, 29, 38, 13, 55, 13, 27, 27, 44, 9, 23, 7, 34, 13, 54, 46, 39, 33, 52, 7, 8, 46, 50, 42, 13, 41, 13, 46, 38, 54, 47, 57, 6, 52, 36, 33, 7, 61, 7, 55, 8, 19, 34, 13, 46, 38, 14, 52, 52, 30, 54, 47, 23, 58, 59, 47, 34, 58, 44, 19, 11, 51, 20, 21, 45, 11, 19, 48, 27, 3, 43, 49, 10, 19, 29, 20, 28, 57, 50, 46, 53, 58, 13, 34, 31, 51, 19, 6, 53, 53, 33, 52, 54, 28, 63, 64, 23, 59, 61, 58, 55, 49, 53, 37, 4, 38, 18, 43, 12, 41, 57, 13, 57, 60, 28, 46, 14, 63, 52, 18, 39, 63, 5, 30, 7, 22, 21, 35, 37, 3, 27, 19, 64, 33, 18, 16, 32, 28, 51, 18, 17, 42, 23, 23, 45, 27, 41, 60, 31, 8, 62, 10, 39, 59, 24, 23, 14, 40, 33, 37, 14, 55, 30, 55, 4, 18, 30, 58, 58, 62, 2, 10, 35, 35, 61, 60, 14, 32, 57, 12, 55, 47, 18, 46, 45, 43, 35, 59, 57, 36, 64, 22, 44, 58, 63, 46, 16, 35, 41, 12, 9, 54, 50, 16, 5, 52, 31, 62, 10, 55, 9, 14, 8, 60, 10, 55, 43, 24, 56, 31, 51, 2, 14, 12, 32, 44, 46, 35, 2, 38, 20, 49, 61, 50, 21, 40, 28, 29, 59, 16, 40, 42, 27, 9, 44, 51, 15, 51, 57, 18, 3, 47, 14, 11, 30, 7, 24, 2, 13, 62, 40, 9, 56, 62, 39, 52, 11, 10, 37, 27, 60, 2, 34, 49, 26, 58, 32, 35, 45, 57, 41, 22, 50, 20, 51, 5, 58, 2, 24, 11, 32, 56, 29, 57, 51, 9, 24, 5, 27, 37, 56, 6, 35, 55, 64, 16, 20, 10, 36, 61, 62, 40, 55, 32, 4, 39, 7, 35, 24, 50, 59, 47, 21, 20, 29, 54, 41, 43, 31, 49, 52, 6, 13, 52, 49, 7, 60, 20, 27, 12, 39, 13, 7, 4, 19, 54, 42, 31, 25, 12, 46, 6, 8, 37, 23, 58, 12, 6, 10, 15, 12, 9, 19, 49, 28, 58, 46, 60, 31, 36, 59, 56, 44, 42, 2, 52, 6, 19, 5, 33, 58, 30, 42, 26, 6, 5, 44, 23, 6, 27, 41, 32, 59, 17, 5, 17, 13, 29, 20, 27, 42, 47, 17, 22, 39, 60, 29, 16, 24, 14, 47, 34, 5, 36, 12, 16, 58, 44, 10, 42, 42, 51, 18, 7, 14, 14, 26, 14, 2, 42, 31, 63, 9, 40, 57, 31, 28, 27, 42, 12, 39, 11, 14, 62, 51, 23, 15, 6, 40, 52, 32, 63, 24, 49, 57, 62, 33, 56, 50, 10, 2, 29, 52, 17, 63, 52, 42, 14, 28, 6, 30, 28, 26, 14, 29, 35, 13, 55, 18, 27, 40, 21, 60, 63, 33, 54, 14, 30, 15, 10, 32, 15, 2, 50, 44, 58, 19, 6, 34, 59, 6, 19, 37, 9, 10, 41, 44, 27, 14, 24, 32, 40, 18, 16, 31, 22, 13, 64, 53, 27, 13, 5, 6, 11, 44, 50, 55, 12, 31, 50, 22, 57, 44, 33, 42, 11, 31, 10, 44, 28, 20, 33, 40, 10, 33, 40, 20, 11, 63, 10, 29, 5, 2, 39, 44, 54, 12, 40, 31, 50, 9, 21, 22, 3, 36, 8, 12, 2, 29, 48, 26, 20, 40, 20, 64, 43, 60, 15, 21, 60, 4, 8, 36, 11, 23, 48, 35, 3, 34, 14, 27, 20, 42, 14, 22, 60, 24, 33, 57, 38, 50, 8, 60, 12, 30, 55, 58, 9, 59, 38, 11, 6, 57, 16, 19, 30, 5, 18, 30, 44, 40, 15, 20, 27, 15, 45, 39, 23, 55, 18, 36, 42, 43, 28, 54, 2, 26, 14, 50, 28, 23, 39, 12, 16, 14, 3, 27, 34, 16, 43, 56, 42, 12, 49, 24, 15, 54, 19, 19, 22, 46, 2, 26, 31, 35, 11, 36, 7, 62, 7, 42, 30, 5, 8, 53, 59, 35, 27, 24, 39, 5, 27, 17, 33, 21, 58, 44, 15, 6, 37, 40, 24, 39, 41, 52, 48, 15, 17, 41, 62, 38, 58, 11, 16, 56, 30, 34, 20, 47, 44, 28, 62, 4, 47, 8, 2, 41, 57, 14, 5, 45, 5, 59, 33, 63, 7, 5, 50, 11, 40, 57, 62, 27, 16, 42, 61, 10, 13, 4, 20, 61, 16, 18, 46, 2, 42, 54, 57, 9, 25, 26, 32, 23, 51, 17, 45, 49, 37, 7, 41, 49, 62, 5, 24, 35, 4, 33, 17, 52, 12, 53, 33, 48, 26, 20, 45, 36, 33, 20, 27, 2, 35, 54, 21, 29, 42, 56, 47, 14, 49, 7, 7, 2, 5, 28, 27, 9, 30, 44, 27, 8, 21, 15, 26, 21, 24, 20, 5, 17, 19, 11, 48, 5, 52, 24, 12, 32, 26, 21, 42, 28, 59, 52, 45, 57, 28, 21, 50, 44, 10, 43, 35, 57, 26, 56, 27, 44, 23, 46, 40, 22, 44, 16, 32, 23, 63, 8, 48, 64, 12, 7, 30, 19, 16, 11, 16, 47, 64, 23, 29, 28, 26, 34, 47, 23, 51, 54, 11, 15, 64, 47, 53, 64, 8, 12, 20, 60, 11, 16, 37, 47, 2, 60, 53, 58, 30, 27, 55, 38, 6, 63, 16, 19, 5, 51, 58, 31, 34, 17, 53, 26, 57, 56, 2, 5, 48, 60, 26, 17, 41, 21, 37, 12, 45, 14, 24, 42, 52, 44, 35, 14, 6, 7, 16, 62, 61, 59, 49, 60, 2, 30, 19, 14, 60, 9, 19, 25, 7, 29, 19, 54, 53, 44, 20, 22, 24, 17, 55, 3, 27, 39, 5, 53, 2, 64, 53, 2, 27, 16, 10, 28, 42, 2, 52, 12, 8, 15, 56, 32, 15, 62, 46, 30, 11, 63, 36, 21, 13, 9, 47, 45, 18, 38, 11, 45, 2, 35, 13, 41, 63, 39, 18, 62, 10, 23, 45, 16, 41, 45, 37, 12, 14, 40, 63, 33, 35, 32, 33, 30, 2, 8, 21, 40, 37, 49, 4, 16, 21, 7, 6, 7, 21, 32, 6, 25, 64, 27, 23, 18, 64, 36, 24, 17, 5, 45, 24, 27, 61, 16, 55, 40, 3, 14, 42, 21, 6, 29, 25, 46, 9, 33, 27, 10, 18, 53, 9, 4, 6, 41, 34, 37, 59, 58, 13, 57, 15, 36, 44, 21, 18, 50, 2, 39, 36, 46, 45]}, {\"axis\": {\"matches\": true}, \"label\": \"m_dep\", \"values\": [0.6, 0.7, 0.9, 0.8, 0.6, 0.7, 0.8, 0.8, 0.7, 0.1, 0.1, 0.5, 0.6, 1.0, 0.7, 0.7, 0.1, 0.1, 0.6, 1.0, 0.8, 1.0, 0.3, 0.7, 0.4, 0.6, 0.3, 1.0, 0.3, 0.4, 0.2, 1.0, 0.9, 0.6, 0.7, 0.4, 0.6, 0.5, 0.3, 0.3, 0.6, 0.2, 0.2, 0.1, 0.8, 0.2, 1.0, 0.8, 0.2, 0.8, 0.7, 0.8, 0.3, 0.3, 0.5, 0.6, 0.5, 0.4, 0.5, 1.0, 0.6, 0.8, 0.8, 0.5, 0.7, 0.8, 0.3, 0.5, 0.6, 0.8, 0.8, 0.7, 0.1, 0.7, 0.5, 0.9, 0.4, 0.6, 0.2, 0.7, 0.9, 0.2, 0.9, 1.0, 0.8, 0.8, 0.5, 0.8, 0.1, 0.5, 0.5, 1.0, 0.4, 0.7, 0.8, 1.0, 0.2, 0.2, 0.7, 0.4, 0.3, 0.8, 0.7, 0.6, 0.5, 0.7, 1.0, 0.1, 0.3, 0.1, 1.0, 0.5, 0.7, 1.0, 0.6, 0.2, 0.3, 0.9, 0.9, 0.5, 0.3, 0.5, 0.9, 0.8, 0.8, 0.6, 0.3, 0.5, 0.6, 0.1, 0.8, 0.1, 1.0, 0.2, 0.7, 0.5, 0.9, 0.5, 0.3, 0.1, 0.1, 0.5, 0.3, 0.5, 0.9, 0.4, 0.2, 0.5, 0.4, 0.8, 0.1, 0.2, 0.9, 0.7, 0.7, 0.9, 0.8, 0.4, 0.5, 0.7, 0.6, 0.3, 0.5, 0.5, 0.1, 0.8, 0.9, 0.4, 0.2, 0.3, 0.5, 0.1, 0.2, 0.3, 1.0, 0.8, 0.8, 0.1, 0.1, 0.4, 0.3, 0.2, 0.1, 1.0, 0.2, 0.8, 0.1, 0.1, 0.5, 0.9, 0.6, 0.2, 0.3, 0.2, 0.1, 0.2, 0.8, 0.7, 0.3, 0.4, 0.6, 0.4, 0.3, 0.7, 0.1, 0.3, 0.3, 0.7, 0.1, 0.4, 0.1, 0.1, 0.5, 0.5, 0.4, 1.0, 0.2, 0.2, 0.4, 0.1, 0.5, 0.5, 0.5, 0.9, 0.1, 0.2, 0.1, 0.6, 0.1, 0.3, 0.8, 0.4, 0.5, 0.1, 0.8, 0.1, 0.1, 0.1, 1.0, 0.4, 0.1, 0.5, 0.3, 0.6, 0.4, 0.2, 0.4, 0.4, 0.8, 0.7, 0.6, 0.1, 1.0, 0.2, 0.6, 0.2, 0.1, 0.5, 0.5, 0.5, 0.7, 0.2, 0.6, 0.7, 0.8, 1.0, 0.3, 0.4, 0.9, 0.3, 0.9, 0.1, 0.3, 0.6, 0.9, 0.5, 0.8, 0.3, 0.8, 0.4, 1.0, 0.9, 0.5, 0.5, 0.1, 0.7, 0.3, 0.2, 0.8, 0.6, 0.2, 0.1, 0.8, 0.6, 0.3, 0.7, 0.3, 0.9, 0.7, 0.1, 0.6, 0.6, 0.1, 0.9, 0.4, 0.3, 0.5, 0.6, 0.8, 0.5, 0.5, 0.8, 0.5, 0.7, 0.6, 0.9, 0.8, 0.4, 0.3, 0.2, 0.6, 0.8, 0.9, 1.0, 0.9, 0.4, 0.4, 0.2, 0.5, 0.4, 0.8, 0.1, 0.1, 0.6, 0.2, 0.3, 0.9, 0.3, 0.8, 0.8, 0.1, 0.3, 0.1, 0.7, 0.3, 0.7, 0.2, 0.7, 0.6, 0.4, 0.3, 0.1, 1.0, 0.5, 0.8, 0.9, 0.9, 0.5, 0.3, 0.3, 0.9, 0.9, 0.8, 0.8, 0.3, 0.5, 0.4, 0.8, 0.7, 1.0, 0.1, 0.2, 0.7, 0.5, 0.6, 0.4, 0.1, 0.4, 0.4, 0.4, 0.9, 0.2, 0.1, 0.7, 0.2, 0.7, 0.2, 1.0, 0.6, 0.5, 0.1, 0.2, 0.6, 0.9, 0.3, 0.4, 0.9, 0.7, 0.2, 0.2, 0.2, 0.8, 0.6, 0.1, 0.2, 0.7, 0.1, 0.7, 0.7, 0.6, 0.2, 0.5, 0.7, 0.6, 0.8, 0.3, 0.3, 0.2, 0.2, 0.8, 0.4, 0.3, 0.2, 0.4, 0.1, 0.4, 0.3, 0.1, 0.5, 0.8, 0.2, 0.7, 0.4, 0.7, 0.8, 0.3, 0.8, 0.5, 0.6, 0.7, 0.9, 0.3, 0.3, 1.0, 0.1, 0.4, 0.9, 0.3, 0.8, 0.7, 0.2, 0.7, 0.8, 1.0, 0.7, 0.7, 0.1, 0.4, 0.9, 0.1, 0.4, 0.7, 0.1, 1.0, 0.6, 0.8, 0.9, 0.7, 0.1, 0.3, 0.7, 0.1, 1.0, 0.7, 0.8, 0.6, 0.8, 0.3, 0.1, 0.1, 0.9, 0.1, 0.3, 0.7, 0.2, 0.3, 0.7, 0.3, 0.8, 0.6, 0.3, 1.0, 0.2, 0.8, 0.1, 0.6, 1.0, 0.8, 0.4, 0.1, 0.1, 0.6, 1.0, 0.6, 0.3, 0.7, 0.2, 0.9, 0.7, 0.7, 0.6, 0.9, 0.5, 0.9, 0.9, 0.6, 0.8, 0.1, 0.1, 1.0, 0.7, 0.9, 0.5, 0.3, 0.1, 0.1, 0.5, 0.1, 0.1, 0.3, 0.7, 0.1, 0.9, 0.5, 1.0, 0.4, 0.9, 0.6, 0.2, 0.5, 0.1, 0.6, 0.1, 1.0, 0.1, 0.2, 0.8, 0.2, 0.9, 0.5, 0.9, 0.5, 0.1, 0.1, 0.2, 0.8, 0.3, 0.7, 0.3, 0.9, 0.1, 0.8, 0.6, 0.1, 0.1, 0.1, 0.4, 0.8, 0.1, 0.1, 0.8, 0.1, 0.9, 0.9, 0.3, 0.1, 0.7, 0.8, 0.1, 0.4, 0.9, 0.1, 0.3, 0.2, 0.1, 0.1, 0.2, 0.6, 0.9, 0.7, 0.6, 0.7, 0.3, 0.6, 0.1, 0.6, 0.1, 0.1, 0.9, 0.1, 0.4, 0.7, 0.3, 0.8, 0.8, 0.3, 0.1, 0.9, 0.9, 0.3, 0.1, 0.2, 0.2, 0.5, 0.1, 0.5, 0.5, 0.4, 0.1, 0.7, 1.0, 0.1, 0.6, 0.5, 0.8, 0.3, 0.2, 0.2, 0.5, 0.6, 0.7, 0.8, 0.2, 0.8, 0.5, 0.5, 0.3, 0.3, 0.9, 0.3, 0.8, 0.1, 0.9, 0.8, 0.1, 0.6, 0.7, 0.9, 0.6, 0.1, 0.8, 0.3, 0.2, 0.2, 0.4, 0.5, 0.7, 1.0, 0.2, 0.2, 0.3, 0.5, 0.7, 0.9, 0.7, 0.5, 0.5, 0.8, 0.7, 0.2, 0.5, 0.7, 0.2, 0.8, 0.3, 0.1, 0.9, 1.0, 0.5, 0.9, 0.9, 0.3, 0.9, 0.9, 0.1, 0.8, 0.1, 0.9, 0.1, 0.9, 0.1, 0.1, 0.4, 0.8, 0.4, 0.3, 0.4, 0.5, 0.7, 1.0, 0.1, 0.7, 0.7, 0.1, 0.6, 0.7, 0.7, 0.4, 0.6, 0.8, 0.6, 0.5, 0.4, 0.3, 0.6, 0.5, 0.7, 0.2, 0.7, 0.9, 0.5, 0.1, 0.1, 0.1, 0.3, 0.3, 0.2, 0.3, 0.7, 0.4, 0.3, 0.8, 0.8, 0.6, 0.3, 1.0, 0.6, 0.6, 0.2, 0.1, 0.2, 0.7, 0.2, 0.5, 0.8, 0.7, 0.8, 0.9, 0.2, 0.8, 0.3, 0.5, 0.9, 0.4, 0.8, 0.1, 0.3, 0.1, 0.2, 0.2, 0.3, 0.8, 0.6, 0.7, 0.4, 0.2, 0.5, 0.3, 0.2, 0.3, 0.6, 0.1, 0.1, 0.8, 0.3, 0.7, 0.9, 0.7, 0.4, 0.2, 0.6, 0.2, 0.1, 0.2, 0.4, 0.6, 0.9, 0.7, 0.2, 0.3, 0.1, 0.7, 0.3, 0.8, 0.6, 0.7, 0.3, 0.6, 0.4, 0.8, 0.8, 0.1, 1.0, 0.6, 1.0, 0.4, 0.2, 0.6, 0.7, 0.7, 0.9, 0.3, 0.3, 0.9, 0.3, 0.1, 0.1, 0.1, 0.2, 0.1, 0.5, 0.5, 0.4, 0.7, 0.8, 0.9, 0.3, 0.9, 1.0, 0.9, 0.5, 0.5, 0.7, 0.1, 0.6, 1.0, 0.7, 0.6, 0.5, 0.6, 0.8, 0.8, 0.5, 0.6, 0.4, 0.9, 0.6, 0.7, 0.4, 0.1, 0.8, 0.2, 0.8, 0.9, 0.5, 0.1, 0.5, 0.8, 0.6, 0.1, 0.4, 0.8, 0.7, 0.7, 0.2, 0.8, 0.5, 0.1, 0.1, 0.3, 0.1, 0.8, 0.8, 0.2, 0.5, 0.1, 0.2, 0.5, 0.2, 0.1, 0.7, 0.2, 0.9, 0.3, 0.1, 0.9, 0.5, 0.7, 0.9, 0.8, 0.1, 1.0, 0.3, 0.6, 0.7, 0.5, 0.8, 0.4, 0.4, 0.9, 0.4, 0.3, 0.4, 0.3, 0.2, 0.2, 0.6, 0.3, 0.4, 0.8, 0.6, 0.9, 0.5, 0.2, 0.4, 0.5, 0.9, 0.4, 0.5, 0.4, 0.7, 1.0, 0.5, 0.8, 0.2, 1.0, 0.1, 0.4, 0.1, 0.9, 0.1, 0.2, 0.1, 0.7, 0.5, 0.9, 0.6, 1.0, 0.2, 0.4, 0.1, 0.3, 0.1, 0.1, 0.2, 0.3, 0.1, 0.2, 0.5, 0.6, 0.3, 0.7, 0.4, 0.4, 1.0, 0.1, 0.1, 1.0, 1.0, 0.2, 0.7, 0.1, 0.8, 0.5, 1.0, 0.3, 0.6, 0.5, 0.5, 0.1, 0.2, 0.9, 0.8, 0.9, 0.9, 0.4, 0.5, 0.1, 0.3, 0.8, 0.4, 0.2, 0.5, 0.3, 0.2, 0.5, 0.9, 0.2, 0.4, 0.4, 0.6, 0.5, 0.1, 0.3, 0.9, 0.2, 0.5, 0.9, 0.9, 0.6, 0.1, 0.5, 0.1, 0.3, 1.0, 0.5, 0.3, 0.1, 0.4, 0.3, 0.9, 0.1, 0.7, 1.0, 0.9, 0.5, 0.1, 0.5, 0.7, 0.2, 0.4, 0.5, 0.3, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.3, 0.3, 1.0, 0.9, 0.1, 0.3, 0.9, 0.8, 0.4, 0.6, 0.9, 0.7, 0.6, 0.2, 0.2, 0.8, 0.7, 0.5, 0.8, 0.2, 0.1, 0.8, 0.3, 0.4, 0.1, 0.4, 0.1, 1.0, 0.9, 0.2, 0.8, 0.1, 0.2, 0.6, 0.7, 0.7, 0.1, 0.7, 0.2, 0.5, 0.6, 0.1, 0.8, 0.6, 0.5, 0.6, 0.2, 0.4, 0.9, 0.9, 0.1, 0.2, 0.9, 0.6, 0.7, 0.7, 0.1, 0.4, 0.9, 0.1, 0.9, 0.7, 0.8, 0.1, 0.9, 1.0, 0.1, 0.8, 0.1, 0.9, 0.1, 0.3, 0.7, 0.1, 0.1, 0.7, 0.7, 0.4, 0.1, 0.8, 0.8, 0.8, 0.7, 0.3, 0.4, 0.2, 0.5, 0.2, 0.3, 0.9, 0.8, 0.6, 0.1, 0.4, 0.9, 0.1, 0.7, 0.3, 0.2, 0.9, 0.1, 0.9, 0.7, 0.6, 0.8, 0.2, 0.5, 0.7, 0.1, 0.2, 0.5, 0.6, 1.0, 0.4, 0.8, 1.0, 0.6, 0.9, 0.9, 1.0, 0.1, 0.8, 0.2, 0.6, 0.3, 0.8, 0.5, 0.3, 0.9, 0.1, 0.5, 0.2, 0.6, 0.1, 0.8, 0.7, 0.9, 0.1, 0.3, 0.2, 0.3, 0.1, 0.3, 0.2, 0.7, 0.4, 0.3, 0.5, 0.1, 0.4, 0.6, 0.1, 0.9, 0.1, 0.7, 0.8, 0.9, 0.3, 0.1, 0.6, 0.7, 0.5, 0.1, 0.7, 0.9, 0.8, 0.6, 0.1, 0.5, 0.3, 0.1, 0.3, 0.5, 0.6, 0.7, 1.0, 0.3, 0.1, 0.8, 0.4, 0.7, 0.9, 0.4, 0.7, 1.0, 0.7, 0.2, 0.7, 0.6, 0.9, 0.8, 0.4, 0.4, 0.1, 0.1, 0.9, 0.4, 0.7, 0.4, 0.6, 0.1, 0.4, 0.6, 0.3, 0.3, 0.3, 0.3, 0.5, 0.1, 0.5, 0.1, 0.6, 0.4, 0.6, 1.0, 0.8, 0.8, 0.3, 0.5, 0.6, 0.4, 0.3, 0.5, 0.5, 0.6, 0.9, 0.1, 0.7, 0.6, 0.2, 0.1, 0.8, 0.8, 0.9, 0.2, 0.2, 0.7, 0.7, 0.3, 0.9, 0.4, 0.9, 0.8, 0.8, 0.2, 0.6, 0.9, 0.5, 0.2, 0.1, 0.4, 0.7, 0.2, 0.1, 0.3, 0.3, 0.8, 0.3, 0.7, 0.2, 0.5, 0.4, 0.6, 0.3, 0.1, 0.7, 0.3, 1.0, 0.1, 0.1, 0.1, 0.9, 0.8, 0.8, 0.4, 0.8, 0.2, 0.6, 0.7, 0.5, 0.1, 0.7, 0.8, 0.5, 0.1, 0.6, 0.7, 0.9, 0.1, 0.2, 0.8, 0.2, 0.5, 0.8, 0.5, 0.4, 0.9, 0.1, 0.1, 0.7, 0.1, 0.5, 0.2, 0.2, 0.4, 0.2, 0.9, 0.6, 0.4, 0.1, 0.1, 0.5, 0.8, 0.8, 0.8, 0.9, 0.1, 0.3, 0.8, 0.1, 0.4, 0.8, 0.6, 0.8, 0.9, 0.6, 0.4, 0.7, 0.1, 0.5, 0.1, 0.2, 0.4, 0.7, 0.5, 0.1, 0.8, 0.9, 0.1, 0.4, 0.2, 0.8, 0.8, 0.1, 0.1, 0.4, 0.6, 0.5, 0.3, 0.5, 0.7, 0.2, 0.4, 0.1, 0.2, 0.9, 0.2, 0.4, 0.3, 0.3, 0.9, 0.6, 0.6, 0.8, 0.9, 1.0, 0.2, 0.2, 0.1, 0.1, 0.1, 1.0, 0.1, 0.3, 0.3, 0.7, 0.8, 0.5, 0.7, 0.5, 1.0, 0.6, 0.3, 0.6, 0.4, 0.1, 0.4, 0.2, 0.7, 0.6, 0.3, 0.6, 0.1, 0.1, 0.7, 0.9, 0.6, 0.8, 0.4, 0.6, 0.4, 0.5, 0.2, 0.5, 0.7, 1.0, 0.2, 0.7, 0.5, 0.3, 0.8, 0.4, 0.3, 0.5, 0.1, 0.8, 0.3, 0.7, 0.4, 0.7, 0.9, 0.8, 0.5, 0.6, 0.1, 0.6, 0.1, 0.9, 0.7, 0.2, 0.4, 0.4, 0.4, 0.9, 0.7, 1.0, 0.1, 0.1, 0.1, 0.5, 0.8, 0.6, 0.1, 0.9, 0.4, 0.9, 0.2, 0.2, 0.4, 0.2, 0.1, 0.7, 0.1, 0.8, 0.1, 0.5, 0.7, 1.0, 0.8, 0.4, 0.6, 0.5, 0.7, 0.7, 0.1, 0.5, 0.3, 0.9, 0.4, 0.5, 0.9, 0.1, 1.0, 0.4, 0.1, 0.5, 0.8, 0.3, 0.2, 0.1, 0.5, 0.2, 0.9, 0.4, 0.9, 0.2, 0.4, 0.1, 0.7, 0.9, 0.8, 0.1, 0.6, 0.6, 0.7, 0.5, 0.8, 0.8, 0.9, 0.1, 1.0, 0.6, 0.6, 0.2, 0.8, 0.8, 0.8, 0.2, 0.4, 0.8, 0.7, 0.4, 0.2, 1.0, 0.1, 0.5, 0.7, 0.9, 0.9, 0.3, 0.5, 1.0, 1.0, 0.1, 0.1, 0.8, 0.9, 0.6, 0.3, 0.2, 0.1, 0.5, 0.7, 0.1, 0.5, 0.6, 0.9, 0.1, 0.7, 0.6, 0.9, 0.7, 0.9, 0.3, 0.3, 0.1, 0.8, 0.3, 0.6, 0.1, 0.6, 0.3, 0.1, 0.3, 0.7, 0.1, 0.1, 1.0, 0.6, 0.1, 0.7, 0.9, 0.9, 0.7, 0.1, 0.3, 0.6, 0.1, 0.1, 0.7, 0.5, 0.1, 0.2, 0.2, 0.5, 0.9, 0.4, 0.2, 0.5, 0.6, 0.3, 0.7, 0.2, 0.5, 0.2, 0.2, 0.1, 0.1, 0.2, 1.0, 0.7, 0.2, 0.1, 0.1, 1.0, 0.8, 1.0, 0.3, 0.1, 1.0, 0.9, 0.2, 0.8, 0.5, 0.5, 0.8, 0.8, 0.5, 0.5, 0.8, 0.8, 1.0, 0.9, 0.1, 0.7, 0.6, 0.2, 0.5, 0.1, 1.0, 0.6, 0.2, 0.3, 0.8, 0.2, 0.3, 0.5, 1.0, 1.0, 1.0, 0.2, 0.3, 0.2, 0.6, 0.1, 0.2, 0.6, 0.1, 0.6, 0.9, 0.8, 0.9, 0.4, 0.8, 0.3, 0.7, 0.9, 0.4, 0.1, 0.9, 0.1, 0.6, 0.2, 0.1, 0.7, 0.3, 0.3, 1.0, 0.2, 0.4, 0.3, 0.9, 0.5, 0.4, 0.6, 0.5, 0.4, 0.5, 0.3, 0.9, 0.7, 0.7, 0.4, 0.6, 0.5, 0.1, 0.3, 0.9, 0.1, 0.5, 0.6, 0.4, 0.2, 0.6, 0.8, 0.8, 0.2, 0.6, 0.7, 0.6, 0.3, 0.4, 1.0, 0.2, 0.1, 0.9, 0.1, 0.9, 0.1, 0.1, 0.1, 1.0, 0.5, 0.9, 0.9, 0.8, 0.1, 0.8, 0.5, 0.6, 0.7, 0.6, 0.1, 0.1, 0.5, 0.5, 0.8, 0.3, 0.2, 0.2, 0.8, 0.5, 0.7, 0.5, 0.1, 0.2, 0.1, 0.1, 0.5, 0.2, 0.7, 0.1, 0.6, 0.5, 0.9, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3, 0.3, 0.7, 0.1, 0.6, 1.0, 0.9, 0.4, 0.9, 0.9, 0.2, 0.1, 1.0, 0.3, 0.7, 0.9, 0.5, 0.5, 0.3, 0.6, 0.9, 0.4, 0.9, 0.2, 0.5, 0.4, 0.9, 0.4, 0.6, 1.0, 0.4, 0.6, 1.0, 0.9, 0.4, 0.6, 0.2, 0.2, 0.8, 0.3, 0.1, 0.1, 0.3, 0.1, 0.8, 0.7, 0.6, 0.4, 0.5, 0.2, 0.1, 0.1, 0.4, 0.1, 0.5, 0.1, 1.0, 0.2, 0.9, 0.7, 0.3, 0.4, 0.3, 0.4, 0.1, 0.8, 0.7, 0.3, 0.1, 0.8, 0.8, 0.7, 0.9, 0.6, 0.8, 0.5, 0.2, 0.8, 0.6, 0.3, 0.1, 0.2, 0.6, 0.1, 0.6, 0.9, 0.7, 1.0, 0.9, 0.9, 0.8, 0.6, 0.5, 0.2, 0.5, 0.7, 0.5, 0.2, 1.0, 0.8, 0.8, 0.9, 0.5, 0.9, 0.4, 1.0, 0.4, 0.3, 0.6, 0.9, 0.9, 0.8, 0.5, 0.5, 0.1, 0.6, 0.1, 1.0, 0.1, 0.1, 0.6, 0.1, 0.8, 0.3, 0.2, 0.7, 0.9, 0.6, 0.2, 1.0, 0.9, 0.4, 0.9, 0.7, 1.0, 0.8, 1.0, 0.2, 0.1, 0.5, 0.6, 0.6, 0.5, 0.4, 0.2, 0.3, 0.2, 0.5, 0.6, 0.6, 0.1, 0.9, 0.3, 0.5, 0.7, 0.2, 0.8, 0.3, 0.7, 1.0, 0.1, 0.4, 0.4, 0.4, 0.6, 0.3, 0.5, 0.2, 0.7, 0.8, 0.3, 0.3, 0.7, 0.5, 0.7, 0.2, 0.8, 0.4, 0.2, 1.0, 0.5, 0.8, 0.5, 0.6, 0.2, 0.1, 0.9, 0.2, 0.8, 1.0, 0.5, 0.2, 0.1, 1.0, 0.2, 0.6, 0.1, 0.2, 0.7, 0.9, 0.6, 0.6, 0.3, 0.8, 0.1, 0.3, 0.1, 1.0, 0.2, 0.7, 0.1, 0.9, 0.4, 0.4, 0.1, 0.9, 0.1, 0.5, 0.1, 0.4, 0.2, 0.8, 0.8, 0.2, 0.6, 0.1, 0.8, 0.2, 0.7, 0.1, 0.9]}, {\"axis\": {\"matches\": true}, \"label\": \"mobile_wt\", \"values\": [188, 136, 145, 131, 141, 164, 139, 187, 174, 93, 182, 177, 159, 198, 185, 159, 196, 121, 101, 121, 81, 156, 199, 114, 111, 114, 132, 143, 96, 200, 88, 150, 107, 100, 157, 160, 160, 119, 87, 159, 132, 185, 152, 166, 110, 118, 164, 196, 162, 111, 198, 127, 109, 102, 145, 104, 148, 107, 180, 128, 134, 144, 159, 168, 141, 81, 155, 182, 165, 80, 138, 104, 142, 141, 90, 188, 197, 172, 116, 111, 85, 199, 180, 114, 163, 96, 182, 178, 155, 100, 177, 171, 103, 83, 140, 196, 132, 194, 141, 156, 146, 119, 192, 199, 121, 103, 172, 198, 83, 163, 106, 199, 135, 194, 153, 89, 82, 107, 199, 102, 194, 80, 130, 200, 198, 89, 88, 189, 181, 100, 99, 184, 197, 195, 89, 108, 133, 179, 182, 155, 134, 147, 137, 141, 134, 141, 148, 141, 199, 159, 147, 190, 176, 84, 166, 139, 97, 109, 198, 142, 96, 152, 124, 166, 118, 183, 150, 160, 157, 110, 101, 101, 183, 197, 189, 143, 93, 101, 197, 165, 113, 178, 198, 84, 138, 92, 95, 103, 151, 150, 165, 190, 150, 117, 163, 117, 94, 106, 93, 119, 192, 173, 166, 105, 168, 115, 171, 182, 110, 142, 111, 159, 135, 128, 91, 181, 181, 194, 112, 88, 133, 111, 116, 123, 166, 200, 109, 128, 138, 127, 112, 105, 83, 192, 142, 129, 199, 183, 183, 107, 139, 187, 157, 110, 129, 127, 165, 99, 107, 105, 183, 93, 195, 101, 131, 190, 154, 141, 168, 162, 178, 88, 191, 175, 177, 100, 86, 136, 93, 148, 114, 89, 103, 98, 133, 125, 126, 144, 177, 187, 119, 100, 200, 110, 158, 164, 102, 170, 84, 163, 134, 151, 139, 143, 108, 114, 121, 181, 80, 105, 189, 181, 113, 117, 191, 98, 87, 178, 177, 185, 180, 80, 199, 184, 155, 165, 170, 199, 94, 161, 101, 177, 182, 83, 160, 115, 189, 129, 183, 193, 138, 191, 143, 126, 140, 106, 91, 114, 182, 169, 109, 106, 193, 120, 86, 196, 184, 185, 118, 149, 117, 123, 191, 138, 148, 181, 199, 194, 175, 86, 91, 172, 171, 187, 195, 92, 162, 99, 178, 185, 183, 131, 138, 135, 115, 189, 151, 97, 166, 145, 166, 142, 174, 187, 191, 138, 187, 146, 144, 137, 134, 119, 162, 145, 195, 141, 91, 173, 131, 169, 184, 93, 156, 171, 123, 83, 95, 173, 142, 193, 171, 112, 99, 101, 131, 186, 161, 176, 176, 100, 105, 84, 166, 158, 160, 182, 186, 134, 149, 194, 126, 116, 165, 153, 111, 134, 82, 130, 200, 123, 135, 146, 134, 156, 198, 183, 145, 114, 195, 158, 84, 107, 98, 178, 154, 94, 95, 84, 106, 186, 170, 162, 165, 109, 122, 118, 176, 133, 153, 96, 87, 100, 174, 104, 115, 120, 159, 101, 132, 128, 114, 95, 83, 148, 142, 200, 145, 169, 186, 109, 174, 132, 195, 124, 155, 131, 124, 186, 172, 104, 104, 170, 109, 176, 119, 159, 123, 83, 185, 134, 151, 160, 192, 166, 103, 90, 200, 134, 154, 187, 91, 135, 146, 117, 86, 100, 145, 190, 129, 114, 119, 193, 148, 182, 117, 152, 161, 193, 186, 137, 166, 176, 184, 160, 194, 185, 181, 146, 192, 83, 89, 190, 101, 113, 160, 123, 180, 200, 102, 169, 90, 191, 190, 143, 90, 113, 83, 118, 121, 150, 169, 180, 115, 171, 80, 129, 170, 112, 82, 92, 95, 88, 169, 198, 166, 88, 80, 147, 185, 196, 181, 182, 175, 118, 151, 142, 145, 163, 173, 193, 169, 85, 122, 128, 163, 139, 81, 188, 128, 102, 179, 190, 184, 125, 145, 172, 162, 98, 166, 196, 126, 197, 138, 114, 150, 131, 185, 130, 107, 118, 123, 101, 154, 185, 87, 160, 111, 147, 143, 167, 112, 178, 95, 149, 171, 167, 132, 189, 121, 136, 128, 107, 182, 186, 81, 146, 148, 132, 179, 117, 179, 192, 200, 120, 147, 128, 198, 191, 124, 168, 102, 104, 165, 151, 190, 145, 103, 106, 169, 144, 103, 102, 89, 127, 108, 131, 154, 159, 155, 155, 164, 85, 198, 183, 105, 81, 116, 119, 93, 176, 193, 130, 113, 185, 135, 112, 108, 154, 165, 169, 183, 112, 150, 169, 114, 144, 133, 121, 140, 184, 145, 192, 89, 159, 197, 175, 128, 122, 113, 158, 172, 125, 98, 96, 105, 86, 185, 187, 182, 138, 141, 102, 195, 83, 157, 168, 185, 142, 96, 157, 130, 165, 186, 122, 185, 155, 131, 105, 171, 138, 164, 142, 129, 143, 144, 174, 80, 197, 106, 196, 163, 145, 121, 198, 124, 166, 160, 161, 136, 83, 139, 97, 181, 146, 110, 187, 124, 179, 187, 188, 181, 199, 102, 108, 92, 145, 199, 144, 100, 153, 147, 112, 161, 198, 163, 102, 114, 163, 196, 82, 108, 158, 145, 125, 198, 131, 97, 123, 168, 188, 98, 95, 175, 130, 88, 126, 150, 101, 84, 124, 141, 92, 83, 115, 126, 157, 122, 93, 125, 191, 152, 120, 118, 169, 95, 105, 200, 134, 94, 98, 137, 163, 177, 80, 107, 97, 173, 133, 85, 127, 160, 190, 89, 127, 129, 197, 177, 170, 136, 143, 187, 172, 170, 156, 187, 128, 184, 82, 161, 136, 185, 122, 173, 139, 199, 167, 153, 182, 200, 112, 157, 198, 107, 116, 92, 100, 100, 158, 193, 190, 133, 150, 105, 192, 151, 166, 125, 87, 146, 83, 175, 155, 187, 101, 193, 90, 198, 168, 164, 162, 182, 200, 185, 102, 144, 182, 126, 155, 105, 168, 146, 168, 138, 94, 119, 113, 88, 152, 130, 84, 80, 95, 109, 153, 93, 93, 107, 151, 166, 135, 199, 102, 104, 147, 157, 159, 107, 101, 109, 147, 149, 134, 184, 148, 91, 134, 115, 155, 191, 162, 102, 122, 84, 138, 95, 94, 101, 132, 86, 89, 89, 100, 99, 164, 90, 124, 104, 170, 104, 82, 146, 187, 91, 91, 177, 193, 188, 197, 141, 188, 132, 124, 175, 87, 131, 104, 159, 157, 173, 158, 173, 100, 135, 186, 111, 167, 172, 139, 179, 162, 164, 97, 146, 90, 156, 99, 146, 199, 160, 159, 167, 87, 113, 132, 145, 97, 131, 199, 184, 172, 156, 85, 89, 135, 150, 189, 154, 162, 165, 88, 88, 103, 154, 102, 154, 137, 106, 160, 187, 175, 151, 124, 111, 191, 89, 111, 123, 92, 197, 137, 88, 197, 86, 198, 128, 144, 91, 111, 156, 179, 105, 118, 129, 126, 104, 140, 152, 146, 105, 162, 105, 136, 123, 90, 89, 185, 159, 194, 130, 90, 178, 174, 186, 105, 124, 138, 86, 138, 161, 182, 102, 148, 188, 107, 88, 174, 139, 80, 138, 158, 153, 173, 144, 186, 131, 107, 97, 143, 179, 105, 151, 134, 174, 129, 155, 190, 130, 83, 200, 86, 153, 86, 196, 112, 140, 182, 154, 198, 131, 118, 135, 103, 102, 116, 192, 101, 172, 148, 160, 178, 80, 146, 159, 186, 170, 182, 167, 127, 182, 104, 112, 85, 88, 136, 84, 115, 187, 121, 97, 104, 146, 161, 81, 158, 116, 150, 125, 156, 188, 180, 167, 170, 147, 101, 187, 115, 140, 185, 116, 123, 88, 105, 194, 84, 96, 196, 186, 142, 132, 134, 168, 181, 148, 181, 107, 185, 123, 181, 163, 140, 135, 146, 87, 172, 184, 194, 126, 109, 161, 130, 173, 198, 145, 88, 170, 186, 86, 114, 85, 146, 88, 152, 156, 144, 113, 139, 164, 126, 124, 163, 103, 113, 88, 99, 126, 136, 185, 105, 139, 101, 181, 172, 153, 123, 84, 131, 96, 111, 143, 134, 126, 87, 91, 131, 118, 170, 142, 156, 110, 170, 174, 158, 81, 178, 158, 100, 168, 141, 165, 130, 84, 172, 125, 177, 146, 95, 137, 133, 153, 178, 164, 192, 162, 198, 86, 80, 84, 177, 85, 129, 103, 156, 198, 112, 131, 135, 136, 157, 152, 124, 121, 149, 185, 86, 123, 182, 95, 173, 142, 82, 194, 170, 94, 196, 130, 125, 98, 104, 178, 94, 150, 131, 145, 128, 151, 144, 178, 168, 191, 173, 150, 92, 148, 124, 196, 109, 192, 123, 111, 145, 151, 113, 82, 114, 161, 188, 197, 117, 200, 192, 168, 169, 99, 100, 103, 93, 86, 199, 135, 106, 132, 179, 100, 184, 133, 182, 90, 160, 111, 120, 106, 197, 182, 103, 160, 166, 137, 99, 157, 99, 133, 124, 119, 144, 184, 95, 171, 111, 110, 101, 115, 145, 81, 109, 126, 101, 100, 119, 95, 160, 147, 149, 173, 183, 92, 88, 80, 169, 81, 172, 172, 80, 180, 151, 115, 109, 141, 154, 91, 135, 92, 186, 196, 90, 98, 169, 190, 185, 113, 141, 140, 121, 114, 173, 198, 159, 151, 150, 91, 190, 120, 198, 184, 158, 146, 110, 175, 153, 89, 94, 148, 197, 88, 127, 113, 98, 189, 151, 149, 117, 114, 99, 145, 158, 150, 104, 125, 196, 196, 134, 102, 103, 130, 120, 118, 169, 179, 147, 160, 176, 107, 132, 160, 157, 163, 160, 151, 185, 105, 92, 87, 155, 84, 81, 123, 122, 101, 199, 197, 180, 87, 147, 117, 89, 104, 169, 156, 103, 193, 189, 101, 183, 84, 86, 193, 199, 86, 138, 160, 146, 189, 182, 111, 89, 177, 100, 82, 199, 92, 159, 108, 155, 140, 153, 151, 192, 104, 130, 112, 95, 172, 191, 146, 182, 178, 124, 154, 115, 135, 93, 124, 113, 164, 124, 136, 138, 153, 132, 155, 120, 154, 122, 167, 86, 193, 141, 159, 171, 200, 87, 87, 191, 187, 142, 121, 127, 165, 103, 112, 80, 172, 134, 101, 119, 154, 174, 89, 105, 116, 170, 175, 150, 191, 145, 117, 135, 89, 192, 82, 97, 189, 161, 97, 182, 193, 135, 81, 89, 165, 131, 165, 179, 186, 169, 182, 104, 134, 189, 185, 88, 180, 151, 164, 199, 167, 194, 159, 157, 174, 88, 136, 81, 109, 93, 158, 114, 134, 175, 162, 196, 114, 155, 146, 93, 115, 119, 96, 199, 125, 120, 190, 111, 118, 152, 112, 80, 117, 178, 101, 121, 172, 127, 138, 158, 178, 143, 87, 190, 142, 169, 200, 114, 153, 196, 106, 138, 127, 82, 117, 180, 139, 174, 180, 91, 118, 110, 80, 156, 181, 153, 146, 82, 183, 156, 131, 85, 197, 200, 194, 145, 89, 147, 148, 146, 139, 177, 153, 129, 158, 129, 192, 167, 194, 132, 199, 121, 90, 83, 136, 104, 187, 186, 110, 182, 130, 161, 86, 188, 129, 158, 80, 82, 121, 104, 111, 157, 181, 92, 180, 128, 139, 198, 105, 164, 82, 88, 106, 150, 113, 105, 123, 111, 105, 138, 89, 126, 114, 119, 85, 125, 146, 98, 106, 133, 152, 136, 190, 117, 80, 88, 109, 109, 100, 83, 129, 128, 179, 152, 93, 167, 147, 173, 161, 82, 107, 85, 87, 98, 112, 197, 196, 103, 121, 195, 89, 141, 199, 109, 104, 155, 185, 157, 195, 102, 83, 124, 196, 106, 139, 162, 200, 197, 171, 152, 150, 168, 94, 130, 165, 169, 108, 103, 158, 131, 189, 101, 147, 137, 170, 123, 132, 131, 195, 185, 150, 125, 115, 198, 117, 190, 200, 164, 83, 131, 109, 114, 160, 158, 122, 170, 174, 100, 105, 127, 173, 137, 88, 114, 94, 187, 188, 116, 196, 163, 108, 94, 101, 187, 186, 128, 181, 176, 182, 102, 91, 134, 123, 122, 157, 162, 138, 197, 122, 169, 158, 146, 97, 130, 103, 155, 111, 97, 169, 132, 162, 153, 168, 101, 97, 136, 90, 124, 124, 101, 80, 160, 156, 153, 93, 99, 163, 195, 93, 154, 107, 110, 157, 134, 178, 148, 94, 162, 105, 102, 129, 185, 141, 141, 110, 157, 174, 89, 81, 193, 194, 176, 163, 112, 136, 149, 86, 194, 110, 113, 191, 85, 148, 104, 138, 143, 111, 163, 89, 117, 199, 88, 99, 129, 169, 161, 144, 199, 164, 149, 144, 91, 161, 80, 114, 83, 85, 113, 198, 122, 84, 106, 187, 108, 145, 168]}, {\"axis\": {\"matches\": true}, \"label\": \"n_cores\", \"values\": [2, 3, 5, 6, 2, 1, 8, 4, 7, 5, 5, 8, 4, 4, 1, 2, 8, 3, 5, 4, 7, 2, 4, 3, 3, 8, 4, 7, 2, 5, 7, 8, 1, 4, 8, 8, 4, 2, 6, 2, 1, 2, 2, 3, 2, 3, 8, 7, 8, 8, 3, 8, 2, 3, 7, 3, 8, 3, 6, 5, 3, 8, 2, 2, 3, 3, 4, 3, 2, 4, 7, 4, 3, 1, 4, 6, 3, 1, 4, 6, 7, 4, 5, 3, 1, 8, 5, 3, 5, 4, 3, 7, 7, 6, 3, 3, 5, 6, 2, 7, 6, 8, 5, 4, 7, 4, 5, 5, 1, 1, 3, 7, 6, 5, 7, 7, 8, 3, 1, 3, 8, 4, 3, 2, 5, 1, 4, 6, 3, 6, 7, 1, 7, 8, 4, 5, 6, 8, 7, 8, 1, 3, 1, 7, 3, 2, 2, 7, 3, 1, 5, 7, 6, 2, 2, 1, 4, 6, 6, 1, 5, 6, 5, 1, 8, 4, 8, 6, 7, 4, 7, 5, 4, 5, 5, 6, 7, 5, 2, 7, 1, 6, 8, 4, 5, 3, 6, 1, 1, 5, 1, 5, 8, 2, 4, 7, 3, 3, 4, 3, 1, 7, 3, 7, 6, 8, 6, 5, 7, 6, 3, 8, 5, 5, 1, 3, 8, 5, 3, 5, 5, 2, 4, 6, 4, 2, 1, 8, 4, 3, 5, 1, 8, 3, 2, 1, 5, 5, 7, 4, 8, 7, 6, 5, 7, 7, 2, 4, 5, 2, 2, 5, 8, 4, 1, 7, 4, 7, 3, 6, 7, 4, 1, 4, 2, 2, 6, 6, 5, 1, 7, 8, 1, 3, 2, 4, 4, 4, 7, 8, 6, 3, 7, 3, 8, 1, 7, 8, 1, 6, 2, 7, 6, 5, 5, 7, 8, 6, 8, 7, 1, 6, 1, 2, 2, 3, 3, 6, 6, 4, 2, 7, 3, 8, 2, 8, 4, 2, 1, 3, 2, 1, 1, 1, 6, 7, 1, 1, 1, 5, 1, 6, 4, 3, 2, 4, 3, 1, 1, 7, 3, 6, 8, 7, 1, 4, 5, 6, 1, 3, 1, 5, 8, 8, 7, 4, 1, 7, 5, 7, 7, 2, 2, 4, 4, 8, 6, 8, 1, 6, 2, 7, 5, 7, 5, 7, 2, 6, 4, 2, 7, 7, 3, 2, 1, 3, 8, 8, 7, 7, 6, 7, 4, 7, 4, 2, 8, 2, 4, 1, 3, 7, 3, 4, 6, 4, 5, 5, 8, 4, 5, 6, 5, 5, 3, 4, 8, 5, 7, 2, 5, 3, 4, 2, 4, 3, 6, 7, 6, 2, 8, 8, 1, 6, 4, 2, 7, 6, 2, 7, 8, 7, 7, 3, 7, 2, 2, 5, 1, 8, 2, 3, 3, 6, 7, 3, 3, 5, 5, 4, 7, 7, 8, 2, 8, 3, 4, 8, 7, 1, 7, 5, 3, 4, 3, 4, 1, 4, 4, 4, 6, 1, 5, 2, 6, 4, 5, 4, 5, 3, 6, 1, 7, 8, 5, 5, 3, 2, 3, 2, 8, 6, 7, 7, 2, 1, 7, 3, 5, 3, 4, 1, 1, 4, 6, 7, 8, 3, 4, 3, 1, 8, 3, 5, 2, 8, 8, 4, 2, 1, 6, 5, 5, 4, 7, 8, 6, 4, 6, 8, 1, 4, 8, 5, 2, 6, 8, 2, 1, 2, 5, 8, 8, 8, 4, 5, 8, 1, 1, 1, 8, 2, 8, 6, 6, 7, 8, 2, 2, 2, 5, 6, 2, 1, 7, 5, 1, 5, 4, 1, 8, 3, 5, 3, 3, 3, 1, 4, 8, 5, 5, 1, 5, 3, 4, 3, 2, 8, 5, 7, 6, 4, 8, 7, 2, 5, 7, 7, 8, 4, 3, 2, 7, 3, 7, 8, 1, 1, 5, 2, 4, 6, 7, 1, 4, 4, 3, 3, 6, 8, 7, 5, 8, 3, 6, 3, 7, 8, 5, 8, 5, 7, 6, 3, 5, 8, 5, 1, 4, 1, 7, 8, 7, 7, 3, 8, 7, 4, 6, 4, 3, 8, 3, 7, 4, 4, 8, 3, 6, 4, 4, 7, 7, 5, 3, 1, 2, 6, 6, 5, 7, 7, 1, 7, 3, 7, 5, 2, 4, 6, 5, 4, 5, 8, 1, 3, 5, 6, 5, 3, 3, 1, 1, 5, 7, 7, 5, 6, 2, 7, 2, 8, 2, 7, 2, 3, 4, 3, 5, 7, 2, 4, 7, 8, 3, 6, 7, 3, 3, 1, 6, 1, 7, 7, 7, 4, 6, 7, 7, 3, 6, 2, 6, 1, 1, 8, 1, 2, 3, 3, 2, 2, 6, 4, 1, 5, 3, 6, 3, 6, 4, 5, 7, 8, 5, 7, 2, 6, 6, 2, 8, 2, 2, 1, 7, 3, 2, 7, 1, 8, 5, 7, 2, 1, 4, 8, 3, 2, 1, 3, 5, 5, 6, 6, 8, 5, 7, 7, 7, 3, 1, 1, 3, 4, 6, 5, 3, 2, 1, 7, 4, 5, 1, 1, 6, 1, 4, 7, 5, 1, 3, 2, 7, 8, 6, 3, 6, 6, 7, 1, 3, 2, 6, 7, 8, 1, 5, 1, 8, 1, 4, 3, 2, 8, 3, 7, 1, 6, 3, 2, 7, 4, 2, 6, 4, 8, 3, 2, 8, 3, 5, 7, 2, 8, 7, 2, 7, 7, 3, 4, 3, 2, 3, 6, 2, 1, 6, 5, 5, 2, 5, 6, 3, 1, 6, 7, 4, 8, 1, 5, 1, 7, 5, 2, 3, 3, 1, 5, 4, 4, 8, 6, 2, 4, 3, 3, 1, 1, 3, 4, 4, 2, 2, 4, 4, 1, 3, 3, 7, 6, 8, 3, 1, 5, 1, 1, 4, 4, 7, 2, 1, 3, 4, 3, 3, 1, 6, 8, 3, 3, 8, 2, 7, 4, 3, 6, 8, 1, 2, 4, 2, 6, 7, 5, 8, 3, 8, 4, 1, 1, 2, 6, 1, 7, 5, 8, 2, 8, 1, 3, 8, 2, 4, 1, 4, 1, 6, 5, 2, 5, 1, 8, 3, 7, 4, 4, 7, 7, 5, 5, 7, 7, 4, 6, 6, 3, 2, 8, 6, 6, 5, 2, 3, 7, 8, 5, 1, 2, 8, 5, 4, 2, 3, 6, 2, 4, 7, 1, 8, 4, 4, 4, 3, 3, 5, 3, 4, 2, 2, 5, 1, 6, 7, 4, 7, 3, 7, 2, 3, 5, 7, 7, 3, 2, 7, 7, 5, 8, 7, 1, 1, 3, 5, 8, 5, 2, 4, 6, 8, 8, 4, 1, 1, 6, 6, 3, 6, 6, 1, 4, 7, 7, 7, 3, 6, 1, 5, 3, 6, 8, 5, 6, 4, 7, 5, 5, 1, 3, 3, 2, 5, 2, 4, 4, 3, 7, 5, 2, 7, 4, 5, 1, 3, 1, 1, 2, 7, 7, 1, 2, 8, 6, 2, 8, 2, 6, 2, 8, 2, 4, 8, 5, 8, 1, 3, 3, 5, 1, 8, 7, 1, 8, 4, 5, 8, 1, 2, 2, 6, 6, 5, 4, 3, 3, 7, 4, 4, 6, 7, 6, 1, 3, 8, 5, 7, 4, 5, 8, 7, 3, 8, 7, 3, 8, 3, 6, 4, 2, 7, 5, 8, 6, 3, 2, 2, 7, 4, 2, 3, 4, 6, 1, 8, 1, 7, 6, 8, 6, 7, 2, 3, 7, 1, 1, 8, 7, 2, 2, 7, 2, 6, 3, 5, 3, 4, 8, 5, 3, 3, 1, 3, 8, 4, 4, 3, 7, 2, 8, 7, 3, 4, 6, 8, 1, 3, 4, 3, 1, 1, 4, 2, 1, 6, 1, 5, 7, 7, 5, 1, 2, 8, 7, 8, 4, 6, 8, 7, 2, 5, 2, 1, 5, 6, 6, 2, 6, 6, 6, 7, 4, 1, 8, 7, 1, 1, 1, 3, 1, 2, 3, 6, 4, 2, 7, 1, 1, 8, 8, 7, 1, 5, 6, 6, 6, 5, 2, 3, 6, 5, 5, 4, 7, 4, 5, 8, 1, 8, 7, 7, 3, 2, 2, 5, 2, 7, 8, 3, 5, 3, 5, 4, 1, 4, 5, 2, 3, 6, 4, 1, 7, 7, 5, 6, 2, 5, 3, 8, 5, 1, 3, 8, 4, 4, 7, 8, 8, 4, 4, 2, 8, 7, 5, 6, 5, 5, 2, 8, 7, 4, 2, 1, 4, 2, 8, 8, 8, 2, 2, 4, 5, 6, 5, 1, 1, 4, 2, 4, 8, 8, 5, 2, 4, 7, 5, 2, 5, 4, 4, 2, 1, 8, 4, 8, 3, 5, 4, 4, 4, 2, 6, 5, 3, 5, 1, 8, 4, 6, 8, 7, 8, 4, 8, 4, 3, 2, 8, 4, 2, 5, 2, 1, 4, 2, 8, 7, 4, 5, 8, 5, 4, 6, 6, 1, 7, 6, 6, 8, 3, 8, 4, 8, 4, 8, 8, 6, 5, 2, 3, 3, 4, 4, 8, 5, 6, 4, 8, 8, 6, 4, 8, 3, 4, 5, 1, 1, 4, 1, 6, 1, 8, 2, 6, 2, 6, 4, 6, 6, 8, 3, 4, 3, 3, 1, 4, 6, 1, 1, 8, 1, 1, 6, 4, 6, 2, 5, 8, 3, 7, 5, 7, 3, 1, 1, 8, 6, 2, 8, 6, 3, 3, 2, 2, 8, 4, 6, 5, 4, 7, 6, 8, 6, 4, 5, 8, 1, 2, 7, 4, 7, 2, 1, 8, 2, 1, 4, 4, 3, 8, 1, 8, 6, 6, 7, 5, 8, 2, 3, 4, 7, 8, 2, 5, 2, 8, 1, 8, 6, 1, 3, 2, 7, 7, 8, 6, 2, 5, 8, 2, 4, 6, 4, 5, 1, 2, 6, 4, 2, 4, 6, 7, 2, 4, 8, 8, 2, 8, 8, 1, 5, 6, 2, 2, 7, 8, 4, 1, 5, 2, 4, 2, 8, 6, 4, 1, 7, 3, 5, 7, 6, 1, 4, 2, 1, 8, 5, 8, 6, 4, 2, 5, 5, 5, 6, 1, 3, 8, 7, 1, 6, 8, 3, 8, 4, 1, 2, 5, 5, 4, 4, 6, 2, 8, 6, 2, 4, 3, 2, 5, 1, 7, 8, 1, 3, 6, 2, 4, 2, 3, 4, 7, 1, 4, 2, 5, 6, 8, 1, 1, 7, 4, 8, 4, 8, 4, 6, 8, 6, 1, 1, 2, 4, 5, 8, 4, 6, 6, 1, 7, 4, 1, 8, 4, 5, 5, 2, 1, 4, 8, 6, 6, 5, 2, 1, 2, 1, 7, 4, 2, 6, 7, 3, 3, 5, 4, 4, 4, 1, 4, 7, 8, 4, 4, 8, 5, 8, 8, 7, 4, 4, 4, 3, 4, 7, 1, 5, 2, 2, 6, 5, 7, 2, 5, 5, 2, 5, 8, 1, 3, 3, 6, 8, 6, 8, 5, 8, 7, 2, 3, 2, 5, 4, 5, 2, 4, 6, 1, 6, 4, 2, 4, 3, 8, 8, 2, 2, 8, 7, 6, 6, 7, 8, 7, 3, 3, 8, 2, 8, 7, 3, 3, 3, 3, 5, 6, 2, 4, 4, 1, 4, 8, 5, 2, 4, 6, 2, 1, 4, 7, 6, 5, 3, 6, 6, 2, 2, 5, 6, 1, 8, 7, 5, 7, 5, 4, 2, 5, 4, 4, 5, 5, 5, 2, 7, 5, 6, 3, 1, 1, 3, 6, 1, 7, 1, 3, 7, 4, 8, 4, 4, 2, 2, 7, 3, 1, 7, 1, 2, 7, 7, 3, 7, 4, 4, 6, 7, 6, 4, 5, 1, 7, 2, 2, 8, 3, 7, 4, 3, 7, 8, 3, 6, 7, 3, 5, 8, 4, 6, 6, 4, 1, 5, 5, 7, 6, 1, 1, 4, 3, 7, 8, 1, 2, 2, 5, 4, 6, 3, 1, 5, 4, 1, 8, 4, 5, 5, 7, 8, 5, 5, 4, 3, 2, 8, 4, 2, 3, 2, 7, 7, 2, 6, 1, 8, 3, 5, 6, 7, 7, 3, 7, 8, 1, 3, 8, 4, 2, 5, 4, 1, 5, 4, 6, 6, 6, 5, 8, 6, 6, 7, 7, 3, 7, 8, 1, 4, 5, 5, 5, 4, 6, 6, 5, 7, 3, 4, 8, 2, 6, 2, 8, 5, 7, 6, 1, 2, 8, 7, 2, 5, 3, 8, 3, 6, 5, 7, 5, 2, 4, 1, 3, 1, 8, 3, 5, 1, 6, 4, 8, 5, 6]}, {\"axis\": {\"matches\": true}, \"label\": \"pc\", \"values\": [2, 6, 6, 9, 14, 7, 10, 0, 14, 15, 1, 18, 17, 11, 17, 16, 4, 17, 18, 11, 14, 2, 7, 20, 13, 3, 19, 6, 18, 7, 9, 20, 18, 0, 3, 17, 10, 18, 17, 20, 14, 10, 19, 7, 14, 1, 14, 10, 8, 15, 19, 19, 0, 10, 0, 20, 19, 9, 14, 4, 5, 6, 0, 18, 3, 17, 14, 0, 1, 1, 11, 4, 16, 5, 6, 20, 15, 3, 20, 8, 7, 19, 3, 15, 13, 17, 5, 11, 13, 14, 5, 18, 0, 15, 9, 19, 16, 18, 1, 7, 13, 9, 4, 10, 3, 3, 10, 16, 4, 7, 4, 20, 1, 14, 19, 9, 0, 11, 12, 0, 7, 2, 10, 7, 4, 20, 1, 17, 16, 1, 3, 14, 10, 12, 5, 0, 8, 7, 14, 2, 18, 4, 5, 6, 11, 2, 7, 4, 7, 7, 17, 14, 12, 9, 8, 6, 4, 18, 0, 16, 13, 16, 3, 20, 17, 8, 4, 3, 8, 20, 6, 5, 8, 4, 1, 15, 10, 1, 13, 4, 20, 12, 5, 19, 12, 13, 9, 17, 5, 5, 18, 15, 8, 6, 15, 8, 13, 7, 4, 20, 0, 15, 9, 17, 19, 1, 17, 16, 4, 6, 1, 0, 17, 16, 0, 7, 2, 9, 1, 6, 19, 10, 12, 11, 4, 5, 19, 1, 19, 18, 6, 6, 17, 19, 15, 8, 12, 16, 1, 2, 1, 0, 20, 10, 14, 12, 15, 15, 10, 4, 1, 0, 7, 7, 17, 16, 13, 11, 1, 17, 10, 1, 1, 10, 16, 15, 14, 12, 5, 6, 18, 2, 12, 14, 15, 8, 13, 19, 13, 4, 20, 14, 11, 7, 9, 3, 7, 0, 20, 4, 11, 13, 18, 14, 5, 1, 19, 2, 13, 17, 19, 7, 20, 14, 8, 20, 2, 20, 13, 8, 6, 2, 14, 17, 0, 17, 9, 13, 8, 8, 20, 13, 2, 17, 20, 14, 11, 15, 16, 8, 10, 3, 10, 17, 13, 1, 1, 19, 6, 1, 9, 13, 15, 20, 4, 4, 7, 13, 15, 4, 16, 20, 5, 16, 18, 12, 1, 3, 13, 3, 19, 18, 7, 2, 12, 14, 2, 17, 12, 12, 12, 18, 19, 0, 14, 10, 0, 10, 14, 9, 13, 10, 4, 1, 20, 5, 17, 15, 8, 12, 16, 3, 16, 19, 1, 13, 12, 2, 12, 4, 12, 18, 10, 5, 9, 3, 2, 11, 15, 3, 8, 13, 17, 0, 17, 12, 5, 4, 2, 12, 12, 17, 14, 13, 6, 4, 10, 2, 7, 19, 7, 15, 2, 8, 5, 11, 15, 10, 16, 0, 9, 2, 9, 16, 12, 13, 13, 16, 18, 3, 10, 4, 5, 12, 2, 19, 7, 14, 5, 12, 12, 17, 20, 14, 15, 10, 6, 11, 0, 12, 13, 10, 4, 2, 7, 14, 13, 18, 16, 4, 4, 3, 17, 2, 18, 3, 15, 14, 4, 14, 20, 10, 9, 9, 0, 17, 12, 20, 2, 4, 16, 18, 2, 6, 0, 2, 10, 15, 6, 15, 20, 13, 14, 19, 14, 6, 0, 18, 4, 20, 17, 10, 7, 11, 0, 16, 15, 11, 11, 18, 8, 10, 17, 13, 2, 7, 20, 16, 6, 11, 9, 7, 9, 18, 0, 17, 4, 7, 3, 4, 14, 6, 0, 20, 10, 7, 18, 10, 7, 7, 10, 16, 14, 4, 17, 15, 15, 15, 20, 9, 8, 16, 15, 1, 13, 10, 16, 12, 9, 15, 18, 17, 17, 8, 20, 4, 10, 15, 18, 4, 12, 5, 18, 1, 20, 2, 12, 10, 7, 13, 5, 20, 7, 6, 0, 13, 14, 15, 11, 8, 5, 3, 15, 0, 14, 15, 7, 9, 6, 10, 2, 18, 1, 16, 6, 19, 3, 3, 3, 2, 2, 2, 0, 10, 18, 11, 5, 11, 8, 6, 2, 1, 10, 3, 6, 8, 7, 14, 16, 0, 16, 11, 18, 20, 13, 7, 16, 17, 17, 10, 4, 2, 9, 12, 0, 5, 13, 7, 8, 3, 17, 13, 1, 9, 4, 11, 7, 6, 17, 15, 3, 7, 13, 3, 19, 9, 14, 1, 12, 9, 3, 15, 7, 3, 14, 12, 6, 6, 7, 9, 13, 3, 4, 12, 0, 8, 6, 7, 15, 14, 1, 3, 10, 20, 0, 11, 13, 15, 12, 13, 15, 1, 9, 3, 8, 18, 2, 17, 10, 11, 10, 10, 17, 8, 20, 6, 11, 2, 14, 10, 9, 1, 0, 7, 13, 20, 18, 3, 9, 9, 3, 7, 18, 9, 10, 18, 1, 10, 3, 20, 18, 4, 1, 0, 19, 10, 2, 13, 5, 19, 8, 17, 4, 2, 0, 2, 16, 10, 4, 1, 16, 18, 17, 15, 10, 17, 18, 10, 15, 17, 1, 8, 15, 15, 9, 3, 3, 2, 6, 19, 17, 17, 16, 19, 2, 2, 18, 9, 18, 10, 8, 20, 10, 18, 2, 14, 14, 5, 16, 14, 0, 10, 3, 0, 19, 9, 20, 8, 9, 6, 8, 16, 3, 7, 12, 18, 14, 16, 5, 6, 6, 6, 10, 7, 9, 15, 16, 18, 18, 18, 19, 7, 10, 6, 12, 7, 3, 11, 19, 4, 7, 20, 2, 14, 4, 15, 19, 4, 7, 18, 10, 14, 19, 0, 4, 0, 13, 17, 9, 11, 12, 6, 16, 5, 13, 10, 18, 1, 20, 1, 4, 6, 1, 16, 9, 11, 9, 7, 17, 6, 17, 11, 18, 11, 4, 19, 11, 12, 3, 0, 6, 14, 9, 20, 20, 5, 13, 4, 2, 1, 15, 11, 20, 4, 16, 4, 14, 16, 7, 9, 1, 10, 14, 12, 3, 7, 2, 2, 15, 16, 10, 1, 4, 2, 0, 0, 17, 16, 16, 15, 18, 10, 16, 7, 14, 16, 14, 3, 14, 13, 9, 6, 6, 8, 19, 17, 0, 0, 10, 11, 9, 10, 12, 12, 11, 18, 11, 14, 6, 9, 14, 0, 3, 9, 0, 15, 17, 0, 6, 14, 19, 18, 16, 15, 15, 20, 9, 9, 12, 5, 3, 12, 16, 12, 19, 1, 15, 17, 6, 7, 17, 10, 0, 0, 19, 19, 15, 7, 18, 20, 1, 3, 20, 16, 10, 17, 2, 12, 0, 0, 7, 10, 17, 1, 11, 2, 3, 13, 4, 14, 14, 8, 7, 14, 14, 14, 13, 5, 9, 5, 7, 7, 4, 7, 13, 3, 7, 0, 12, 11, 6, 5, 17, 20, 6, 8, 19, 7, 5, 20, 18, 12, 1, 14, 0, 12, 7, 16, 0, 3, 4, 2, 13, 20, 1, 3, 13, 15, 12, 14, 3, 10, 15, 13, 2, 14, 16, 3, 9, 0, 13, 20, 7, 13, 12, 13, 2, 1, 9, 11, 10, 0, 5, 1, 7, 20, 9, 19, 1, 17, 10, 20, 5, 6, 11, 9, 13, 14, 18, 9, 17, 9, 1, 6, 16, 3, 3, 3, 6, 12, 6, 4, 17, 11, 2, 0, 6, 11, 1, 1, 14, 13, 14, 6, 3, 1, 3, 15, 15, 0, 18, 2, 9, 12, 13, 17, 1, 0, 20, 12, 14, 12, 7, 1, 9, 12, 15, 9, 10, 1, 0, 1, 15, 0, 1, 2, 13, 7, 11, 10, 14, 18, 8, 8, 6, 15, 17, 2, 1, 3, 20, 0, 1, 12, 20, 14, 2, 8, 15, 11, 10, 9, 15, 7, 11, 4, 20, 12, 10, 20, 5, 3, 5, 9, 1, 9, 16, 7, 3, 8, 6, 5, 6, 6, 13, 19, 7, 3, 19, 9, 4, 19, 17, 20, 13, 15, 9, 0, 9, 10, 14, 3, 2, 3, 20, 8, 12, 0, 18, 6, 17, 10, 12, 11, 5, 4, 18, 4, 5, 2, 17, 10, 8, 17, 5, 15, 14, 3, 7, 13, 16, 8, 6, 8, 6, 1, 0, 13, 15, 8, 11, 14, 0, 9, 4, 19, 0, 11, 20, 3, 19, 9, 16, 17, 13, 7, 10, 11, 9, 11, 5, 9, 0, 10, 17, 8, 0, 18, 9, 17, 7, 13, 14, 11, 0, 6, 4, 20, 0, 18, 20, 6, 7, 18, 9, 13, 16, 8, 19, 1, 8, 13, 12, 18, 9, 15, 1, 19, 1, 4, 12, 15, 15, 1, 19, 9, 12, 1, 20, 9, 16, 7, 13, 8, 6, 15, 16, 9, 17, 17, 0, 11, 6, 8, 2, 2, 2, 5, 0, 8, 2, 10, 8, 19, 20, 2, 19, 20, 10, 10, 3, 19, 8, 1, 1, 9, 8, 6, 1, 17, 8, 7, 7, 18, 5, 7, 20, 10, 4, 9, 7, 2, 17, 15, 5, 13, 19, 18, 2, 4, 7, 10, 4, 5, 9, 9, 20, 1, 19, 19, 4, 6, 13, 2, 0, 20, 8, 9, 20, 8, 19, 1, 7, 1, 12, 10, 8, 18, 2, 18, 8, 3, 4, 8, 8, 0, 20, 7, 20, 12, 10, 5, 11, 13, 9, 10, 7, 20, 2, 20, 0, 6, 20, 0, 20, 13, 11, 1, 10, 11, 20, 11, 14, 8, 15, 1, 6, 15, 14, 8, 14, 14, 14, 12, 2, 6, 17, 7, 6, 7, 15, 0, 4, 10, 18, 9, 16, 20, 18, 2, 17, 10, 10, 10, 16, 18, 17, 4, 2, 4, 16, 7, 17, 1, 10, 18, 3, 6, 10, 9, 11, 5, 19, 10, 14, 15, 10, 12, 16, 8, 7, 16, 19, 8, 5, 8, 16, 10, 1, 18, 1, 19, 1, 3, 19, 2, 3, 3, 19, 19, 10, 0, 10, 2, 14, 4, 9, 1, 2, 17, 6, 10, 14, 2, 13, 3, 5, 17, 1, 20, 7, 1, 19, 0, 10, 14, 15, 12, 14, 7, 11, 12, 19, 2, 17, 13, 2, 3, 2, 11, 16, 10, 17, 18, 17, 13, 4, 2, 10, 20, 17, 11, 15, 3, 7, 19, 19, 12, 8, 7, 1, 20, 10, 2, 10, 12, 14, 12, 13, 14, 12, 6, 4, 14, 20, 7, 13, 13, 7, 2, 17, 12, 17, 16, 7, 1, 3, 17, 14, 3, 13, 8, 9, 1, 18, 20, 17, 19, 15, 9, 17, 11, 9, 0, 6, 19, 3, 7, 20, 7, 6, 10, 13, 9, 5, 1, 14, 7, 9, 9, 6, 0, 4, 5, 15, 9, 18, 19, 14, 7, 0, 8, 14, 19, 12, 10, 5, 0, 9, 7, 1, 9, 0, 9, 16, 20, 13, 20, 12, 16, 14, 1, 11, 9, 12, 20, 3, 1, 6, 2, 8, 20, 14, 10, 3, 19, 7, 16, 3, 16, 8, 7, 19, 1, 7, 20, 3, 8, 18, 5, 18, 10, 8, 6, 0, 14, 9, 20, 6, 1, 11, 9, 9, 3, 1, 3, 6, 9, 10, 4, 3, 6, 15, 17, 8, 11, 13, 19, 9, 10, 19, 2, 7, 4, 10, 3, 16, 4, 7, 0, 9, 19, 4, 16, 15, 6, 16, 11, 18, 2, 20, 12, 7, 6, 9, 11, 2, 9, 20, 18, 1, 10, 17, 6, 19, 16, 6, 17, 0, 8, 2, 2, 1, 16, 8, 7, 4, 10, 15, 12, 2, 17, 0, 16, 14, 10, 14, 17, 17, 8, 12, 12, 2, 16, 4, 7, 6, 11, 14, 3, 2, 10, 7, 19, 9, 4, 6, 15, 8, 14, 10, 14, 4, 19, 3, 19, 15, 4, 13, 12, 5, 20, 17, 8, 20, 20, 12, 13, 4, 16, 15, 20, 0, 8, 2, 7, 1, 0, 0, 4, 20, 16, 20, 10, 18, 14, 20, 1, 20, 8, 13, 6, 12, 18, 4, 12, 9, 16, 9, 20, 9, 2, 15, 2, 4, 11, 0, 11, 11, 10, 14, 0, 8, 11, 8, 18, 10, 9, 11, 7, 17, 1, 11, 11, 20, 6, 14, 8, 15, 7, 7, 0, 12, 10, 7, 12, 15, 15, 8, 20, 16, 11, 6, 8, 15, 12, 1, 16, 14, 5, 10, 16, 20, 9, 1, 6, 12, 2, 19, 4, 7, 20, 17, 4, 11, 6, 4, 8, 0, 20, 4, 7, 9, 3, 16, 8, 7, 16, 11, 2, 10, 20, 20, 16, 2, 15, 11, 15, 3, 11, 9, 19, 4, 0, 2, 14, 3, 3, 5, 16]}, {\"axis\": {\"matches\": true}, \"label\": \"px_height\", \"values\": [20, 905, 1263, 1216, 1208, 1004, 381, 512, 386, 1137, 248, 151, 607, 344, 356, 862, 984, 441, 658, 902, 1314, 974, 407, 466, 201, 291, 550, 511, 187, 1171, 1358, 413, 85, 178, 311, 1134, 429, 609, 347, 1448, 434, 1725, 685, 880, 1580, 186, 202, 27, 1025, 885, 1042, 1382, 546, 1013, 690, 667, 822, 581, 265, 127, 916, 361, 322, 753, 504, 651, 1565, 108, 459, 205, 570, 1172, 160, 4, 763, 339, 1040, 166, 1168, 990, 319, 1397, 708, 1077, 928, 516, 96, 1025, 666, 557, 1605, 1226, 769, 403, 177, 942, 651, 1572, 476, 1233, 836, 824, 391, 320, 325, 125, 846, 1770, 293, 1633, 1198, 768, 70, 117, 1158, 1055, 635, 322, 275, 1445, 550, 1242, 597, 538, 59, 819, 179, 1259, 396, 713, 262, 622, 327, 522, 313, 226, 605, 692, 1386, 304, 1590, 229, 839, 507, 436, 756, 387, 1265, 1852, 875, 140, 577, 975, 510, 583, 674, 1125, 747, 1612, 311, 116, 554, 811, 730, 577, 660, 450, 1223, 1109, 45, 299, 506, 635, 1045, 393, 650, 433, 839, 1176, 650, 218, 564, 986, 561, 65, 420, 1365, 619, 849, 611, 953, 642, 1417, 247, 710, 718, 1100, 59, 938, 825, 286, 1273, 295, 852, 710, 1277, 129, 690, 701, 309, 213, 1112, 199, 55, 1230, 1052, 541, 55, 809, 1738, 404, 262, 759, 42, 397, 964, 233, 1022, 411, 954, 1528, 416, 638, 231, 574, 148, 730, 564, 1290, 632, 529, 1273, 667, 1170, 169, 105, 793, 490, 1201, 246, 417, 1177, 78, 323, 707, 457, 356, 936, 912, 179, 1914, 526, 156, 344, 865, 1552, 546, 1097, 372, 1285, 1242, 302, 594, 303, 1858, 403, 32, 590, 235, 630, 357, 246, 475, 1054, 555, 607, 1360, 142, 681, 104, 637, 410, 1830, 802, 311, 56, 344, 1053, 221, 506, 1728, 63, 418, 854, 178, 1869, 185, 170, 454, 866, 44, 1331, 1285, 741, 520, 114, 1153, 474, 39, 419, 584, 312, 19, 207, 363, 209, 159, 649, 1132, 1392, 1001, 712, 418, 384, 119, 627, 1086, 195, 14, 745, 253, 333, 629, 707, 1314, 1715, 513, 13, 1243, 300, 58, 1107, 134, 275, 176, 1236, 336, 1116, 1010, 205, 1453, 788, 1301, 463, 188, 1028, 1419, 1471, 168, 1125, 130, 481, 1411, 79, 1187, 561, 96, 77, 493, 1173, 964, 634, 639, 127, 431, 71, 111, 356, 798, 747, 408, 373, 1362, 475, 743, 447, 42, 30, 162, 347, 346, 727, 180, 1641, 269, 250, 1186, 77, 507, 379, 264, 1486, 694, 579, 495, 315, 329, 548, 531, 191, 823, 96, 841, 195, 21, 235, 1417, 286, 649, 966, 1172, 270, 781, 418, 126, 228, 1091, 1452, 674, 398, 449, 1184, 675, 1792, 177, 1013, 1058, 592, 810, 533, 558, 681, 1017, 765, 1176, 947, 1314, 609, 754, 619, 453, 205, 282, 130, 494, 275, 447, 281, 6, 292, 164, 631, 897, 268, 861, 331, 1064, 509, 622, 957, 494, 236, 980, 1789, 831, 902, 636, 831, 518, 233, 725, 170, 157, 1142, 840, 401, 395, 48, 982, 478, 581, 1215, 594, 683, 536, 83, 905, 249, 803, 459, 584, 1596, 418, 364, 946, 186, 1428, 1613, 1127, 691, 1802, 751, 859, 187, 206, 1673, 710, 1331, 263, 97, 129, 1213, 189, 628, 320, 837, 1087, 781, 251, 262, 767, 1226, 389, 920, 1027, 271, 369, 1258, 417, 1345, 613, 912, 666, 236, 186, 416, 430, 168, 1467, 179, 159, 120, 762, 600, 286, 1350, 27, 1597, 1177, 402, 577, 52, 364, 934, 250, 214, 612, 1111, 163, 202, 729, 1079, 361, 338, 1698, 1052, 1108, 951, 1197, 473, 1294, 410, 178, 782, 434, 464, 137, 492, 1250, 385, 427, 1024, 240, 886, 1233, 183, 88, 1016, 80, 611, 569, 304, 276, 1131, 551, 436, 553, 1391, 981, 680, 687, 1060, 217, 1140, 304, 504, 832, 935, 446, 942, 897, 1208, 159, 632, 1308, 1002, 1331, 150, 1248, 108, 320, 437, 295, 1058, 79, 593, 693, 300, 234, 1545, 59, 1103, 91, 177, 58, 447, 942, 88, 371, 1446, 181, 1311, 1791, 1568, 706, 871, 1074, 501, 271, 920, 1171, 293, 833, 485, 182, 207, 1698, 165, 1086, 192, 1156, 377, 709, 729, 543, 553, 526, 637, 113, 1114, 461, 1430, 304, 471, 806, 80, 204, 545, 112, 1331, 1055, 609, 290, 1039, 1485, 1122, 462, 1384, 1502, 589, 82, 68, 689, 208, 1339, 722, 717, 433, 1298, 186, 1397, 618, 1214, 613, 649, 193, 22, 1464, 280, 690, 39, 337, 1036, 1347, 22, 327, 778, 482, 591, 229, 901, 715, 172, 681, 867, 286, 451, 1119, 526, 1299, 217, 1109, 434, 671, 1404, 856, 381, 85, 562, 1111, 209, 997, 91, 730, 606, 445, 286, 985, 1617, 375, 1795, 486, 757, 273, 491, 762, 448, 231, 1713, 438, 68, 698, 510, 965, 1399, 532, 8, 154, 198, 860, 461, 109, 193, 961, 243, 1626, 480, 123, 400, 190, 626, 1, 548, 632, 349, 1686, 1455, 95, 580, 297, 951, 667, 1047, 408, 1150, 333, 641, 653, 103, 517, 785, 531, 564, 761, 1179, 673, 472, 412, 977, 359, 224, 714, 195, 843, 308, 522, 364, 806, 518, 519, 792, 382, 214, 859, 342, 713, 172, 118, 540, 18, 987, 535, 1092, 327, 517, 994, 491, 268, 210, 144, 729, 82, 188, 150, 1222, 331, 1178, 347, 75, 492, 65, 881, 351, 1842, 758, 1323, 1175, 1092, 647, 1102, 319, 215, 945, 381, 546, 1096, 575, 519, 342, 291, 1091, 352, 165, 385, 1920, 396, 1134, 721, 687, 777, 176, 458, 485, 641, 798, 256, 370, 191, 1571, 276, 573, 160, 90, 138, 983, 275, 149, 42, 793, 625, 521, 1530, 442, 152, 1242, 29, 501, 87, 648, 521, 120, 985, 341, 751, 993, 43, 1262, 361, 104, 618, 815, 1587, 138, 584, 750, 820, 8, 171, 960, 160, 612, 709, 624, 538, 1295, 1164, 829, 199, 825, 519, 169, 1734, 76, 1698, 819, 387, 985, 970, 767, 512, 56, 590, 462, 126, 578, 655, 1028, 88, 1692, 358, 64, 259, 236, 873, 858, 1405, 237, 650, 1949, 541, 361, 335, 1638, 1109, 111, 1285, 1480, 322, 1220, 511, 1011, 629, 140, 1081, 297, 1512, 275, 1136, 1145, 477, 954, 741, 547, 1194, 146, 471, 169, 1895, 1537, 236, 821, 466, 393, 400, 774, 284, 465, 705, 38, 51, 850, 349, 387, 706, 951, 179, 426, 653, 1578, 280, 251, 1463, 1432, 477, 1122, 145, 416, 847, 868, 594, 518, 663, 371, 1749, 586, 409, 35, 1008, 442, 983, 875, 952, 244, 65, 1749, 716, 433, 134, 179, 804, 787, 704, 991, 379, 489, 1518, 855, 1261, 115, 1203, 545, 818, 42, 1103, 168, 719, 163, 1162, 626, 329, 330, 956, 1140, 217, 271, 444, 1003, 227, 253, 88, 334, 1281, 168, 898, 46, 118, 1187, 1191, 526, 346, 89, 137, 1226, 473, 368, 239, 622, 654, 853, 686, 992, 115, 275, 461, 1428, 614, 193, 1105, 398, 480, 775, 1482, 154, 454, 737, 419, 853, 1211, 314, 172, 527, 874, 674, 657, 475, 1209, 15, 293, 813, 173, 457, 533, 912, 798, 412, 130, 201, 299, 1277, 626, 378, 333, 805, 1901, 1573, 360, 20, 98, 703, 273, 919, 982, 879, 1287, 1109, 211, 1058, 532, 290, 890, 327, 1260, 570, 621, 328, 1307, 497, 219, 1163, 24, 468, 1272, 503, 1495, 1333, 267, 149, 1070, 206, 158, 48, 249, 935, 140, 184, 238, 800, 211, 670, 725, 81, 1211, 1362, 945, 1043, 444, 311, 812, 202, 531, 404, 954, 274, 956, 1438, 1446, 692, 777, 1157, 454, 478, 217, 468, 1107, 626, 485, 43, 322, 880, 1511, 667, 765, 614, 925, 1185, 1706, 347, 1603, 806, 1207, 431, 179, 423, 1131, 925, 62, 800, 211, 81, 667, 405, 678, 1619, 935, 864, 1495, 870, 1379, 1055, 220, 419, 1111, 1097, 447, 273, 590, 116, 306, 300, 1259, 1573, 1392, 113, 526, 1209, 655, 1123, 692, 796, 1021, 995, 1244, 293, 1240, 759, 398, 358, 403, 211, 194, 1076, 1238, 229, 327, 135, 5, 715, 511, 549, 420, 838, 314, 739, 62, 3, 963, 1438, 1621, 19, 887, 567, 584, 496, 1245, 682, 724, 1384, 257, 17, 983, 439, 925, 409, 293, 384, 149, 447, 717, 195, 153, 1699, 501, 530, 396, 670, 193, 351, 846, 111, 1213, 755, 1406, 497, 1826, 50, 817, 901, 212, 329, 837, 248, 632, 366, 174, 882, 688, 263, 118, 1012, 586, 294, 818, 426, 151, 409, 641, 958, 1328, 354, 1709, 1099, 783, 194, 238, 919, 758, 1039, 1054, 122, 100, 1221, 312, 599, 1274, 62, 315, 1123, 1878, 318, 574, 1353, 86, 340, 602, 963, 127, 142, 274, 371, 788, 395, 643, 854, 1335, 1541, 382, 846, 960, 154, 468, 1094, 272, 165, 394, 629, 831, 903, 923, 362, 693, 1250, 1135, 66, 924, 204, 376, 1083, 229, 875, 35, 18, 103, 105, 168, 18, 659, 534, 285, 482, 1571, 605, 158, 1573, 127, 343, 289, 1703, 1482, 907, 491, 287, 1151, 1024, 1085, 991, 1052, 1078, 98, 396, 323, 410, 1750, 1475, 1836, 283, 539, 440, 397, 204, 347, 1003, 0, 264, 543, 467, 875, 1116, 1325, 1313, 552, 335, 50, 593, 268, 382, 443, 576, 596, 115, 356, 10, 1399, 68, 643, 809, 683, 56, 492, 699, 290, 23, 394, 713, 1063, 1421, 623, 83, 1563, 777, 83, 203, 938, 1077, 773, 332, 570, 742, 1420, 1801, 1693, 31, 1098, 1765, 887, 760, 486, 2, 202, 90, 469, 86, 1180, 140, 1100, 194, 440, 1563, 163, 192, 837, 356, 155, 1032, 40, 1194, 1179, 902, 58, 1385, 614, 672, 1088, 499, 157, 485, 571, 431, 499, 563, 944, 1138, 161, 126, 15, 500, 203, 961, 608, 277, 114, 630, 404, 248, 225, 699, 649, 416, 1310, 930, 835, 664, 460, 778, 225, 785, 1581, 9, 7, 174, 437, 278, 1288, 979, 56, 913, 282, 322, 1252, 1128, 606, 655, 954, 68, 134, 922, 1874, 251, 1362, 281, 224, 778, 546, 397, 386, 214, 461, 125, 335, 342, 625, 494, 1145, 600, 484, 199, 21, 885, 308, 1015, 1303, 406, 124, 150, 108, 90, 588, 1028, 776, 278, 897, 651, 1010, 737, 1399, 376, 1117, 234, 691, 380, 603, 117, 766, 1499, 1175, 988, 190, 674, 518, 1618, 857, 371, 73, 98, 395, 1064, 712, 774, 1012, 730, 777, 973, 245, 937, 937, 574, 678, 82, 398, 1096, 1419, 157, 430, 1524, 1649, 254, 1790, 1438, 1304, 813, 948, 1161, 867, 181, 754, 242, 405, 1569, 105, 978, 725, 657, 1147, 910, 295, 26, 332, 519, 161, 1080, 649, 71, 744, 437, 115, 547, 495, 1466, 117, 760, 764, 776, 53, 371, 728, 1791, 606, 1225, 419, 367, 1230, 267, 273, 705, 214, 424, 84, 605, 1175, 657, 371, 833, 102, 1299, 258, 56, 443, 1279, 313, 523, 919, 570, 380, 119, 296, 358, 1364, 424, 961, 431, 950, 647, 1960, 116, 306, 808, 94, 380, 593, 1684, 1221, 410, 600, 189, 308, 148, 297, 1050, 1191, 1399, 1658, 1153, 35, 553, 46, 207, 67, 818, 1257, 138, 11, 889, 831, 713, 527, 1284, 911, 1047, 123, 406, 927, 407, 1041, 781, 1191, 103, 125, 919, 536, 638, 639, 496, 686, 1064, 240, 525, 348, 1262, 1899, 258, 1188, 730, 484, 547, 800, 347, 679, 1224, 948, 366, 223, 1619, 332, 167, 782, 881, 91, 487, 120, 1626, 74, 634, 334, 931, 921, 805, 1057, 616, 740, 542, 344, 211, 79, 887, 670, 383, 718, 144, 1442, 291, 1352, 209, 861, 426, 266, 523, 40, 262, 623, 4, 1001, 334, 453, 952, 321, 1451, 1048, 398, 417, 470, 1217, 298, 36, 85, 655, 167, 212, 617, 271, 159, 786, 46, 645, 1371, 881, 449, 338, 1332, 119, 256, 428, 1546, 1210, 789, 390, 110, 1330, 674, 253, 317, 183, 892, 388, 52, 914, 287, 773, 675, 367, 510, 1325, 1661, 665, 662, 0, 284, 661, 757, 560, 470, 146, 1315, 533, 560, 405, 227, 479, 227, 149, 88, 114, 662, 441, 888, 664, 64, 747, 956, 776, 652, 1168, 201, 724, 599, 3, 1652, 190, 1171, 1081, 1211, 976, 675, 455, 42, 1196, 503, 1694, 306, 838, 291, 173, 1017, 698, 610, 223, 206, 1457, 742, 591, 347, 241, 743, 4, 576, 888, 528, 1222, 915, 868, 336, 483]}, {\"axis\": {\"matches\": true}, \"label\": \"px_width\", \"values\": [756, 1988, 1716, 1786, 1212, 1654, 1018, 1149, 836, 1224, 874, 1005, 748, 1440, 563, 1864, 1850, 810, 878, 1064, 1854, 1385, 822, 788, 1245, 1434, 645, 1075, 1311, 1263, 1739, 654, 1152, 1919, 881, 1249, 815, 1307, 730, 1613, 967, 1932, 714, 1456, 1652, 1810, 1791, 774, 1433, 1854, 1832, 1383, 629, 1287, 804, 1036, 1449, 820, 713, 683, 969, 975, 547, 1353, 1570, 1618, 1858, 1781, 1225, 603, 1724, 1217, 1026, 638, 1413, 1242, 1071, 778, 1552, 1486, 1206, 1616, 1752, 1122, 1049, 1894, 741, 1118, 1203, 1402, 1924, 1242, 802, 1105, 1990, 1179, 891, 1684, 961, 1317, 1739, 881, 984, 520, 902, 1504, 1634, 1796, 1846, 1869, 1471, 874, 1974, 513, 1244, 1661, 973, 771, 989, 1954, 1338, 1712, 863, 1376, 1203, 902, 786, 1965, 1732, 1398, 829, 792, 580, 1056, 1264, 1248, 669, 1686, 1539, 1042, 1926, 616, 1131, 1697, 536, 786, 1269, 1298, 1967, 1025, 800, 1359, 1996, 1013, 590, 1925, 1989, 826, 1983, 1545, 1533, 621, 1272, 1729, 1071, 974, 1554, 1611, 1937, 1942, 845, 627, 842, 1737, 1199, 1740, 648, 1136, 1220, 879, 1065, 831, 1191, 1631, 588, 500, 1698, 1299, 898, 1381, 984, 1533, 1464, 539, 1179, 751, 1497, 1215, 1948, 1542, 1235, 1345, 589, 1182, 1052, 1429, 873, 836, 1478, 1460, 705, 1666, 1077, 1020, 1235, 1516, 1499, 583, 1958, 1995, 642, 1178, 1404, 1040, 1453, 1677, 517, 1560, 1226, 1200, 1647, 1308, 1615, 687, 637, 1606, 1543, 980, 1441, 1383, 1009, 1754, 860, 1543, 1108, 673, 1758, 886, 1485, 1038, 946, 1462, 654, 1604, 1882, 1816, 765, 1398, 1980, 1559, 1928, 1529, 952, 1517, 1741, 1596, 1564, 1222, 692, 1617, 1571, 1247, 1767, 714, 1935, 1933, 1509, 954, 662, 888, 527, 820, 1493, 1929, 764, 1092, 1891, 1699, 683, 1664, 763, 572, 1963, 1489, 1294, 997, 655, 1458, 1243, 519, 1767, 566, 515, 1168, 882, 1942, 776, 564, 1975, 1212, 1118, 1388, 1462, 1130, 754, 1726, 1876, 709, 557, 1914, 775, 1910, 1923, 1370, 994, 1078, 724, 1104, 1182, 1795, 1224, 1442, 1023, 1361, 652, 935, 1345, 1292, 1256, 1364, 1786, 1429, 682, 1199, 1688, 1898, 1438, 1463, 1584, 1059, 1253, 1442, 939, 986, 1029, 1280, 898, 1381, 1565, 1299, 1726, 1331, 1922, 1038, 992, 1186, 1920, 1866, 981, 1195, 1706, 749, 1711, 681, 1974, 1510, 814, 1025, 1247, 1814, 1233, 642, 721, 1670, 550, 699, 1138, 1903, 817, 1126, 1139, 926, 1506, 726, 1753, 794, 1161, 1350, 619, 660, 1468, 1705, 1476, 1666, 1026, 1033, 1529, 559, 973, 1223, 956, 1797, 882, 596, 574, 990, 1053, 858, 758, 1495, 1104, 1352, 1304, 1205, 562, 585, 1441, 1075, 1629, 1975, 1798, 545, 1657, 1341, 817, 1058, 1203, 1744, 740, 1133, 889, 1719, 1285, 1850, 1905, 1158, 1325, 772, 1743, 882, 1208, 1050, 1366, 899, 1224, 1131, 1884, 1370, 1497, 867, 681, 884, 1358, 998, 1989, 862, 1785, 799, 793, 695, 728, 1940, 1304, 1078, 1259, 790, 1594, 548, 1256, 1911, 1649, 1264, 1262, 1972, 1484, 1090, 1259, 1439, 592, 1777, 870, 1106, 948, 1393, 982, 1586, 1579, 644, 1157, 539, 750, 1472, 1337, 1663, 861, 773, 1313, 1849, 1613, 1259, 1151, 1807, 1763, 1029, 952, 654, 1827, 1916, 1924, 1580, 1942, 1582, 1234, 517, 1917, 1759, 939, 1463, 848, 1803, 915, 1354, 1143, 632, 899, 1642, 1450, 1463, 1013, 1587, 1665, 1815, 739, 963, 1508, 1000, 544, 1627, 759, 1416, 1088, 1596, 760, 857, 1100, 676, 658, 1239, 1901, 1813, 1578, 1878, 1864, 1109, 1171, 1949, 1722, 1920, 1809, 964, 1237, 1082, 828, 1241, 545, 1710, 1793, 1463, 1100, 1951, 1704, 1897, 809, 1306, 1771, 1536, 1509, 1545, 1987, 1022, 1337, 522, 1883, 1787, 910, 781, 1501, 1176, 1285, 583, 684, 1838, 904, 1552, 1854, 559, 1682, 1383, 655, 1715, 906, 637, 1230, 1315, 599, 1302, 727, 1787, 1096, 1173, 816, 1970, 1101, 1654, 1191, 1089, 1033, 1781, 901, 1701, 1308, 1232, 630, 1184, 1991, 1256, 1917, 510, 1511, 1813, 747, 1947, 935, 1421, 1652, 672, 1031, 527, 739, 1596, 575, 1364, 1478, 1282, 1353, 1435, 1651, 516, 1000, 1754, 1703, 1935, 1898, 1832, 721, 1048, 1873, 1465, 1477, 1748, 1383, 1159, 1630, 857, 1293, 1162, 1747, 736, 1418, 757, 1750, 549, 818, 974, 938, 1505, 1116, 991, 577, 1374, 1251, 1747, 1674, 800, 1016, 1327, 501, 1621, 858, 1532, 1294, 810, 811, 1318, 1655, 1702, 1013, 1883, 1862, 1301, 1605, 718, 699, 1742, 1558, 766, 1799, 1658, 1666, 1998, 1491, 1891, 1262, 650, 907, 1234, 519, 1595, 895, 1589, 1564, 1384, 1611, 1733, 1634, 922, 1291, 874, 1089, 1604, 1162, 1648, 910, 723, 999, 1300, 819, 1238, 643, 1633, 735, 1226, 1339, 1445, 1424, 883, 1452, 806, 849, 1812, 947, 1605, 1916, 1895, 1123, 1409, 1484, 1005, 1675, 1724, 1958, 1930, 1518, 1704, 589, 859, 821, 1671, 1865, 822, 1018, 829, 522, 1009, 1564, 1448, 896, 605, 1056, 1330, 670, 1909, 1185, 1008, 1642, 1668, 1482, 1229, 823, 1326, 932, 926, 1641, 660, 1051, 1889, 1982, 893, 1163, 589, 962, 697, 1312, 1862, 1469, 952, 704, 671, 1583, 809, 1151, 791, 1391, 1336, 1638, 690, 1073, 643, 1660, 1395, 783, 1208, 799, 1145, 1169, 563, 1360, 1423, 1152, 1577, 1181, 1228, 1523, 867, 774, 1513, 812, 580, 783, 662, 1081, 697, 1406, 1001, 1995, 1500, 530, 832, 1026, 672, 1267, 670, 928, 1897, 1382, 1011, 1929, 1076, 688, 1403, 734, 1234, 1159, 1968, 1165, 1481, 1569, 1554, 1876, 1793, 1367, 1125, 1913, 1203, 828, 1155, 618, 1545, 670, 563, 1293, 1084, 1723, 831, 1933, 1980, 1469, 1514, 937, 1119, 1556, 1263, 1819, 1638, 1435, 1394, 1197, 1108, 1956, 1026, 614, 516, 1334, 806, 1230, 687, 558, 507, 1670, 765, 1190, 1744, 1248, 714, 1973, 1670, 506, 736, 1019, 674, 1409, 1420, 774, 1377, 1517, 1663, 1538, 511, 541, 940, 1666, 1659, 1330, 983, 772, 1366, 1219, 507, 1735, 1291, 719, 1915, 917, 730, 1808, 1908, 1977, 1452, 1972, 930, 1190, 1968, 1165, 1877, 1997, 671, 1284, 974, 1759, 1465, 760, 661, 1304, 698, 1143, 1782, 1445, 1261, 1902, 1782, 1667, 1040, 1960, 1394, 1591, 1910, 890, 864, 1994, 823, 1552, 1463, 1910, 1392, 510, 1427, 1731, 875, 1348, 616, 1263, 635, 622, 1767, 653, 1716, 570, 1813, 1356, 1553, 1985, 1471, 705, 1208, 906, 663, 1916, 1976, 1761, 1520, 915, 1768, 1096, 631, 843, 519, 1152, 1810, 1682, 739, 1005, 676, 1595, 1623, 1178, 772, 1964, 1413, 1963, 1795, 1801, 1992, 1649, 571, 1746, 1903, 1055, 893, 877, 1088, 830, 1805, 541, 1781, 722, 646, 1309, 1758, 1079, 1087, 982, 1191, 1361, 1055, 1767, 1626, 1661, 521, 1247, 1564, 1515, 822, 1638, 932, 710, 1883, 1401, 1383, 1791, 1432, 1300, 1090, 1262, 1637, 1068, 755, 874, 1531, 1348, 656, 1434, 1292, 1983, 1623, 869, 1555, 1827, 772, 1764, 1046, 594, 1896, 1320, 1687, 1069, 1170, 1595, 1347, 1324, 966, 1538, 846, 1389, 574, 607, 1636, 709, 882, 1781, 1305, 1433, 1352, 1966, 1178, 1500, 1212, 989, 1602, 1263, 728, 1607, 1490, 550, 1762, 1455, 556, 1737, 1229, 1606, 1239, 734, 1264, 864, 1014, 500, 1411, 525, 1967, 1377, 1843, 652, 797, 1043, 1418, 565, 887, 656, 874, 1352, 1195, 751, 639, 1804, 1920, 1581, 912, 745, 511, 1011, 1278, 1344, 1291, 896, 1892, 1247, 684, 1109, 1247, 773, 1019, 645, 1788, 1422, 1558, 1858, 1767, 618, 1037, 1554, 759, 571, 1469, 551, 1579, 1838, 1841, 876, 1079, 620, 747, 1012, 522, 1499, 768, 740, 807, 1089, 507, 804, 1335, 1544, 1378, 1378, 1923, 1702, 739, 1796, 1456, 1884, 707, 898, 1609, 603, 1723, 1593, 1988, 1057, 1032, 1951, 1473, 1477, 1106, 741, 1656, 1134, 922, 627, 1385, 1145, 1756, 711, 1899, 679, 1252, 1387, 1886, 1477, 1930, 860, 1539, 1144, 613, 909, 1743, 1469, 544, 873, 947, 1389, 798, 519, 1676, 1651, 1147, 1011, 1688, 1613, 1469, 1738, 1838, 1261, 1362, 1461, 819, 1062, 997, 598, 564, 1316, 1746, 1694, 1783, 1524, 1728, 1678, 828, 1622, 979, 1052, 1958, 1310, 1504, 970, 1931, 1858, 1528, 791, 1614, 1409, 1437, 1472, 1418, 1004, 1683, 1892, 1744, 1163, 621, 1383, 784, 1066, 1400, 1766, 846, 1662, 1520, 1923, 1773, 501, 1274, 986, 676, 974, 1309, 888, 794, 1454, 1654, 1084, 1210, 1306, 1206, 603, 952, 1878, 1285, 568, 1206, 533, 696, 1850, 640, 1496, 1378, 1552, 887, 1769, 855, 1167, 1386, 1284, 1517, 627, 1836, 1356, 941, 1793, 1269, 1039, 1182, 884, 1162, 613, 518, 1195, 1591, 1345, 764, 1226, 1552, 1233, 1827, 865, 1133, 644, 1156, 967, 1970, 530, 1781, 1553, 1227, 1393, 1893, 1076, 891, 1273, 1393, 686, 1708, 1804, 1376, 1073, 1340, 581, 1796, 1709, 1981, 831, 1267, 1915, 1681, 1728, 1156, 1062, 507, 1039, 1079, 682, 1007, 1027, 1527, 935, 1873, 1619, 1163, 1144, 1925, 941, 694, 1496, 870, 1103, 1301, 1299, 1713, 910, 1759, 1250, 850, 1435, 1923, 1448, 1877, 1132, 1957, 1492, 1473, 1316, 991, 903, 646, 588, 540, 1004, 730, 855, 512, 1098, 1717, 1193, 1657, 1639, 620, 1431, 724, 1880, 1591, 925, 1506, 627, 1723, 1363, 1134, 1803, 1897, 1564, 977, 1648, 978, 1551, 1857, 1988, 1873, 676, 1240, 563, 800, 1021, 727, 1284, 1987, 519, 703, 675, 925, 1724, 1945, 1331, 1802, 679, 1105, 1327, 1010, 1364, 892, 884, 679, 636, 1407, 1567, 1684, 584, 790, 1988, 1349, 527, 818, 1492, 804, 826, 849, 1742, 1205, 1463, 816, 816, 1692, 1995, 559, 915, 1226, 1183, 881, 866, 1645, 821, 1539, 1923, 1790, 1536, 1373, 1786, 1775, 1964, 1032, 660, 1787, 1614, 853, 1546, 1350, 1046, 1437, 892, 569, 1803, 1011, 1311, 1405, 588, 663, 1776, 1435, 1727, 1262, 969, 963, 1780, 1617, 1088, 1718, 695, 763, 1399, 817, 542, 1090, 1055, 1003, 1491, 647, 1575, 1196, 1713, 1771, 1950, 1970, 1155, 1849, 738, 1742, 755, 1234, 851, 895, 1658, 1420, 1983, 1175, 1806, 1583, 1615, 1545, 1118, 1713, 1944, 1294, 1175, 638, 623, 1372, 1190, 1931, 1429, 710, 710, 1963, 1702, 688, 1459, 970, 1042, 1197, 1836, 1976, 599, 1853, 1159, 1994, 844, 1424, 891, 1046, 1970, 833, 675, 1009, 1641, 761, 509, 1487, 1042, 711, 679, 832, 1811, 1208, 1947, 1814, 1036, 833, 1677, 1963, 557, 1809, 1196, 1247, 584, 1103, 700, 1684, 1759, 1646, 1288, 1330, 1947, 1028, 1550, 1483, 681, 877, 1952, 1578, 1503, 1905, 1375, 1374, 1916, 989, 665, 583, 755, 1853, 1540, 1726, 939, 1040, 1148, 1090, 1633, 829, 1083, 1164, 1171, 1630, 512, 1775, 1570, 1698, 770, 1207, 1848, 1829, 954, 1988, 1832, 1673, 1180, 1007, 1288, 1258, 1199, 1532, 1127, 742, 1763, 1099, 1986, 1882, 1657, 1393, 1566, 503, 508, 764, 533, 1029, 1289, 1290, 1686, 1244, 1113, 1111, 957, 1035, 1717, 1115, 1722, 1873, 1005, 1504, 1702, 818, 1896, 1316, 1717, 736, 1264, 1263, 1161, 891, 1905, 679, 1032, 793, 1429, 1586, 938, 1023, 1441, 1195, 1935, 1889, 1150, 708, 1793, 712, 565, 1482, 1021, 682, 529, 839, 614, 1851, 1639, 1030, 1727, 1739, 1849, 1963, 1432, 538, 1517, 1334, 877, 732, 1746, 1415, 1643, 1211, 1411, 591, 742, 918, 1726, 1882, 1663, 1794, 1817, 510, 874, 908, 1177, 1924, 1006, 1411, 1371, 1020, 1635, 1709, 1854, 1393, 1643, 1220, 1534, 1477, 681, 1135, 528, 1430, 1364, 1839, 1760, 1227, 1343, 1656, 1831, 1087, 1804, 1418, 1985, 1414, 704, 730, 1520, 1904, 789, 1948, 751, 1238, 1852, 1455, 513, 1231, 1676, 1205, 1234, 1777, 1962, 970, 909, 951, 1129, 1317, 722, 1429, 1913, 865, 1079, 1079, 1811, 1351, 956, 1931, 912, 840, 678, 1551, 1608, 615, 893, 1152, 1249, 1109, 896, 1904, 1318, 1775, 725, 1888, 1629, 642, 542, 1311, 1436, 769, 1351, 1176, 896, 963, 1726, 1870, 1469, 1123, 724, 520, 844, 1670, 1037, 642, 1451, 1255, 990, 623, 1386, 1769, 1738, 872, 562, 1275, 1677, 1382, 790, 534, 1814, 1531, 823, 1254, 1879, 1989, 1211, 756, 1317, 1686, 1455, 590, 1805, 661, 1603, 605, 539, 979, 593, 1353, 1163, 1985, 980, 1800, 1836, 718, 997, 994, 1036, 1951, 1912, 1177, 775, 822, 1710, 1696, 1633, 1141, 610, 831, 509, 1022, 709, 819, 874, 1721, 1466, 711, 745, 1247, 1010, 1397, 1933, 1179, 582, 1127, 1299, 629, 1983, 657, 1673, 1979, 1396, 1353, 742, 537, 807, 1651, 986, 1798, 558, 885, 651, 1219, 1289, 1018, 1437, 737, 1167, 1919, 999, 724, 957, 854, 1426, 743, 1809, 1099, 1416, 1890, 1965, 1632, 670, 754]}, {\"axis\": {\"matches\": true}, \"label\": \"ram\", \"values\": [2549, 2631, 2603, 2769, 1411, 1067, 3220, 700, 1099, 513, 3946, 3826, 1482, 2680, 373, 568, 3554, 3752, 1835, 2337, 2819, 3283, 1433, 1037, 2583, 2782, 3763, 3286, 2373, 478, 3532, 508, 2227, 3845, 1262, 1326, 2113, 3429, 3169, 2150, 2484, 3339, 1878, 1629, 504, 1152, 3587, 2296, 1270, 3238, 2059, 2053, 3112, 1440, 2908, 2552, 905, 3963, 2056, 2910, 1457, 431, 470, 2148, 2955, 3366, 3068, 3834, 1050, 3993, 3378, 2192, 392, 3709, 590, 1814, 907, 1449, 3448, 1837, 3464, 2593, 3484, 2009, 2048, 837, 854, 3210, 2746, 2334, 2822, 1971, 1410, 349, 1418, 3616, 880, 1601, 1412, 1692, 2600, 1308, 2413, 1047, 1204, 1799, 2676, 2597, 3029, 2343, 2016, 2213, 790, 3182, 3472, 2871, 3187, 1945, 488, 1214, 2598, 3242, 3534, 3426, 2488, 3255, 2863, 1496, 3801, 666, 3799, 1366, 2962, 2399, 2338, 2700, 2235, 3825, 284, 1427, 1324, 398, 3685, 2390, 278, 3556, 2196, 3799, 2268, 2965, 2341, 3557, 1354, 2147, 505, 1394, 374, 506, 3702, 1078, 2945, 2981, 490, 536, 2317, 3704, 2126, 2050, 2462, 1260, 3770, 1886, 3323, 2060, 3926, 2177, 728, 2478, 2842, 1617, 1472, 764, 2337, 3965, 3693, 797, 1687, 2940, 1545, 3834, 2505, 2243, 3600, 3971, 2844, 2227, 1665, 3355, 1866, 1287, 1046, 1441, 690, 2504, 2677, 2243, 2984, 1362, 3629, 3559, 2571, 2107, 1022, 2844, 711, 3117, 978, 1869, 1179, 3844, 593, 588, 3836, 2669, 2958, 3031, 3388, 2377, 2458, 2766, 3054, 3791, 625, 2227, 3256, 707, 1457, 3755, 1733, 720, 3560, 2311, 3846, 1717, 3704, 3800, 278, 1767, 1846, 829, 2986, 2394, 2169, 1181, 575, 2977, 532, 1702, 1074, 3352, 1027, 2039, 461, 527, 3271, 606, 2513, 1675, 2698, 1129, 468, 1210, 3086, 1595, 2332, 2589, 3760, 1851, 417, 294, 3576, 3945, 927, 2044, 2334, 1477, 3161, 3660, 1604, 2211, 3822, 3922, 1203, 2822, 447, 1652, 3608, 1228, 666, 1499, 3321, 1206, 952, 3927, 311, 955, 1109, 2969, 725, 1655, 3490, 1774, 3615, 714, 2768, 3739, 610, 715, 663, 1725, 834, 3865, 3059, 651, 796, 785, 3252, 1322, 2832, 1615, 2125, 990, 1595, 1213, 2500, 2801, 3451, 1201, 3139, 1938, 3185, 2111, 2369, 3625, 1051, 1036, 2938, 2855, 2727, 2973, 2581, 2690, 916, 2563, 3359, 941, 2280, 827, 473, 3865, 1274, 364, 3699, 2927, 1470, 1050, 2616, 2915, 2686, 1243, 3264, 2261, 2993, 706, 2542, 2523, 336, 2457, 418, 3917, 1246, 2971, 2324, 3121, 3801, 3333, 3475, 2360, 2895, 3652, 3097, 2101, 3918, 3869, 732, 3684, 3969, 1414, 1892, 3131, 2532, 3215, 3323, 1529, 2403, 3419, 3648, 1974, 2806, 2239, 582, 3890, 1141, 2511, 3838, 2734, 2330, 2244, 1851, 808, 1587, 2492, 2122, 1122, 3709, 3021, 624, 3937, 1797, 1458, 3348, 1834, 2775, 1277, 457, 438, 3210, 985, 3701, 1052, 1175, 1070, 3411, 1175, 301, 489, 1733, 2756, 2612, 1724, 2190, 1900, 3915, 1043, 3817, 2362, 1107, 2982, 2042, 1343, 2610, 2335, 1614, 1138, 604, 3652, 3955, 258, 1066, 1731, 3860, 3630, 1965, 1459, 2323, 3291, 3272, 918, 3488, 3961, 590, 3701, 463, 3644, 1713, 429, 2765, 2084, 1125, 2048, 1223, 3672, 3100, 343, 1970, 2728, 3635, 2180, 2488, 3864, 1155, 3566, 864, 869, 591, 1333, 2208, 2610, 2819, 1955, 3416, 3803, 574, 2129, 1948, 2978, 1619, 3716, 3454, 3777, 2712, 1434, 3968, 3383, 3278, 3970, 2674, 2776, 2336, 2430, 891, 728, 3886, 1711, 2856, 2447, 814, 2951, 3653, 1891, 701, 3139, 2811, 1701, 1853, 2445, 2287, 2003, 3595, 1726, 3952, 850, 1446, 1300, 719, 988, 2606, 2912, 2339, 1028, 3173, 1667, 1897, 1214, 3269, 445, 1073, 665, 3129, 3501, 3461, 3206, 3717, 3917, 861, 2885, 2777, 1017, 980, 2775, 3309, 3607, 1257, 759, 3720, 475, 1122, 1336, 1185, 705, 3169, 3483, 1183, 3373, 2801, 565, 3153, 3421, 316, 448, 2366, 3881, 462, 1480, 1087, 2711, 2144, 1165, 2998, 2249, 752, 1409, 3072, 1305, 3597, 3132, 3856, 1524, 3673, 3293, 3035, 584, 2675, 2382, 1391, 2346, 606, 1324, 1076, 643, 783, 3809, 2104, 1762, 3137, 1756, 3663, 2437, 1432, 1620, 1655, 2832, 961, 2608, 514, 2304, 933, 1896, 3655, 1973, 3278, 2183, 824, 2598, 1254, 3622, 1704, 1624, 3809, 2574, 950, 273, 446, 1510, 1122, 707, 751, 3393, 3771, 1851, 2735, 2253, 452, 1419, 969, 3441, 3786, 2710, 3533, 1735, 3587, 2298, 2027, 2406, 2419, 995, 2078, 1145, 626, 1509, 3371, 297, 3508, 2385, 1301, 2167, 3914, 2312, 2297, 411, 665, 3226, 740, 3878, 3406, 2440, 3959, 3703, 3714, 337, 3176, 1205, 2473, 1229, 2896, 2039, 520, 1378, 3254, 1803, 2501, 3762, 1796, 990, 2630, 1380, 3487, 1201, 3015, 3204, 433, 1906, 898, 2799, 2020, 770, 305, 2953, 1671, 2574, 3772, 2870, 2577, 2246, 2405, 3377, 3619, 1184, 2965, 3713, 3798, 1026, 1086, 2236, 1303, 3048, 616, 3764, 419, 2858, 504, 392, 820, 3387, 3447, 1403, 2295, 1973, 3755, 2073, 3872, 302, 1519, 2549, 404, 595, 1663, 2764, 2614, 1094, 2826, 2496, 2103, 966, 2172, 905, 3916, 3902, 2487, 3914, 363, 3233, 2532, 3836, 3076, 1513, 1724, 542, 3481, 3991, 1790, 509, 282, 3006, 1913, 1333, 3746, 3941, 3970, 3925, 424, 1284, 277, 1713, 485, 3002, 418, 1652, 2893, 1406, 629, 3724, 3835, 1354, 2376, 3438, 2655, 3361, 3535, 2150, 2587, 2736, 676, 2941, 3336, 3796, 3654, 2934, 2889, 3315, 1993, 2509, 3534, 2246, 3165, 286, 776, 3213, 716, 424, 3486, 3237, 1944, 643, 2317, 1436, 3190, 2338, 1334, 3657, 2080, 1609, 3078, 590, 2552, 3355, 1968, 3358, 1539, 942, 3669, 3612, 2351, 604, 1285, 515, 654, 2540, 3869, 3412, 509, 2547, 2735, 1369, 2942, 2812, 3497, 619, 3483, 1475, 1950, 3592, 3573, 2020, 1329, 3568, 1947, 1998, 725, 3302, 3884, 1185, 1656, 2870, 911, 3892, 3897, 656, 606, 1653, 860, 2197, 3458, 2066, 3676, 1518, 1018, 1360, 3449, 2524, 703, 1251, 582, 1732, 1252, 2146, 1927, 3328, 2462, 1172, 3598, 2437, 3271, 1511, 2981, 3119, 3647, 3148, 2829, 2373, 3998, 316, 999, 1808, 589, 3317, 670, 584, 2173, 1561, 2073, 1401, 3764, 1275, 1464, 2885, 3458, 3019, 1083, 3538, 2493, 3726, 1531, 601, 2991, 3912, 309, 323, 1489, 1322, 1846, 417, 3269, 3731, 667, 1241, 3941, 1018, 3142, 1444, 934, 3863, 3506, 1944, 348, 550, 971, 3984, 2973, 2019, 1587, 1870, 1352, 570, 546, 3624, 2944, 1209, 2752, 3868, 3087, 947, 1484, 347, 3227, 3299, 1028, 714, 2641, 1229, 2622, 929, 1211, 1653, 511, 3034, 1414, 2438, 3744, 1377, 3520, 3796, 1699, 336, 1095, 325, 1223, 1649, 1898, 2609, 2265, 2775, 1445, 2614, 356, 523, 3745, 1229, 1402, 2085, 587, 2457, 2030, 816, 1667, 2958, 3705, 2800, 3773, 2050, 1754, 1904, 2705, 3537, 1370, 1222, 3102, 2268, 2648, 2173, 3162, 1368, 2746, 619, 724, 912, 2784, 793, 560, 312, 545, 1591, 2746, 893, 2514, 1032, 819, 3672, 3833, 1482, 2043, 1229, 1816, 2166, 2469, 2575, 1375, 1732, 673, 2301, 2676, 1781, 2248, 921, 391, 1719, 3262, 1947, 2693, 441, 1886, 1267, 687, 2890, 1737, 887, 398, 2389, 3431, 3601, 2459, 3230, 3646, 2200, 3862, 3077, 621, 2372, 2574, 2857, 3143, 3197, 2727, 1519, 2438, 2674, 2107, 2262, 815, 2829, 3022, 2573, 2678, 1282, 568, 2074, 2227, 1080, 2867, 1075, 3063, 2479, 2394, 2814, 2094, 2315, 2519, 735, 1891, 1386, 3260, 2022, 1716, 3317, 1454, 2290, 3940, 461, 2219, 470, 2495, 2944, 2361, 1783, 2832, 931, 1344, 3916, 1882, 2865, 757, 3469, 2399, 3300, 2719, 3012, 2836, 2385, 594, 2968, 3762, 892, 467, 345, 3033, 2110, 3105, 874, 3681, 3865, 737, 3153, 2800, 315, 1412, 2286, 2392, 1857, 1419, 3305, 959, 2635, 1754, 2563, 1936, 1869, 3637, 896, 2349, 774, 3397, 485, 3964, 1887, 2977, 2114, 1542, 3038, 2481, 841, 3565, 1958, 2454, 3392, 262, 530, 3703, 2562, 574, 2632, 259, 688, 2554, 3859, 2977, 571, 402, 878, 2130, 3260, 454, 2638, 2819, 1464, 1824, 876, 2706, 659, 455, 2378, 2278, 3436, 3424, 401, 681, 3212, 3105, 2036, 3153, 1642, 509, 3465, 3900, 1105, 2064, 3117, 1142, 1060, 2381, 3366, 1339, 2560, 285, 2476, 1862, 3488, 3178, 733, 756, 3622, 1277, 3703, 1905, 2636, 2014, 3056, 465, 3869, 1108, 1906, 851, 3142, 1300, 1430, 362, 2456, 2360, 3700, 2620, 437, 3774, 1218, 696, 3407, 3902, 2086, 1356, 1704, 3571, 2132, 3209, 2096, 1273, 3623, 368, 854, 3646, 3130, 3784, 3885, 1494, 2973, 1471, 2001, 1486, 1321, 1877, 3499, 2124, 1221, 2316, 1068, 1930, 1316, 1742, 3265, 2700, 2115, 3915, 3144, 1571, 2253, 1433, 2583, 1817, 2915, 3340, 2790, 577, 3220, 3488, 1277, 3593, 2736, 2311, 1464, 1355, 2382, 1886, 832, 751, 436, 722, 1075, 2802, 1780, 1247, 3066, 403, 2518, 1069, 2651, 2974, 657, 3372, 1464, 1326, 3393, 323, 1628, 2610, 3721, 763, 792, 1568, 1276, 3472, 2520, 3423, 258, 1868, 2171, 780, 712, 1012, 318, 3948, 629, 1161, 1403, 3707, 1305, 2926, 2674, 471, 1724, 1220, 2754, 594, 1005, 2439, 2302, 1083, 2219, 1571, 2107, 593, 857, 3518, 1284, 545, 3566, 2821, 3442, 1309, 1179, 1713, 1663, 1812, 1384, 1348, 3957, 1212, 3925, 348, 1382, 980, 838, 378, 909, 3632, 3396, 1861, 348, 2528, 2190, 663, 961, 1280, 2189, 531, 2331, 794, 2644, 1666, 2157, 1196, 2072, 1590, 2916, 3707, 726, 3577, 2627, 685, 1693, 2137, 3696, 3771, 1342, 1379, 1164, 908, 1019, 3685, 1470, 2299, 999, 3011, 3142, 1543, 3692, 2753, 2156, 2637, 331, 1214, 2248, 3073, 422, 2293, 3958, 432, 1152, 1303, 797, 298, 3847, 2110, 1921, 2423, 1658, 2361, 1380, 3892, 315, 2289, 2182, 2184, 2872, 2367, 1919, 3296, 462, 1591, 1593, 2456, 2933, 3154, 3615, 3563, 1440, 1907, 324, 1404, 2738, 2722, 2294, 1424, 1133, 555, 3785, 3675, 435, 3183, 3495, 1853, 2648, 2647, 702, 1836, 3894, 2944, 3424, 2678, 2424, 2671, 3078, 2304, 1146, 2335, 1422, 2096, 1303, 1115, 456, 3396, 2750, 941, 2278, 2273, 2355, 2328, 2638, 558, 1345, 425, 2192, 3397, 1208, 267, 3518, 3122, 1050, 3358, 2929, 624, 1598, 2052, 1249, 412, 595, 263, 2577, 639, 3206, 1550, 1646, 1930, 3695, 3235, 2334, 1998, 2258, 1869, 1825, 3946, 436, 2678, 3887, 1813, 817, 3614, 2013, 543, 3400, 2623, 2700, 1400, 3127, 2803, 2518, 3564, 2515, 3654, 3930, 3476, 3242, 2666, 1695, 3510, 3438, 2033, 1017, 1686, 862, 957, 3494, 3083, 1656, 313, 3282, 3629, 1687, 1149, 3899, 1783, 1629, 2528, 3256, 2341, 2785, 3761, 1513, 2240, 820, 1875, 920, 3615, 1412, 361, 1540, 1077, 3610, 512, 3024, 1610, 2201, 3338, 3473, 2203, 940, 1155, 3872, 1882, 1798, 2598, 1567, 1491, 2003, 3905, 291, 1693, 1788, 3322, 1751, 1641, 2854, 3115, 3259, 1612, 2319, 1258, 1543, 3991, 3555, 3411, 524, 3541, 1998, 3437, 1637, 3064, 2855, 1393, 1633, 2392, 3284, 464, 1234, 292, 2521, 2658, 587, 2190, 879, 1743, 1181, 3330, 3564, 562, 1659, 1816, 1338, 2992, 315, 3249, 3242, 2725, 586, 2454, 804, 2460, 1295, 2625, 1999, 2698, 3742, 1193, 3511, 885, 1044, 2548, 2940, 3966, 3484, 1246, 2757, 2378, 1794, 3933, 1637, 783, 3094, 1774, 256, 1365, 2339, 2049, 3241, 867, 2175, 3497, 609, 3285, 3684, 3586, 2322, 354, 3204, 1400, 2308, 1082, 3104, 2847, 2256, 1113, 3210, 3990, 984, 3117, 1622, 2967, 1037, 2908, 1167, 2715, 1734, 2259, 2921, 1244, 3132, 2488, 1052, 2927, 2090, 2131, 3362, 1053, 1799, 3124, 1672, 1552, 1938, 3202, 1302, 2296, 575, 2359, 3314, 440, 2002, 3167, 3480, 595, 2466, 1337, 1462, 503, 2066, 708, 2029, 308, 552, 2082, 3314, 3996, 1795, 594, 2473, 690, 3845, 2004, 2083, 2661, 1903, 2466, 3779, 3904, 265, 1417, 1769, 2948, 2572, 3267, 1459, 1633, 2052, 1086, 2402, 3376, 1507, 641, 3451, 1607, 3521, 1241, 881, 2563, 3033, 769, 3454, 819, 435, 586, 650, 3038, 3736, 2948, 739, 686, 841, 1183, 3943, 1037, 2633, 3911, 1958, 3297, 3141, 3351, 2355, 1308, 991, 3248, 1519, 493, 1747, 2517, 1389, 2522, 1885, 704, 953, 3206, 928, 2177, 2610, 489, 3839, 1861, 299, 340, 3606, 799, 2367, 445, 3927, 3756, 770, 1641, 2343, 2787, 2495, 3208, 1234, 1069, 3568, 3585, 1974, 2419, 340, 2282, 298, 968, 2391, 1534, 696, 2782, 755, 3746, 2651, 1341, 2651, 1824, 1905, 1456, 3155, 773, 1829, 3883, 1675, 1601, 1958, 2376, 1647, 3548, 2694, 2195, 2506, 2073, 3767, 1150, 841, 1675, 1301, 1817, 2321, 1974, 3433, 1205, 872, 2052, 3654, 1503, 3104, 343, 3430, 2470, 1396, 2668, 1457, 2452, 1871, 1112, 967, 2498, 387, 635, 2711, 2023, 2215, 824, 3851, 2156, 2885, 495, 2358, 1744, 1832, 2016, 1300, 2313, 3248, 2216, 3142, 1850, 1424, 1620, 2592, 296, 3579, 1180, 3962, 3978, 668, 2032, 3057, 869, 3919]}, {\"axis\": {\"matches\": true}, \"label\": \"sc_h\", \"values\": [9, 17, 11, 16, 8, 17, 13, 16, 17, 19, 5, 14, 18, 7, 14, 17, 10, 10, 19, 11, 17, 17, 11, 8, 11, 18, 16, 17, 10, 12, 17, 5, 18, 7, 12, 10, 13, 6, 6, 18, 18, 18, 15, 15, 9, 8, 10, 16, 15, 16, 5, 19, 12, 17, 6, 14, 14, 9, 7, 15, 14, 15, 7, 14, 10, 18, 9, 16, 11, 7, 13, 9, 15, 11, 9, 5, 10, 11, 17, 8, 19, 14, 9, 10, 11, 15, 16, 13, 9, 10, 19, 18, 16, 16, 19, 13, 19, 18, 14, 6, 16, 16, 17, 14, 11, 5, 12, 18, 19, 19, 16, 18, 13, 8, 9, 12, 12, 15, 5, 5, 12, 18, 10, 5, 13, 19, 9, 6, 12, 12, 17, 7, 12, 10, 12, 12, 19, 12, 12, 9, 17, 17, 18, 12, 19, 18, 12, 16, 13, 13, 10, 16, 16, 19, 12, 16, 18, 10, 17, 8, 10, 15, 7, 17, 16, 17, 18, 7, 12, 9, 5, 17, 9, 19, 16, 7, 14, 14, 16, 17, 14, 19, 6, 16, 17, 13, 11, 13, 9, 17, 15, 12, 17, 18, 7, 18, 17, 15, 11, 13, 5, 7, 14, 6, 19, 11, 13, 5, 19, 18, 11, 15, 11, 14, 9, 18, 5, 13, 16, 11, 19, 6, 12, 12, 14, 11, 6, 18, 19, 7, 14, 11, 17, 6, 9, 19, 18, 7, 9, 12, 11, 7, 10, 7, 8, 8, 16, 18, 12, 9, 7, 17, 6, 13, 15, 6, 12, 17, 14, 9, 6, 5, 7, 10, 14, 9, 10, 18, 9, 12, 19, 19, 17, 8, 17, 19, 9, 8, 8, 13, 17, 11, 19, 15, 11, 12, 8, 7, 14, 7, 14, 11, 14, 15, 15, 5, 15, 8, 19, 12, 5, 7, 19, 5, 18, 18, 13, 8, 11, 7, 16, 12, 13, 18, 19, 14, 8, 18, 13, 14, 16, 12, 6, 9, 13, 10, 16, 15, 7, 11, 17, 6, 14, 5, 17, 18, 15, 14, 14, 15, 8, 18, 7, 17, 13, 17, 6, 5, 13, 12, 15, 18, 13, 19, 10, 8, 17, 15, 11, 10, 18, 11, 17, 14, 13, 6, 18, 8, 11, 19, 16, 7, 5, 12, 10, 17, 13, 11, 9, 13, 8, 12, 10, 18, 10, 17, 19, 17, 7, 16, 5, 10, 9, 19, 14, 9, 12, 12, 10, 8, 10, 18, 13, 13, 9, 7, 9, 12, 7, 13, 15, 17, 5, 6, 9, 10, 19, 14, 15, 11, 6, 16, 15, 12, 14, 12, 9, 8, 19, 5, 18, 5, 13, 13, 17, 17, 15, 12, 15, 8, 16, 8, 12, 12, 13, 15, 5, 11, 8, 5, 16, 16, 9, 6, 8, 17, 7, 14, 17, 5, 12, 15, 17, 9, 16, 14, 19, 11, 11, 9, 7, 9, 6, 6, 9, 11, 18, 9, 16, 16, 11, 15, 14, 15, 16, 9, 13, 7, 17, 6, 8, 5, 17, 11, 16, 6, 7, 7, 6, 13, 7, 17, 9, 19, 9, 15, 12, 18, 19, 9, 14, 7, 5, 13, 13, 18, 7, 15, 13, 7, 7, 5, 16, 16, 8, 16, 14, 19, 10, 7, 18, 13, 14, 7, 14, 16, 11, 7, 7, 18, 17, 6, 7, 11, 17, 6, 15, 8, 7, 16, 8, 5, 17, 8, 14, 11, 13, 11, 19, 14, 9, 17, 6, 9, 8, 15, 6, 18, 17, 13, 6, 12, 11, 15, 7, 19, 9, 12, 13, 14, 15, 12, 16, 9, 15, 7, 9, 8, 11, 11, 14, 10, 14, 11, 6, 18, 9, 16, 15, 18, 16, 14, 6, 6, 17, 8, 13, 19, 5, 14, 17, 16, 7, 12, 14, 8, 18, 15, 12, 8, 12, 17, 12, 9, 19, 13, 6, 7, 16, 17, 10, 17, 7, 5, 12, 6, 14, 14, 12, 17, 18, 5, 16, 6, 7, 7, 14, 14, 7, 12, 6, 8, 15, 13, 15, 7, 17, 15, 17, 10, 17, 17, 16, 12, 12, 19, 7, 12, 15, 15, 7, 17, 13, 12, 15, 19, 17, 13, 16, 7, 14, 17, 11, 13, 8, 7, 5, 19, 10, 15, 13, 13, 13, 16, 13, 11, 10, 9, 14, 8, 10, 6, 11, 19, 5, 9, 10, 17, 19, 16, 6, 18, 16, 5, 12, 11, 10, 18, 12, 5, 16, 15, 16, 14, 18, 10, 15, 12, 6, 18, 10, 17, 16, 5, 6, 12, 11, 9, 6, 12, 17, 16, 9, 8, 15, 5, 14, 6, 15, 8, 19, 13, 18, 8, 12, 10, 15, 10, 7, 6, 14, 10, 12, 11, 14, 15, 15, 10, 17, 7, 9, 18, 18, 10, 18, 18, 14, 9, 7, 10, 18, 13, 19, 5, 17, 19, 8, 11, 13, 17, 9, 13, 16, 15, 7, 11, 11, 11, 10, 15, 19, 19, 10, 6, 12, 18, 10, 6, 13, 7, 5, 17, 11, 10, 12, 6, 11, 17, 8, 14, 5, 16, 15, 15, 5, 17, 12, 16, 17, 9, 7, 9, 15, 13, 6, 10, 18, 8, 5, 17, 9, 17, 18, 14, 5, 14, 7, 15, 11, 10, 12, 19, 16, 14, 6, 16, 5, 16, 18, 17, 16, 16, 10, 13, 19, 12, 12, 7, 19, 16, 9, 13, 8, 10, 5, 5, 16, 17, 17, 13, 12, 10, 16, 9, 11, 10, 9, 5, 17, 11, 11, 7, 17, 6, 6, 6, 5, 12, 11, 11, 17, 15, 5, 19, 14, 18, 12, 14, 7, 11, 14, 16, 7, 12, 12, 16, 17, 15, 5, 13, 10, 13, 6, 8, 6, 11, 17, 10, 15, 12, 9, 19, 14, 17, 7, 9, 18, 18, 11, 11, 6, 19, 16, 12, 18, 8, 16, 11, 12, 10, 16, 19, 14, 13, 17, 13, 9, 11, 6, 18, 14, 8, 12, 7, 8, 11, 8, 9, 10, 9, 7, 8, 19, 17, 17, 12, 19, 17, 19, 7, 16, 17, 14, 9, 14, 12, 11, 17, 17, 10, 9, 17, 5, 7, 12, 12, 8, 19, 15, 5, 16, 11, 9, 17, 12, 13, 8, 18, 8, 12, 16, 13, 15, 7, 9, 13, 6, 13, 11, 5, 7, 17, 17, 14, 14, 9, 19, 6, 12, 12, 5, 9, 17, 5, 5, 5, 17, 6, 14, 13, 8, 19, 7, 16, 10, 17, 6, 11, 11, 7, 17, 12, 17, 16, 16, 13, 6, 7, 7, 7, 19, 14, 10, 7, 17, 16, 12, 7, 9, 5, 12, 19, 15, 9, 16, 10, 8, 15, 14, 15, 7, 16, 17, 10, 18, 13, 15, 6, 10, 15, 17, 18, 16, 14, 5, 19, 16, 15, 8, 15, 19, 17, 6, 17, 8, 18, 16, 7, 15, 14, 14, 6, 15, 18, 17, 17, 11, 14, 14, 17, 17, 14, 15, 9, 14, 9, 13, 16, 16, 13, 13, 16, 16, 11, 12, 8, 9, 10, 5, 12, 17, 9, 19, 13, 8, 10, 13, 7, 19, 7, 14, 14, 7, 7, 11, 13, 17, 17, 15, 6, 10, 8, 16, 17, 13, 11, 17, 13, 14, 11, 12, 5, 18, 12, 19, 14, 6, 11, 12, 9, 12, 15, 17, 10, 9, 9, 14, 15, 19, 8, 14, 8, 8, 14, 13, 19, 17, 8, 10, 12, 17, 17, 17, 18, 7, 8, 11, 17, 10, 15, 7, 10, 13, 8, 6, 10, 17, 15, 11, 18, 12, 9, 14, 12, 14, 8, 13, 5, 10, 7, 12, 10, 19, 11, 15, 8, 14, 16, 8, 13, 11, 19, 16, 7, 11, 16, 7, 14, 9, 17, 15, 16, 19, 5, 19, 15, 7, 5, 16, 9, 15, 16, 18, 7, 13, 19, 13, 12, 14, 7, 17, 7, 19, 17, 16, 6, 9, 15, 14, 17, 14, 13, 16, 12, 11, 18, 12, 16, 12, 14, 8, 11, 14, 8, 16, 13, 7, 10, 7, 14, 11, 15, 14, 12, 9, 10, 15, 6, 16, 18, 13, 13, 19, 9, 5, 8, 10, 12, 13, 11, 18, 11, 13, 11, 15, 7, 18, 11, 6, 18, 19, 6, 13, 8, 18, 14, 13, 8, 9, 16, 17, 10, 6, 15, 6, 17, 8, 19, 14, 17, 14, 9, 14, 12, 17, 7, 6, 6, 16, 7, 19, 10, 6, 7, 8, 12, 19, 11, 11, 19, 18, 14, 15, 7, 18, 15, 7, 6, 17, 9, 8, 16, 17, 17, 14, 19, 10, 8, 13, 14, 16, 19, 11, 17, 7, 11, 18, 16, 11, 16, 19, 6, 17, 7, 8, 12, 9, 19, 17, 12, 6, 12, 16, 9, 12, 18, 9, 12, 16, 17, 19, 9, 13, 18, 12, 10, 16, 9, 5, 19, 11, 17, 12, 8, 17, 12, 13, 14, 11, 18, 17, 5, 13, 7, 11, 16, 12, 10, 8, 7, 18, 18, 11, 7, 15, 8, 10, 10, 12, 16, 5, 11, 17, 18, 15, 6, 11, 9, 10, 16, 17, 15, 18, 19, 13, 12, 19, 6, 7, 13, 7, 13, 13, 13, 8, 11, 12, 19, 19, 7, 5, 12, 6, 8, 8, 6, 8, 8, 18, 9, 17, 15, 17, 14, 5, 7, 10, 7, 13, 7, 5, 12, 17, 18, 11, 19, 7, 16, 14, 5, 17, 17, 15, 14, 10, 18, 12, 8, 18, 14, 19, 13, 12, 9, 5, 17, 15, 12, 14, 19, 12, 13, 19, 13, 9, 12, 15, 15, 18, 6, 6, 5, 15, 17, 16, 12, 14, 6, 15, 13, 8, 17, 17, 8, 8, 16, 7, 15, 19, 14, 12, 7, 7, 11, 17, 10, 9, 7, 17, 16, 17, 19, 6, 17, 6, 10, 7, 12, 17, 6, 6, 8, 17, 10, 12, 7, 12, 10, 6, 8, 16, 14, 15, 11, 8, 16, 17, 9, 11, 10, 19, 18, 7, 14, 13, 17, 11, 18, 5, 13, 14, 14, 16, 11, 8, 7, 6, 13, 8, 8, 15, 16, 19, 15, 8, 5, 13, 8, 11, 9, 10, 15, 13, 15, 10, 19, 16, 16, 18, 16, 6, 12, 6, 13, 5, 6, 11, 17, 18, 7, 7, 14, 19, 10, 16, 16, 17, 15, 16, 12, 6, 19, 11, 13, 18, 12, 14, 10, 7, 10, 17, 5, 17, 18, 12, 5, 17, 9, 18, 9, 11, 17, 7, 16, 9, 18, 10, 15, 7, 19, 8, 6, 15, 6, 18, 16, 7, 16, 14, 12, 17, 11, 12, 16, 15, 16, 19, 10, 15, 19, 9, 17, 10, 18, 15, 15, 11, 13, 16, 16, 11, 8, 6, 13, 17, 11, 12, 17, 18, 18, 7, 7, 6, 12, 12, 18, 9, 19, 12, 14, 8, 14, 18, 11, 11, 15, 19, 19, 10, 18, 17, 11, 15, 17, 9, 6, 14, 10, 14, 17, 6, 18, 7, 15, 12, 5, 15, 17, 14, 8, 14, 17, 8, 11, 15, 5, 7, 13, 14, 16, 5, 17, 10, 19, 13, 13, 12, 15, 13, 6, 14, 17, 14, 17, 10, 5, 12, 19, 7, 9, 17, 10, 19, 6, 15, 9, 12, 7, 16, 10, 5, 15, 11, 19, 9, 8, 8, 9, 5, 8, 10, 10, 12, 16, 17, 15, 15, 5, 14, 12, 7, 5, 9, 5, 15, 15, 9, 6, 18, 14, 19, 17, 10, 12, 10, 17, 18, 14, 8, 18, 8, 13, 5, 13, 6, 16, 13, 7, 7, 6, 12, 14, 5, 14, 8, 13, 18, 7, 16, 17, 10, 10, 14, 18, 16, 12, 12, 10, 9, 17, 8, 6, 16, 10, 7, 11, 10, 8, 17, 14, 7, 17, 10, 8, 11, 7, 7, 16, 10, 14, 15, 17, 5, 10, 6, 12, 6, 14, 7, 18, 5, 17, 19, 15, 13, 16, 9, 11, 17, 18, 12, 14, 10, 7, 19, 12, 14, 18, 5, 5, 9, 15, 7, 9, 13, 8, 10, 7, 17, 6, 12, 7, 9, 9, 10, 6, 19, 7, 11, 15, 12, 14, 19, 13, 8, 10, 13, 14, 17, 15, 17, 9, 19, 10, 7, 8, 15, 10, 6, 15, 13, 10, 14, 13, 9, 16, 9, 15, 9, 12, 5, 19, 6, 15, 17, 13, 11, 9, 18, 19]}, {\"axis\": {\"matches\": true}, \"label\": \"sc_w\", \"values\": [7, 3, 2, 8, 2, 1, 8, 3, 1, 10, 2, 9, 0, 1, 9, 15, 9, 2, 13, 1, 15, 1, 5, 7, 0, 9, 1, 8, 1, 7, 11, 1, 5, 0, 1, 4, 7, 5, 1, 12, 2, 10, 0, 12, 3, 3, 5, 12, 3, 13, 0, 12, 5, 8, 0, 7, 11, 4, 5, 13, 4, 6, 0, 2, 4, 8, 4, 11, 1, 1, 11, 7, 6, 0, 2, 4, 7, 7, 13, 7, 10, 11, 6, 3, 5, 9, 0, 3, 5, 1, 3, 7, 10, 10, 17, 5, 4, 17, 3, 2, 8, 7, 14, 10, 4, 2, 6, 1, 17, 13, 4, 13, 6, 4, 1, 1, 9, 4, 3, 0, 9, 11, 4, 4, 5, 14, 3, 4, 0, 10, 7, 0, 9, 5, 8, 3, 4, 11, 9, 1, 0, 0, 17, 11, 0, 12, 6, 1, 5, 10, 1, 11, 7, 1, 2, 10, 4, 0, 0, 0, 5, 6, 1, 14, 4, 16, 9, 6, 8, 2, 3, 4, 1, 6, 12, 5, 8, 8, 12, 4, 7, 12, 1, 1, 14, 2, 0, 9, 5, 7, 4, 10, 9, 12, 5, 10, 9, 2, 10, 4, 4, 4, 6, 4, 15, 7, 4, 0, 15, 17, 0, 9, 3, 9, 5, 5, 3, 1, 7, 8, 17, 5, 11, 10, 11, 6, 4, 8, 16, 2, 10, 10, 6, 5, 8, 8, 12, 1, 0, 2, 1, 6, 1, 1, 6, 3, 3, 15, 5, 0, 1, 9, 5, 5, 9, 4, 8, 0, 8, 1, 3, 1, 3, 0, 0, 3, 8, 4, 6, 1, 13, 15, 6, 4, 13, 11, 5, 2, 4, 9, 16, 7, 10, 5, 5, 0, 4, 5, 3, 4, 3, 1, 12, 5, 13, 0, 1, 6, 10, 8, 4, 1, 3, 3, 5, 11, 1, 6, 10, 6, 12, 0, 0, 2, 2, 1, 1, 5, 12, 5, 0, 10, 5, 6, 3, 7, 0, 11, 1, 6, 15, 1, 0, 0, 1, 15, 1, 12, 7, 11, 4, 7, 0, 1, 10, 4, 0, 0, 3, 5, 11, 16, 10, 17, 0, 4, 14, 9, 1, 7, 7, 1, 11, 13, 0, 4, 8, 2, 7, 18, 1, 6, 1, 9, 1, 16, 11, 7, 7, 11, 1, 8, 6, 2, 6, 15, 3, 16, 6, 1, 0, 7, 2, 18, 1, 7, 8, 1, 0, 3, 5, 0, 12, 8, 2, 5, 0, 10, 2, 9, 7, 10, 2, 3, 2, 1, 5, 9, 13, 7, 5, 14, 1, 6, 2, 2, 3, 1, 5, 1, 16, 2, 8, 6, 10, 0, 7, 11, 8, 1, 0, 1, 11, 9, 5, 11, 3, 3, 2, 1, 12, 8, 8, 2, 7, 2, 3, 9, 15, 1, 3, 7, 12, 8, 14, 10, 4, 9, 1, 5, 5, 7, 2, 1, 8, 2, 0, 1, 2, 14, 3, 9, 6, 10, 4, 4, 12, 1, 10, 0, 0, 1, 5, 8, 8, 0, 4, 4, 1, 3, 1, 6, 8, 13, 2, 6, 11, 14, 17, 4, 6, 3, 3, 9, 1, 7, 1, 13, 8, 2, 2, 0, 12, 5, 6, 8, 8, 17, 4, 4, 3, 5, 10, 3, 3, 2, 7, 0, 1, 3, 15, 1, 6, 4, 14, 1, 1, 7, 5, 5, 6, 1, 2, 7, 10, 5, 6, 10, 9, 6, 4, 6, 2, 8, 7, 14, 4, 5, 6, 11, 3, 6, 0, 5, 4, 7, 2, 7, 9, 4, 3, 10, 10, 3, 7, 6, 3, 5, 2, 3, 9, 1, 3, 2, 1, 12, 4, 2, 14, 17, 8, 10, 3, 1, 16, 2, 12, 6, 4, 3, 8, 5, 3, 8, 5, 7, 4, 10, 1, 4, 7, 14, 11, 0, 13, 12, 0, 3, 8, 1, 8, 16, 4, 3, 1, 3, 10, 0, 9, 15, 0, 0, 6, 0, 4, 1, 0, 10, 3, 9, 3, 3, 5, 5, 9, 0, 13, 13, 12, 1, 9, 6, 13, 10, 9, 2, 3, 7, 6, 3, 2, 4, 6, 11, 1, 1, 12, 1, 14, 0, 11, 5, 5, 10, 7, 6, 3, 1, 4, 11, 9, 1, 8, 9, 12, 10, 3, 1, 6, 2, 0, 4, 10, 16, 1, 5, 4, 6, 3, 3, 2, 7, 11, 3, 11, 6, 2, 15, 0, 4, 12, 9, 6, 11, 8, 1, 6, 3, 1, 13, 7, 13, 8, 3, 0, 1, 8, 5, 1, 4, 8, 0, 5, 1, 0, 1, 8, 2, 7, 3, 10, 5, 14, 7, 3, 3, 6, 7, 4, 2, 4, 5, 6, 3, 11, 8, 11, 2, 14, 1, 8, 5, 6, 8, 5, 4, 0, 3, 0, 5, 0, 6, 8, 0, 14, 10, 1, 9, 12, 13, 5, 3, 14, 3, 4, 8, 2, 1, 8, 2, 12, 1, 9, 4, 7, 0, 0, 2, 1, 6, 3, 13, 2, 3, 7, 5, 6, 4, 5, 12, 3, 15, 8, 13, 1, 12, 2, 10, 0, 5, 5, 1, 0, 12, 4, 5, 8, 6, 2, 7, 0, 2, 17, 11, 0, 4, 6, 14, 4, 5, 8, 12, 6, 2, 0, 1, 2, 1, 7, 3, 11, 11, 1, 2, 8, 5, 7, 3, 12, 12, 1, 5, 7, 4, 0, 0, 13, 2, 5, 8, 3, 3, 15, 6, 5, 1, 7, 3, 4, 4, 2, 6, 3, 0, 1, 2, 4, 7, 3, 0, 0, 4, 3, 16, 2, 7, 10, 10, 2, 7, 1, 2, 5, 9, 8, 11, 10, 12, 2, 11, 8, 4, 4, 5, 4, 10, 4, 9, 7, 11, 2, 2, 12, 8, 3, 4, 11, 7, 5, 9, 3, 10, 12, 8, 6, 0, 3, 9, 3, 5, 7, 14, 9, 7, 4, 11, 4, 6, 4, 1, 12, 5, 0, 5, 7, 2, 4, 3, 9, 2, 0, 0, 6, 13, 8, 3, 6, 1, 3, 4, 10, 12, 5, 4, 9, 8, 10, 14, 8, 4, 4, 2, 1, 6, 6, 11, 6, 12, 2, 1, 4, 4, 7, 0, 6, 4, 4, 2, 2, 0, 3, 6, 10, 5, 0, 6, 1, 11, 1, 3, 4, 10, 10, 8, 12, 1, 12, 1, 4, 11, 3, 4, 16, 4, 3, 0, 1, 0, 10, 5, 3, 13, 0, 2, 0, 1, 4, 6, 9, 2, 11, 8, 3, 13, 3, 2, 0, 5, 5, 3, 18, 5, 4, 4, 3, 9, 6, 3, 1, 2, 0, 0, 5, 6, 12, 0, 0, 2, 13, 7, 5, 11, 16, 1, 6, 10, 3, 2, 3, 9, 8, 3, 8, 13, 2, 3, 15, 13, 3, 1, 11, 4, 0, 1, 5, 6, 8, 3, 6, 4, 1, 1, 4, 14, 10, 3, 4, 12, 3, 14, 12, 9, 8, 4, 2, 3, 12, 15, 6, 10, 5, 12, 5, 9, 11, 1, 2, 0, 4, 11, 13, 2, 11, 3, 0, 1, 0, 6, 13, 1, 12, 11, 5, 1, 3, 0, 11, 4, 6, 0, 0, 7, 11, 3, 7, 5, 10, 1, 1, 6, 2, 4, 15, 10, 2, 4, 5, 7, 0, 0, 10, 13, 11, 6, 8, 5, 8, 3, 3, 3, 4, 2, 6, 5, 1, 8, 15, 4, 6, 7, 7, 16, 7, 4, 0, 5, 7, 6, 1, 4, 6, 6, 8, 5, 1, 5, 9, 7, 9, 15, 11, 6, 0, 8, 4, 4, 1, 1, 2, 1, 6, 7, 1, 10, 12, 7, 7, 5, 3, 1, 8, 16, 13, 3, 9, 0, 5, 9, 2, 11, 3, 13, 2, 0, 18, 7, 1, 1, 15, 4, 12, 11, 16, 6, 7, 6, 9, 10, 1, 3, 0, 3, 7, 1, 9, 4, 7, 0, 5, 2, 9, 10, 3, 2, 9, 16, 8, 14, 1, 5, 4, 4, 7, 6, 10, 0, 4, 0, 5, 7, 5, 10, 12, 1, 3, 0, 9, 5, 1, 15, 11, 6, 0, 4, 1, 2, 9, 4, 4, 0, 10, 1, 4, 10, 3, 2, 2, 7, 0, 12, 2, 0, 12, 1, 12, 7, 7, 7, 7, 2, 16, 8, 1, 2, 1, 1, 0, 11, 12, 2, 3, 6, 9, 5, 9, 6, 1, 4, 0, 3, 11, 8, 5, 3, 0, 10, 16, 2, 2, 12, 6, 8, 12, 3, 13, 9, 4, 5, 3, 3, 3, 11, 1, 12, 0, 4, 4, 2, 3, 8, 7, 8, 10, 5, 4, 10, 11, 15, 8, 7, 3, 5, 13, 2, 4, 10, 3, 10, 8, 2, 3, 10, 11, 3, 3, 5, 4, 4, 12, 7, 18, 5, 5, 16, 1, 3, 0, 3, 0, 9, 6, 15, 8, 1, 11, 7, 7, 2, 6, 4, 8, 3, 10, 5, 8, 10, 4, 7, 1, 5, 10, 12, 5, 2, 4, 6, 6, 8, 0, 11, 1, 4, 7, 3, 11, 2, 8, 0, 3, 11, 1, 12, 13, 9, 1, 0, 9, 2, 4, 0, 5, 0, 7, 6, 4, 8, 5, 3, 1, 6, 1, 2, 1, 3, 4, 1, 2, 0, 9, 2, 11, 9, 8, 5, 1, 6, 2, 3, 9, 2, 3, 10, 6, 7, 9, 11, 4, 3, 11, 2, 10, 15, 14, 10, 3, 16, 6, 7, 4, 3, 10, 10, 0, 2, 0, 4, 10, 0, 13, 15, 11, 10, 17, 11, 4, 5, 10, 2, 4, 1, 4, 0, 3, 16, 2, 8, 2, 1, 8, 4, 4, 16, 10, 4, 5, 0, 0, 13, 15, 13, 10, 3, 0, 9, 11, 4, 7, 0, 6, 2, 11, 0, 5, 16, 0, 9, 2, 0, 7, 5, 1, 7, 4, 1, 2, 1, 5, 6, 0, 6, 13, 1, 12, 7, 7, 6, 16, 3, 0, 7, 9, 17, 2, 8, 9, 1, 8, 11, 4, 6, 4, 3, 9, 3, 1, 0, 5, 3, 4, 4, 1, 2, 17, 5, 7, 4, 12, 6, 8, 6, 9, 10, 1, 6, 6, 18, 8, 2, 8, 9, 1, 8, 1, 7, 2, 0, 0, 16, 13, 3, 5, 10, 14, 2, 6, 7, 2, 2, 2, 4, 2, 9, 4, 11, 7, 10, 8, 7, 0, 1, 9, 4, 14, 3, 4, 2, 16, 3, 17, 2, 1, 3, 6, 13, 4, 4, 4, 2, 5, 13, 0, 5, 0, 3, 1, 2, 1, 0, 4, 1, 14, 10, 7, 8, 3, 7, 0, 1, 10, 8, 8, 14, 5, 15, 11, 9, 8, 3, 13, 14, 1, 7, 1, 0, 15, 10, 2, 12, 3, 7, 0, 3, 1, 4, 8, 7, 3, 2, 8, 11, 3, 4, 10, 3, 5, 8, 9, 8, 4, 1, 7, 10, 5, 7, 2, 1, 1, 3, 9, 4, 0, 17, 3, 3, 4, 3, 9, 8, 12, 5, 10, 0, 4, 9, 6, 1, 2, 10, 11, 9, 0, 13, 8, 8, 1, 7, 0, 8, 6, 5, 10, 8, 8, 16, 0, 3, 7, 18, 3, 0, 5, 5, 17, 1, 10, 7, 9, 3, 6, 4, 2, 1, 8, 15, 7, 4, 0, 3, 1, 0, 4, 1, 3, 5, 9, 3, 10, 3, 7, 10, 0, 0, 1, 2, 0, 11, 0, 3, 14, 11, 13, 11, 3, 9, 9, 0, 0, 8, 3, 1, 6, 10, 3, 0, 4, 0, 6, 5, 5, 2, 9, 7, 0, 8, 6, 0, 9, 0, 8, 8, 5, 9, 5, 11, 15, 10, 2, 1, 4, 3, 5, 2, 5, 0, 3, 7, 8, 3, 5, 1, 2, 12, 8, 7, 9, 4, 6, 14, 1, 1, 5, 13, 2, 1, 2, 6, 2, 7, 0, 3, 0, 7, 1, 5, 3, 3, 3, 10, 7, 17, 2, 12, 9, 5, 6, 2, 4, 3, 4, 3, 5, 9, 2, 1, 4, 6, 0, 5, 5, 5, 7, 5, 3, 1, 0, 5, 14, 5, 1, 8, 5, 10, 17, 5, 2, 1, 6, 5, 7, 7, 13, 3, 18, 3, 3, 4, 6, 5, 3, 8, 10, 0, 1, 3, 5, 6, 4, 12, 2, 8, 3, 8, 3, 11, 16, 4, 10, 1, 10, 4]}, {\"axis\": {\"matches\": true}, \"label\": \"talk_time\", \"values\": [19, 7, 9, 11, 15, 10, 18, 5, 20, 12, 7, 13, 2, 4, 3, 11, 19, 18, 16, 18, 3, 15, 20, 20, 12, 7, 4, 12, 10, 10, 12, 6, 3, 12, 15, 15, 2, 4, 2, 11, 11, 19, 4, 14, 12, 20, 3, 12, 5, 10, 15, 16, 10, 12, 18, 13, 17, 20, 4, 9, 20, 6, 15, 5, 19, 13, 4, 8, 4, 7, 2, 13, 4, 12, 6, 13, 17, 6, 10, 16, 6, 16, 11, 11, 17, 4, 7, 4, 8, 11, 16, 14, 5, 2, 12, 12, 8, 14, 18, 7, 20, 11, 15, 18, 8, 11, 11, 14, 9, 14, 18, 7, 3, 5, 20, 13, 4, 10, 9, 14, 6, 4, 6, 17, 15, 9, 18, 7, 7, 13, 7, 20, 4, 7, 20, 9, 4, 2, 4, 9, 17, 5, 17, 5, 10, 16, 19, 4, 4, 15, 20, 12, 19, 11, 3, 6, 8, 11, 19, 10, 16, 7, 4, 4, 7, 11, 7, 12, 10, 17, 13, 20, 7, 12, 20, 13, 4, 20, 2, 19, 7, 15, 17, 6, 16, 15, 18, 2, 10, 13, 15, 6, 7, 10, 18, 3, 12, 4, 14, 16, 13, 13, 6, 15, 8, 4, 17, 12, 6, 6, 13, 10, 13, 10, 10, 8, 7, 16, 20, 4, 10, 18, 11, 13, 2, 6, 16, 13, 3, 7, 19, 6, 13, 20, 6, 19, 20, 8, 11, 11, 16, 7, 3, 16, 11, 14, 11, 11, 7, 10, 6, 7, 7, 15, 12, 2, 10, 5, 15, 3, 5, 12, 8, 15, 15, 4, 16, 14, 17, 6, 6, 4, 5, 9, 14, 15, 13, 9, 9, 11, 9, 18, 18, 2, 4, 6, 3, 5, 9, 18, 18, 8, 18, 11, 12, 20, 16, 6, 5, 19, 14, 8, 16, 15, 19, 7, 19, 17, 19, 15, 13, 10, 6, 13, 12, 13, 6, 20, 7, 9, 5, 18, 10, 7, 14, 5, 8, 7, 15, 13, 15, 2, 12, 16, 11, 12, 11, 4, 14, 16, 15, 2, 18, 12, 15, 13, 5, 19, 8, 18, 15, 9, 12, 19, 14, 9, 4, 6, 20, 11, 3, 7, 13, 11, 14, 8, 10, 10, 18, 5, 13, 16, 20, 16, 8, 9, 4, 17, 16, 8, 2, 12, 16, 16, 2, 7, 3, 18, 16, 19, 18, 2, 8, 10, 14, 18, 6, 7, 15, 7, 4, 20, 6, 3, 9, 5, 16, 12, 7, 13, 4, 10, 6, 17, 7, 16, 20, 19, 2, 5, 20, 10, 2, 16, 10, 18, 19, 7, 4, 15, 16, 20, 14, 19, 16, 2, 2, 7, 8, 3, 8, 14, 15, 6, 13, 8, 8, 4, 16, 14, 4, 19, 2, 11, 9, 19, 5, 12, 19, 9, 10, 7, 3, 10, 11, 9, 18, 4, 16, 6, 16, 17, 6, 19, 7, 13, 17, 12, 3, 14, 6, 8, 11, 6, 10, 14, 10, 18, 9, 18, 18, 3, 13, 18, 15, 17, 18, 16, 14, 11, 20, 13, 2, 9, 9, 11, 10, 15, 9, 15, 8, 16, 19, 12, 18, 18, 10, 11, 20, 13, 6, 11, 3, 2, 2, 18, 10, 14, 7, 6, 9, 8, 7, 9, 20, 3, 19, 10, 15, 12, 15, 12, 7, 4, 12, 6, 8, 14, 19, 19, 3, 4, 8, 18, 10, 9, 12, 15, 17, 8, 17, 17, 15, 15, 10, 14, 8, 5, 2, 3, 10, 16, 10, 18, 6, 14, 14, 9, 17, 17, 3, 6, 16, 14, 4, 6, 10, 15, 20, 6, 6, 15, 17, 16, 5, 9, 12, 5, 17, 14, 2, 9, 5, 17, 14, 3, 12, 4, 13, 4, 17, 3, 5, 3, 9, 6, 18, 7, 20, 7, 19, 13, 15, 19, 12, 2, 18, 12, 4, 8, 20, 5, 16, 6, 11, 17, 14, 9, 9, 12, 9, 8, 5, 2, 13, 18, 5, 20, 20, 16, 13, 11, 13, 8, 9, 20, 2, 9, 17, 5, 9, 3, 15, 17, 2, 5, 3, 8, 12, 16, 9, 7, 14, 18, 7, 5, 12, 8, 15, 13, 20, 2, 11, 20, 4, 18, 11, 7, 4, 18, 9, 15, 5, 19, 19, 7, 8, 18, 8, 2, 4, 5, 18, 4, 11, 10, 17, 20, 10, 5, 2, 14, 2, 4, 16, 19, 18, 4, 16, 7, 4, 12, 13, 6, 9, 13, 11, 18, 15, 6, 6, 13, 13, 15, 16, 20, 16, 13, 9, 14, 18, 17, 8, 3, 16, 6, 7, 2, 14, 8, 8, 20, 13, 12, 20, 2, 19, 4, 3, 18, 10, 8, 11, 3, 19, 2, 16, 8, 8, 10, 11, 8, 8, 4, 19, 15, 17, 7, 14, 15, 2, 5, 7, 13, 3, 12, 12, 3, 16, 13, 17, 4, 15, 14, 18, 7, 19, 14, 8, 10, 15, 3, 14, 3, 6, 2, 5, 13, 11, 7, 15, 2, 12, 4, 7, 7, 17, 14, 9, 20, 11, 20, 11, 13, 19, 13, 4, 13, 16, 7, 18, 8, 4, 17, 14, 13, 15, 14, 11, 14, 5, 11, 11, 15, 13, 17, 8, 15, 14, 4, 18, 9, 12, 19, 12, 15, 20, 15, 2, 19, 12, 3, 18, 6, 10, 13, 9, 19, 19, 14, 10, 4, 10, 8, 9, 18, 6, 15, 14, 16, 19, 5, 13, 6, 16, 17, 11, 18, 16, 7, 6, 6, 20, 18, 7, 18, 6, 15, 20, 20, 20, 6, 17, 10, 9, 19, 10, 18, 2, 17, 14, 9, 11, 17, 5, 8, 15, 11, 14, 7, 7, 18, 15, 13, 19, 2, 6, 10, 17, 17, 12, 2, 8, 9, 14, 15, 9, 6, 13, 20, 13, 4, 9, 13, 4, 19, 8, 6, 8, 17, 9, 11, 10, 12, 12, 5, 9, 19, 6, 15, 13, 15, 7, 4, 4, 19, 10, 3, 3, 15, 3, 6, 8, 15, 14, 9, 13, 16, 17, 20, 9, 9, 12, 9, 14, 6, 10, 8, 15, 16, 19, 18, 11, 19, 7, 16, 2, 12, 7, 7, 4, 16, 8, 12, 8, 16, 17, 2, 7, 14, 5, 18, 7, 6, 7, 8, 19, 10, 7, 15, 7, 12, 3, 13, 13, 6, 7, 14, 3, 19, 20, 3, 12, 15, 19, 4, 4, 3, 3, 8, 4, 7, 6, 3, 12, 19, 13, 16, 6, 17, 9, 13, 10, 3, 19, 7, 2, 4, 18, 19, 3, 17, 7, 8, 9, 6, 20, 17, 10, 8, 9, 2, 10, 13, 7, 8, 10, 15, 17, 17, 14, 2, 15, 4, 7, 7, 19, 11, 12, 8, 7, 17, 13, 18, 8, 18, 16, 15, 7, 11, 9, 20, 20, 12, 10, 5, 3, 15, 13, 16, 16, 9, 11, 15, 9, 17, 14, 2, 5, 2, 20, 6, 11, 5, 17, 9, 12, 4, 10, 3, 19, 18, 13, 11, 8, 5, 14, 6, 15, 3, 4, 15, 14, 19, 10, 2, 16, 14, 7, 8, 6, 4, 15, 8, 8, 12, 20, 4, 12, 16, 9, 2, 19, 13, 20, 20, 5, 5, 14, 9, 16, 6, 17, 20, 9, 15, 4, 16, 4, 3, 5, 20, 5, 14, 17, 16, 5, 10, 15, 7, 11, 11, 12, 11, 20, 13, 19, 5, 16, 2, 6, 4, 9, 14, 9, 4, 16, 14, 20, 6, 2, 4, 12, 11, 20, 19, 15, 18, 11, 16, 8, 4, 8, 5, 20, 8, 9, 3, 18, 15, 4, 15, 12, 17, 5, 17, 14, 17, 15, 17, 6, 14, 5, 20, 3, 5, 18, 13, 6, 2, 18, 5, 13, 10, 19, 7, 10, 10, 5, 3, 12, 6, 2, 19, 6, 9, 8, 4, 5, 11, 14, 8, 3, 10, 2, 10, 10, 14, 6, 17, 5, 11, 18, 6, 10, 6, 4, 20, 7, 5, 4, 19, 13, 10, 6, 13, 19, 7, 13, 17, 18, 17, 15, 20, 3, 19, 16, 20, 11, 19, 8, 2, 6, 15, 12, 10, 16, 2, 4, 14, 12, 20, 11, 12, 19, 6, 11, 19, 12, 15, 6, 8, 4, 8, 15, 14, 8, 18, 8, 14, 2, 9, 17, 13, 16, 6, 16, 10, 12, 6, 11, 16, 3, 19, 18, 16, 14, 7, 17, 19, 8, 11, 10, 2, 10, 2, 16, 19, 12, 3, 14, 9, 19, 2, 10, 20, 3, 7, 7, 6, 13, 4, 17, 13, 19, 17, 12, 11, 19, 9, 6, 18, 5, 10, 13, 19, 15, 20, 5, 6, 18, 4, 9, 5, 20, 8, 20, 8, 16, 14, 12, 10, 16, 15, 14, 16, 18, 6, 7, 17, 16, 12, 17, 5, 14, 18, 19, 14, 19, 2, 7, 6, 12, 10, 18, 8, 16, 18, 9, 7, 19, 7, 16, 19, 16, 16, 5, 19, 16, 16, 13, 20, 9, 11, 20, 18, 5, 19, 14, 18, 19, 3, 4, 15, 7, 5, 12, 3, 9, 4, 16, 19, 6, 15, 18, 12, 9, 11, 12, 6, 19, 16, 17, 6, 3, 7, 16, 15, 8, 15, 2, 19, 10, 3, 2, 11, 17, 12, 6, 3, 6, 20, 7, 14, 4, 20, 19, 17, 20, 11, 15, 20, 14, 11, 20, 2, 7, 11, 7, 9, 3, 19, 5, 2, 15, 12, 16, 20, 13, 10, 11, 14, 12, 2, 4, 7, 18, 19, 20, 15, 18, 9, 17, 9, 14, 9, 12, 6, 18, 17, 2, 15, 3, 4, 18, 18, 18, 14, 20, 2, 8, 2, 2, 13, 13, 11, 11, 9, 17, 17, 16, 8, 4, 3, 12, 17, 15, 8, 10, 4, 2, 3, 8, 5, 3, 10, 3, 6, 15, 18, 10, 14, 17, 10, 7, 4, 14, 15, 8, 7, 20, 16, 9, 16, 18, 13, 16, 19, 13, 11, 6, 5, 2, 4, 10, 18, 17, 4, 20, 17, 18, 20, 2, 9, 8, 20, 11, 14, 11, 15, 19, 18, 5, 5, 18, 6, 10, 11, 8, 5, 16, 16, 12, 2, 5, 3, 8, 10, 13, 5, 10, 3, 4, 9, 6, 20, 16, 12, 11, 4, 15, 15, 12, 7, 10, 10, 3, 7, 14, 3, 4, 11, 18, 13, 20, 20, 12, 12, 2, 14, 17, 4, 16, 10, 19, 19, 18, 4, 4, 4, 2, 11, 3, 7, 4, 4, 15, 8, 16, 18, 6, 16, 19, 13, 9, 8, 9, 18, 15, 17, 8, 11, 7, 5, 17, 7, 12, 20, 7, 19, 13, 16, 18, 17, 12, 2, 6, 13, 20, 15, 14, 11, 5, 15, 2, 16, 20, 15, 4, 19, 7, 6, 11, 20, 10, 2, 16, 10, 5, 11, 3, 14, 18, 5, 17, 6, 8, 7, 13, 9, 3, 9, 10, 11, 4, 19, 6, 8, 12, 9, 2, 16, 7, 15, 11, 19, 16, 14, 19, 17, 16, 2, 8, 11, 3, 13, 6, 11, 16, 7, 13, 19, 10, 5, 14, 13, 5, 7, 3, 6, 13, 5, 20, 17, 19, 18, 14, 13, 5, 15, 17, 12, 7, 11, 12, 17, 17, 4, 20, 9, 15, 4, 9, 20, 4, 14, 5, 7, 7, 12, 19, 16, 5, 16, 19, 2, 5, 6, 3, 14, 7, 4, 8, 17, 10, 9, 16, 17, 15, 17, 3, 2, 4, 3, 4, 10, 14, 5, 3, 2, 11, 11, 8, 4, 15, 9, 8, 2, 16, 11, 19, 6, 2, 7, 6, 2, 10, 14, 7, 8, 2, 7, 4, 20, 4, 9, 17, 11, 2, 11, 17, 10, 16, 5, 9, 10, 10, 7, 12, 15, 13, 15, 7, 16, 4, 14, 7, 3, 10, 18, 19, 4, 17, 10, 2, 3, 17, 3, 17, 9, 5, 4, 12, 3, 14, 19, 20, 7, 2, 13, 5, 9, 12, 7, 11, 3, 5, 19, 4, 11, 4, 16, 7, 2, 17, 12, 6, 20, 13, 10, 3, 4, 14, 16, 13, 20, 3, 19, 3, 9, 19, 20, 8, 4, 15, 14, 8, 3, 15, 8, 8, 17, 7, 14, 13, 2, 14, 15, 10, 11, 16, 2, 11, 4, 6, 5, 8, 7, 19, 3, 7, 20, 4, 5, 3, 19, 16, 5, 19, 2]}, {\"axis\": {\"matches\": true}, \"label\": \"three_g\", \"values\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"touch_screen\", \"values\": [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"wifi\", \"values\": [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]}, {\"axis\": {\"matches\": true}, \"label\": \"price_range\", \"values\": [1, 2, 2, 2, 1, 1, 3, 0, 0, 0, 3, 3, 1, 2, 0, 0, 3, 3, 1, 1, 3, 3, 1, 0, 1, 2, 3, 3, 2, 0, 3, 0, 1, 3, 0, 1, 1, 3, 2, 2, 2, 3, 1, 1, 0, 1, 3, 1, 0, 3, 2, 2, 3, 1, 2, 2, 0, 3, 1, 2, 1, 0, 0, 2, 2, 3, 3, 3, 0, 3, 3, 2, 0, 3, 0, 1, 0, 0, 3, 1, 3, 2, 3, 1, 1, 1, 0, 3, 2, 2, 3, 2, 0, 0, 1, 3, 0, 2, 0, 1, 3, 1, 1, 0, 0, 1, 3, 3, 3, 3, 1, 1, 0, 2, 3, 3, 2, 1, 0, 1, 2, 3, 3, 3, 2, 3, 2, 1, 3, 0, 3, 1, 2, 1, 2, 2, 1, 3, 0, 0, 2, 0, 3, 2, 0, 3, 1, 3, 2, 2, 1, 3, 1, 2, 0, 1, 0, 0, 3, 1, 2, 2, 0, 0, 2, 3, 1, 2, 3, 1, 3, 1, 2, 2, 3, 2, 0, 3, 2, 1, 0, 0, 2, 3, 3, 0, 2, 3, 1, 3, 1, 2, 3, 3, 2, 2, 2, 3, 1, 1, 0, 1, 0, 2, 2, 2, 2, 1, 3, 3, 2, 2, 0, 2, 0, 3, 0, 1, 1, 3, 0, 0, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 0, 1, 2, 0, 1, 3, 1, 0, 2, 2, 3, 2, 3, 3, 0, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 3, 1, 1, 0, 0, 3, 1, 3, 1, 2, 1, 0, 0, 3, 1, 2, 2, 3, 1, 0, 0, 3, 3, 1, 1, 2, 1, 3, 3, 1, 1, 3, 3, 2, 3, 0, 1, 3, 1, 0, 1, 3, 0, 0, 3, 0, 1, 1, 2, 0, 2, 3, 2, 3, 0, 2, 3, 1, 0, 0, 1, 0, 3, 3, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 0, 2, 2, 2, 1, 2, 3, 0, 1, 2, 2, 2, 2, 1, 3, 0, 2, 2, 1, 2, 0, 0, 3, 1, 0, 3, 2, 0, 0, 2, 3, 2, 1, 3, 1, 3, 0, 3, 2, 0, 1, 0, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 3, 3, 1, 3, 3, 1, 3, 3, 0, 1, 3, 2, 3, 3, 2, 2, 3, 3, 0, 2, 2, 0, 3, 1, 2, 3, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 3, 1, 1, 3, 1, 3, 1, 0, 0, 3, 1, 3, 0, 0, 1, 3, 1, 0, 0, 2, 2, 2, 1, 1, 1, 3, 0, 3, 2, 1, 3, 1, 0, 2, 2, 1, 0, 0, 3, 3, 0, 0, 1, 3, 3, 1, 0, 2, 2, 3, 0, 3, 3, 0, 3, 0, 3, 1, 0, 2, 2, 1, 2, 0, 3, 2, 0, 1, 2, 3, 1, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 2, 2, 3, 3, 0, 1, 1, 2, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 0, 0, 3, 1, 2, 2, 0, 2, 2, 1, 0, 3, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 0, 1, 0, 0, 2, 2, 2, 0, 3, 1, 2, 1, 2, 0, 0, 1, 3, 3, 3, 3, 3, 3, 0, 2, 2, 0, 0, 3, 3, 3, 0, 0, 3, 0, 0, 1, 2, 0, 2, 3, 1, 3, 2, 0, 2, 2, 0, 0, 1, 3, 0, 1, 1, 2, 2, 0, 2, 2, 0, 1, 3, 0, 2, 2, 3, 1, 3, 2, 2, 1, 2, 2, 1, 1, 0, 1, 0, 1, 0, 3, 1, 1, 3, 1, 3, 2, 1, 1, 0, 2, 0, 2, 0, 2, 0, 1, 3, 1, 2, 2, 0, 1, 0, 3, 2, 1, 3, 2, 0, 0, 1, 2, 0, 0, 0, 2, 3, 1, 2, 1, 0, 0, 0, 2, 3, 2, 3, 0, 3, 2, 1, 2, 1, 0, 2, 0, 0, 1, 3, 0, 3, 1, 1, 1, 3, 2, 2, 0, 0, 3, 0, 3, 3, 2, 3, 3, 3, 0, 2, 1, 2, 0, 3, 1, 0, 1, 3, 1, 3, 3, 1, 0, 1, 1, 2, 1, 3, 3, 0, 2, 0, 3, 2, 0, 0, 2, 1, 2, 3, 3, 2, 1, 2, 2, 3, 1, 2, 3, 3, 1, 1, 2, 1, 2, 0, 2, 0, 3, 0, 0, 0, 3, 3, 0, 2, 2, 3, 1, 3, 0, 1, 2, 0, 0, 0, 3, 2, 0, 2, 2, 1, 0, 2, 0, 3, 3, 2, 3, 0, 3, 2, 3, 2, 2, 1, 0, 3, 3, 1, 0, 0, 2, 1, 2, 3, 3, 3, 3, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 3, 3, 1, 3, 3, 2, 3, 3, 1, 1, 3, 0, 3, 3, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 0, 0, 3, 0, 0, 3, 2, 1, 0, 2, 1, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2, 3, 0, 0, 3, 3, 1, 0, 1, 0, 0, 3, 3, 3, 0, 2, 2, 1, 2, 2, 2, 0, 3, 1, 1, 3, 3, 1, 2, 3, 2, 2, 0, 3, 3, 0, 2, 2, 0, 3, 3, 0, 1, 1, 0, 1, 2, 1, 3, 1, 0, 0, 3, 2, 0, 1, 0, 1, 2, 1, 1, 2, 2, 0, 3, 2, 3, 1, 2, 3, 3, 2, 2, 2, 3, 0, 0, 1, 0, 3, 0, 0, 2, 1, 1, 1, 3, 0, 1, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 3, 0, 0, 1, 1, 1, 0, 3, 2, 0, 1, 3, 0, 3, 1, 0, 3, 3, 1, 0, 0, 0, 3, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 3, 3, 2, 0, 0, 0, 2, 3, 0, 1, 3, 1, 2, 0, 0, 2, 0, 2, 0, 3, 3, 1, 3, 3, 1, 0, 0, 0, 0, 1, 1, 2, 2, 2, 1, 2, 0, 0, 3, 0, 1, 1, 0, 3, 2, 0, 1, 2, 3, 2, 3, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 2, 3, 1, 2, 0, 1, 0, 3, 0, 0, 0, 0, 1, 3, 0, 1, 1, 0, 3, 3, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 1, 2, 2, 2, 0, 0, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0, 0, 2, 3, 3, 1, 3, 3, 1, 3, 2, 0, 2, 2, 3, 3, 3, 3, 1, 2, 2, 1, 2, 0, 2, 3, 2, 2, 1, 0, 1, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 2, 1, 1, 3, 0, 1, 3, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0, 3, 1, 3, 2, 2, 3, 2, 0, 3, 3, 1, 0, 0, 3, 2, 2, 0, 3, 2, 0, 3, 2, 0, 1, 2, 2, 1, 0, 2, 0, 2, 1, 2, 1, 1, 3, 0, 1, 1, 2, 0, 3, 1, 2, 2, 1, 3, 2, 0, 2, 2, 2, 3, 0, 0, 3, 2, 1, 2, 0, 0, 2, 3, 2, 0, 0, 0, 2, 3, 0, 2, 2, 1, 1, 1, 2, 1, 0, 2, 1, 3, 3, 0, 0, 2, 3, 1, 3, 1, 0, 3, 3, 0, 1, 3, 1, 1, 2, 3, 0, 3, 0, 1, 1, 3, 2, 0, 0, 3, 1, 3, 1, 2, 2, 3, 0, 3, 1, 1, 0, 3, 0, 2, 0, 2, 2, 3, 2, 0, 3, 1, 0, 3, 3, 1, 1, 0, 3, 1, 3, 1, 1, 2, 0, 1, 3, 3, 3, 3, 1, 3, 0, 1, 1, 0, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 1, 2, 1, 2, 3, 2, 0, 3, 3, 1, 2, 3, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 0, 2, 0, 2, 2, 0, 3, 1, 0, 3, 0, 1, 3, 3, 0, 0, 1, 1, 3, 2, 3, 0, 2, 1, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 2, 2, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 3, 0, 0, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 3, 0, 3, 0, 1, 0, 0, 0, 0, 3, 2, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2, 1, 3, 3, 0, 3, 2, 0, 1, 2, 3, 3, 0, 1, 1, 1, 1, 3, 1, 2, 0, 2, 3, 1, 3, 1, 1, 2, 0, 1, 2, 3, 0, 1, 3, 0, 0, 0, 0, 0, 3, 1, 2, 2, 2, 1, 1, 3, 0, 1, 2, 2, 3, 1, 1, 3, 0, 1, 0, 1, 3, 3, 3, 3, 1, 1, 0, 1, 2, 2, 3, 1, 1, 0, 3, 3, 0, 2, 2, 1, 2, 2, 0, 2, 3, 3, 3, 2, 2, 3, 2, 2, 1, 1, 1, 2, 0, 1, 0, 3, 2, 0, 2, 2, 2, 2, 1, 0, 0, 0, 1, 2, 1, 0, 3, 2, 0, 3, 2, 0, 1, 2, 1, 0, 0, 0, 2, 0, 3, 1, 1, 2, 3, 3, 2, 1, 2, 1, 2, 3, 0, 2, 3, 1, 0, 3, 1, 0, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 0, 0, 3, 2, 1, 0, 3, 3, 1, 0, 3, 2, 1, 2, 3, 2, 2, 3, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 3, 0, 3, 1, 1, 3, 3, 1, 1, 1, 3, 2, 1, 3, 1, 1, 2, 3, 0, 1, 1, 3, 2, 1, 2, 2, 3, 2, 1, 1, 1, 3, 3, 3, 0, 3, 2, 3, 1, 3, 2, 1, 2, 3, 3, 0, 0, 0, 2, 2, 0, 2, 0, 2, 1, 3, 3, 0, 1, 2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 3, 1, 3, 0, 0, 2, 2, 3, 3, 1, 2, 1, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 3, 0, 1, 3, 0, 2, 3, 3, 2, 0, 3, 1, 1, 0, 2, 2, 2, 0, 3, 3, 0, 3, 2, 3, 0, 2, 0, 2, 1, 3, 3, 0, 3, 2, 0, 2, 1, 2, 3, 1, 1, 3, 0, 1, 1, 3, 0, 1, 0, 1, 2, 0, 1, 3, 3, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 3, 3, 1, 0, 2, 0, 3, 1, 2, 2, 1, 2, 3, 3, 0, 0, 1, 2, 2, 2, 1, 2, 1, 0, 2, 3, 0, 0, 3, 0, 3, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 3, 2, 0, 0, 0, 0, 3, 0, 2, 3, 2, 3, 3, 2, 3, 1, 0, 2, 0, 0, 1, 1, 1, 2, 2, 0, 0, 3, 0, 1, 2, 0, 3, 1, 0, 0, 3, 0, 1, 0, 3, 3, 0, 1, 2, 2, 2, 3, 0, 1, 3, 3, 1, 3, 0, 1, 0, 0, 2, 1, 0, 2, 0, 3, 1, 0, 2, 1, 2, 1, 3, 0, 2, 3, 1, 1, 1, 2, 1, 3, 2, 2, 2, 2, 3, 1, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 3, 0, 3, 0, 3, 3, 1, 2, 0, 2, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 3, 1, 3, 0, 2, 1, 1, 1, 1, 2, 2, 1, 3, 1, 0, 1, 1, 0, 3, 0, 3, 3, 0, 2, 3, 0, 3]}], \"hovertemplate\": \"%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<br>price_range=%{marker.color}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": [1, 2, 2, 2, 1, 1, 3, 0, 0, 0, 3, 3, 1, 2, 0, 0, 3, 3, 1, 1, 3, 3, 1, 0, 1, 2, 3, 3, 2, 0, 3, 0, 1, 3, 0, 1, 1, 3, 2, 2, 2, 3, 1, 1, 0, 1, 3, 1, 0, 3, 2, 2, 3, 1, 2, 2, 0, 3, 1, 2, 1, 0, 0, 2, 2, 3, 3, 3, 0, 3, 3, 2, 0, 3, 0, 1, 0, 0, 3, 1, 3, 2, 3, 1, 1, 1, 0, 3, 2, 2, 3, 2, 0, 0, 1, 3, 0, 2, 0, 1, 3, 1, 1, 0, 0, 1, 3, 3, 3, 3, 1, 1, 0, 2, 3, 3, 2, 1, 0, 1, 2, 3, 3, 3, 2, 3, 2, 1, 3, 0, 3, 1, 2, 1, 2, 2, 1, 3, 0, 0, 2, 0, 3, 2, 0, 3, 1, 3, 2, 2, 1, 3, 1, 2, 0, 1, 0, 0, 3, 1, 2, 2, 0, 0, 2, 3, 1, 2, 3, 1, 3, 1, 2, 2, 3, 2, 0, 3, 2, 1, 0, 0, 2, 3, 3, 0, 2, 3, 1, 3, 1, 2, 3, 3, 2, 2, 2, 3, 1, 1, 0, 1, 0, 2, 2, 2, 2, 1, 3, 3, 2, 2, 0, 2, 0, 3, 0, 1, 1, 3, 0, 0, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 0, 1, 2, 0, 1, 3, 1, 0, 2, 2, 3, 2, 3, 3, 0, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 3, 1, 1, 0, 0, 3, 1, 3, 1, 2, 1, 0, 0, 3, 1, 2, 2, 3, 1, 0, 0, 3, 3, 1, 1, 2, 1, 3, 3, 1, 1, 3, 3, 2, 3, 0, 1, 3, 1, 0, 1, 3, 0, 0, 3, 0, 1, 1, 2, 0, 2, 3, 2, 3, 0, 2, 3, 1, 0, 0, 1, 0, 3, 3, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 0, 2, 2, 2, 1, 2, 3, 0, 1, 2, 2, 2, 2, 1, 3, 0, 2, 2, 1, 2, 0, 0, 3, 1, 0, 3, 2, 0, 0, 2, 3, 2, 1, 3, 1, 3, 0, 3, 2, 0, 1, 0, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 3, 3, 1, 3, 3, 1, 3, 3, 0, 1, 3, 2, 3, 3, 2, 2, 3, 3, 0, 2, 2, 0, 3, 1, 2, 3, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 3, 1, 1, 3, 1, 3, 1, 0, 0, 3, 1, 3, 0, 0, 1, 3, 1, 0, 0, 2, 2, 2, 1, 1, 1, 3, 0, 3, 2, 1, 3, 1, 0, 2, 2, 1, 0, 0, 3, 3, 0, 0, 1, 3, 3, 1, 0, 2, 2, 3, 0, 3, 3, 0, 3, 0, 3, 1, 0, 2, 2, 1, 2, 0, 3, 2, 0, 1, 2, 3, 1, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 2, 2, 3, 3, 0, 1, 1, 2, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 0, 0, 3, 1, 2, 2, 0, 2, 2, 1, 0, 3, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 0, 1, 0, 0, 2, 2, 2, 0, 3, 1, 2, 1, 2, 0, 0, 1, 3, 3, 3, 3, 3, 3, 0, 2, 2, 0, 0, 3, 3, 3, 0, 0, 3, 0, 0, 1, 2, 0, 2, 3, 1, 3, 2, 0, 2, 2, 0, 0, 1, 3, 0, 1, 1, 2, 2, 0, 2, 2, 0, 1, 3, 0, 2, 2, 3, 1, 3, 2, 2, 1, 2, 2, 1, 1, 0, 1, 0, 1, 0, 3, 1, 1, 3, 1, 3, 2, 1, 1, 0, 2, 0, 2, 0, 2, 0, 1, 3, 1, 2, 2, 0, 1, 0, 3, 2, 1, 3, 2, 0, 0, 1, 2, 0, 0, 0, 2, 3, 1, 2, 1, 0, 0, 0, 2, 3, 2, 3, 0, 3, 2, 1, 2, 1, 0, 2, 0, 0, 1, 3, 0, 3, 1, 1, 1, 3, 2, 2, 0, 0, 3, 0, 3, 3, 2, 3, 3, 3, 0, 2, 1, 2, 0, 3, 1, 0, 1, 3, 1, 3, 3, 1, 0, 1, 1, 2, 1, 3, 3, 0, 2, 0, 3, 2, 0, 0, 2, 1, 2, 3, 3, 2, 1, 2, 2, 3, 1, 2, 3, 3, 1, 1, 2, 1, 2, 0, 2, 0, 3, 0, 0, 0, 3, 3, 0, 2, 2, 3, 1, 3, 0, 1, 2, 0, 0, 0, 3, 2, 0, 2, 2, 1, 0, 2, 0, 3, 3, 2, 3, 0, 3, 2, 3, 2, 2, 1, 0, 3, 3, 1, 0, 0, 2, 1, 2, 3, 3, 3, 3, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 3, 3, 1, 3, 3, 2, 3, 3, 1, 1, 3, 0, 3, 3, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 0, 0, 3, 0, 0, 3, 2, 1, 0, 2, 1, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2, 3, 0, 0, 3, 3, 1, 0, 1, 0, 0, 3, 3, 3, 0, 2, 2, 1, 2, 2, 2, 0, 3, 1, 1, 3, 3, 1, 2, 3, 2, 2, 0, 3, 3, 0, 2, 2, 0, 3, 3, 0, 1, 1, 0, 1, 2, 1, 3, 1, 0, 0, 3, 2, 0, 1, 0, 1, 2, 1, 1, 2, 2, 0, 3, 2, 3, 1, 2, 3, 3, 2, 2, 2, 3, 0, 0, 1, 0, 3, 0, 0, 2, 1, 1, 1, 3, 0, 1, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 3, 0, 0, 1, 1, 1, 0, 3, 2, 0, 1, 3, 0, 3, 1, 0, 3, 3, 1, 0, 0, 0, 3, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 3, 3, 2, 0, 0, 0, 2, 3, 0, 1, 3, 1, 2, 0, 0, 2, 0, 2, 0, 3, 3, 1, 3, 3, 1, 0, 0, 0, 0, 1, 1, 2, 2, 2, 1, 2, 0, 0, 3, 0, 1, 1, 0, 3, 2, 0, 1, 2, 3, 2, 3, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 2, 3, 1, 2, 0, 1, 0, 3, 0, 0, 0, 0, 1, 3, 0, 1, 1, 0, 3, 3, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 1, 2, 2, 2, 0, 0, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0, 0, 2, 3, 3, 1, 3, 3, 1, 3, 2, 0, 2, 2, 3, 3, 3, 3, 1, 2, 2, 1, 2, 0, 2, 3, 2, 2, 1, 0, 1, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 2, 1, 1, 3, 0, 1, 3, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0, 3, 1, 3, 2, 2, 3, 2, 0, 3, 3, 1, 0, 0, 3, 2, 2, 0, 3, 2, 0, 3, 2, 0, 1, 2, 2, 1, 0, 2, 0, 2, 1, 2, 1, 1, 3, 0, 1, 1, 2, 0, 3, 1, 2, 2, 1, 3, 2, 0, 2, 2, 2, 3, 0, 0, 3, 2, 1, 2, 0, 0, 2, 3, 2, 0, 0, 0, 2, 3, 0, 2, 2, 1, 1, 1, 2, 1, 0, 2, 1, 3, 3, 0, 0, 2, 3, 1, 3, 1, 0, 3, 3, 0, 1, 3, 1, 1, 2, 3, 0, 3, 0, 1, 1, 3, 2, 0, 0, 3, 1, 3, 1, 2, 2, 3, 0, 3, 1, 1, 0, 3, 0, 2, 0, 2, 2, 3, 2, 0, 3, 1, 0, 3, 3, 1, 1, 0, 3, 1, 3, 1, 1, 2, 0, 1, 3, 3, 3, 3, 1, 3, 0, 1, 1, 0, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 1, 2, 1, 2, 3, 2, 0, 3, 3, 1, 2, 3, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 0, 2, 0, 2, 2, 0, 3, 1, 0, 3, 0, 1, 3, 3, 0, 0, 1, 1, 3, 2, 3, 0, 2, 1, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 2, 2, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 3, 0, 0, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 3, 0, 3, 0, 1, 0, 0, 0, 0, 3, 2, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2, 1, 3, 3, 0, 3, 2, 0, 1, 2, 3, 3, 0, 1, 1, 1, 1, 3, 1, 2, 0, 2, 3, 1, 3, 1, 1, 2, 0, 1, 2, 3, 0, 1, 3, 0, 0, 0, 0, 0, 3, 1, 2, 2, 2, 1, 1, 3, 0, 1, 2, 2, 3, 1, 1, 3, 0, 1, 0, 1, 3, 3, 3, 3, 1, 1, 0, 1, 2, 2, 3, 1, 1, 0, 3, 3, 0, 2, 2, 1, 2, 2, 0, 2, 3, 3, 3, 2, 2, 3, 2, 2, 1, 1, 1, 2, 0, 1, 0, 3, 2, 0, 2, 2, 2, 2, 1, 0, 0, 0, 1, 2, 1, 0, 3, 2, 0, 3, 2, 0, 1, 2, 1, 0, 0, 0, 2, 0, 3, 1, 1, 2, 3, 3, 2, 1, 2, 1, 2, 3, 0, 2, 3, 1, 0, 3, 1, 0, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 0, 0, 3, 2, 1, 0, 3, 3, 1, 0, 3, 2, 1, 2, 3, 2, 2, 3, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 3, 0, 3, 1, 1, 3, 3, 1, 1, 1, 3, 2, 1, 3, 1, 1, 2, 3, 0, 1, 1, 3, 2, 1, 2, 2, 3, 2, 1, 1, 1, 3, 3, 3, 0, 3, 2, 3, 1, 3, 2, 1, 2, 3, 3, 0, 0, 0, 2, 2, 0, 2, 0, 2, 1, 3, 3, 0, 1, 2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 3, 1, 3, 0, 0, 2, 2, 3, 3, 1, 2, 1, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 3, 0, 1, 3, 0, 2, 3, 3, 2, 0, 3, 1, 1, 0, 2, 2, 2, 0, 3, 3, 0, 3, 2, 3, 0, 2, 0, 2, 1, 3, 3, 0, 3, 2, 0, 2, 1, 2, 3, 1, 1, 3, 0, 1, 1, 3, 0, 1, 0, 1, 2, 0, 1, 3, 3, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 3, 3, 1, 0, 2, 0, 3, 1, 2, 2, 1, 2, 3, 3, 0, 0, 1, 2, 2, 2, 1, 2, 1, 0, 2, 3, 0, 0, 3, 0, 3, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 3, 2, 0, 0, 0, 0, 3, 0, 2, 3, 2, 3, 3, 2, 3, 1, 0, 2, 0, 0, 1, 1, 1, 2, 2, 0, 0, 3, 0, 1, 2, 0, 3, 1, 0, 0, 3, 0, 1, 0, 3, 3, 0, 1, 2, 2, 2, 3, 0, 1, 3, 3, 1, 3, 0, 1, 0, 0, 2, 1, 0, 2, 0, 3, 1, 0, 2, 1, 2, 1, 3, 0, 2, 3, 1, 1, 1, 2, 1, 3, 2, 2, 2, 2, 3, 1, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 3, 0, 3, 0, 3, 3, 1, 2, 0, 2, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 3, 1, 3, 0, 2, 1, 1, 1, 1, 2, 2, 1, 3, 1, 0, 1, 1, 0, 3, 0, 3, 3, 0, 2, 3, 0, 3], \"coloraxis\": \"coloraxis\", \"symbol\": \"circle\"}, \"name\": \"\", \"showlegend\": false, \"type\": \"splom\"}],                        {\"coloraxis\": {\"colorbar\": {\"title\": {\"text\": \"price_range\"}}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"dragmode\": \"select\", \"height\": 2000, \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 2000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5aafa0d3-4df8-4d01-bd8a-b67026e66e7c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.scatter_matrix(train, color = 'price_range', width = 2000, height = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mathematical-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 21 columns):\n",
      "battery_power    2000 non-null int64\n",
      "blue             2000 non-null int64\n",
      "clock_speed      2000 non-null float64\n",
      "dual_sim         2000 non-null int64\n",
      "fc               2000 non-null int64\n",
      "four_g           2000 non-null int64\n",
      "int_memory       2000 non-null int64\n",
      "m_dep            2000 non-null float64\n",
      "mobile_wt        2000 non-null int64\n",
      "n_cores          2000 non-null int64\n",
      "pc               2000 non-null int64\n",
      "px_height        2000 non-null int64\n",
      "px_width         2000 non-null int64\n",
      "ram              2000 non-null int64\n",
      "sc_h             2000 non-null int64\n",
      "sc_w             2000 non-null int64\n",
      "talk_time        2000 non-null int64\n",
      "three_g          2000 non-null int64\n",
      "touch_screen     2000 non-null int64\n",
      "wifi             2000 non-null int64\n",
      "price_range      2000 non-null int64\n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 328.2 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handled-zealand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1238.518500</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>1.522250</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>4.309500</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>32.046500</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>140.249000</td>\n",
       "      <td>4.520500</td>\n",
       "      <td>...</td>\n",
       "      <td>645.108000</td>\n",
       "      <td>1251.515500</td>\n",
       "      <td>2124.213000</td>\n",
       "      <td>12.306500</td>\n",
       "      <td>5.767000</td>\n",
       "      <td>11.011000</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>439.418206</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.816004</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>4.341444</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>18.145715</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>35.399655</td>\n",
       "      <td>2.287837</td>\n",
       "      <td>...</td>\n",
       "      <td>443.780811</td>\n",
       "      <td>432.199447</td>\n",
       "      <td>1084.732044</td>\n",
       "      <td>4.213245</td>\n",
       "      <td>4.356398</td>\n",
       "      <td>5.463955</td>\n",
       "      <td>0.426273</td>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.500076</td>\n",
       "      <td>1.118314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>501.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>851.750000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282.750000</td>\n",
       "      <td>874.750000</td>\n",
       "      <td>1207.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1226.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>2146.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1615.250000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>947.250000</td>\n",
       "      <td>1633.000000</td>\n",
       "      <td>3064.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1998.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>3998.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       battery_power       blue  clock_speed     dual_sim           fc  \\\n",
       "count    2000.000000  2000.0000  2000.000000  2000.000000  2000.000000   \n",
       "mean     1238.518500     0.4950     1.522250     0.509500     4.309500   \n",
       "std       439.418206     0.5001     0.816004     0.500035     4.341444   \n",
       "min       501.000000     0.0000     0.500000     0.000000     0.000000   \n",
       "25%       851.750000     0.0000     0.700000     0.000000     1.000000   \n",
       "50%      1226.000000     0.0000     1.500000     1.000000     3.000000   \n",
       "75%      1615.250000     1.0000     2.200000     1.000000     7.000000   \n",
       "max      1998.000000     1.0000     3.000000     1.000000    19.000000   \n",
       "\n",
       "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.521500    32.046500     0.501750   140.249000     4.520500  ...   \n",
       "std       0.499662    18.145715     0.288416    35.399655     2.287837  ...   \n",
       "min       0.000000     2.000000     0.100000    80.000000     1.000000  ...   \n",
       "25%       0.000000    16.000000     0.200000   109.000000     3.000000  ...   \n",
       "50%       1.000000    32.000000     0.500000   141.000000     4.000000  ...   \n",
       "75%       1.000000    48.000000     0.800000   170.000000     7.000000  ...   \n",
       "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
       "\n",
       "         px_height     px_width          ram         sc_h         sc_w  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    645.108000  1251.515500  2124.213000    12.306500     5.767000   \n",
       "std     443.780811   432.199447  1084.732044     4.213245     4.356398   \n",
       "min       0.000000   500.000000   256.000000     5.000000     0.000000   \n",
       "25%     282.750000   874.750000  1207.500000     9.000000     2.000000   \n",
       "50%     564.000000  1247.000000  2146.500000    12.000000     5.000000   \n",
       "75%     947.250000  1633.000000  3064.500000    16.000000     9.000000   \n",
       "max    1960.000000  1998.000000  3998.000000    19.000000    18.000000   \n",
       "\n",
       "         talk_time      three_g  touch_screen         wifi  price_range  \n",
       "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
       "mean     11.011000     0.761500      0.503000     0.507000     1.500000  \n",
       "std       5.463955     0.426273      0.500116     0.500076     1.118314  \n",
       "min       2.000000     0.000000      0.000000     0.000000     0.000000  \n",
       "25%       6.000000     1.000000      0.000000     0.000000     0.750000  \n",
       "50%      11.000000     1.000000      1.000000     1.000000     1.500000  \n",
       "75%      16.000000     1.000000      1.000000     1.000000     2.250000  \n",
       "max      20.000000     1.000000      1.000000     1.000000     3.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "soviet-alcohol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      "id               1000 non-null int64\n",
      "battery_power    1000 non-null int64\n",
      "blue             1000 non-null int64\n",
      "clock_speed      1000 non-null float64\n",
      "dual_sim         1000 non-null int64\n",
      "fc               1000 non-null int64\n",
      "four_g           1000 non-null int64\n",
      "int_memory       1000 non-null int64\n",
      "m_dep            1000 non-null float64\n",
      "mobile_wt        1000 non-null int64\n",
      "n_cores          1000 non-null int64\n",
      "pc               1000 non-null int64\n",
      "px_height        1000 non-null int64\n",
      "px_width         1000 non-null int64\n",
      "ram              1000 non-null int64\n",
      "sc_h             1000 non-null int64\n",
      "sc_w             1000 non-null int64\n",
      "talk_time        1000 non-null int64\n",
      "three_g          1000 non-null int64\n",
      "touch_screen     1000 non-null int64\n",
      "wifi             1000 non-null int64\n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 164.2 KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "international-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>...</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>500.500000</td>\n",
       "      <td>1248.510000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>1.540900</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>4.593000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>33.652000</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>139.51100</td>\n",
       "      <td>...</td>\n",
       "      <td>10.054000</td>\n",
       "      <td>627.121000</td>\n",
       "      <td>1239.774000</td>\n",
       "      <td>2138.998000</td>\n",
       "      <td>11.995000</td>\n",
       "      <td>5.316000</td>\n",
       "      <td>11.085000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.819436</td>\n",
       "      <td>432.458227</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.499961</td>\n",
       "      <td>4.463325</td>\n",
       "      <td>0.500081</td>\n",
       "      <td>18.128694</td>\n",
       "      <td>0.280861</td>\n",
       "      <td>34.85155</td>\n",
       "      <td>...</td>\n",
       "      <td>6.095099</td>\n",
       "      <td>432.929699</td>\n",
       "      <td>439.670981</td>\n",
       "      <td>1088.092278</td>\n",
       "      <td>4.320607</td>\n",
       "      <td>4.240062</td>\n",
       "      <td>5.497636</td>\n",
       "      <td>0.429708</td>\n",
       "      <td>0.50025</td>\n",
       "      <td>0.500201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.750000</td>\n",
       "      <td>895.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>109.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>263.750000</td>\n",
       "      <td>831.750000</td>\n",
       "      <td>1237.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>500.500000</td>\n",
       "      <td>1246.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>139.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>564.500000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>2153.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>750.250000</td>\n",
       "      <td>1629.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1637.750000</td>\n",
       "      <td>3065.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1907.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>3989.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  battery_power         blue  clock_speed     dual_sim  \\\n",
       "count  1000.000000    1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean    500.500000    1248.510000     0.516000     1.540900     0.517000   \n",
       "std     288.819436     432.458227     0.499994     0.829268     0.499961   \n",
       "min       1.000000     500.000000     0.000000     0.500000     0.000000   \n",
       "25%     250.750000     895.000000     0.000000     0.700000     0.000000   \n",
       "50%     500.500000    1246.500000     1.000000     1.500000     1.000000   \n",
       "75%     750.250000    1629.250000     1.000000     2.300000     1.000000   \n",
       "max    1000.000000    1999.000000     1.000000     3.000000     1.000000   \n",
       "\n",
       "                fc       four_g   int_memory        m_dep   mobile_wt  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.00000  ...   \n",
       "mean      4.593000     0.487000    33.652000     0.517500   139.51100  ...   \n",
       "std       4.463325     0.500081    18.128694     0.280861    34.85155  ...   \n",
       "min       0.000000     0.000000     2.000000     0.100000    80.00000  ...   \n",
       "25%       1.000000     0.000000    18.000000     0.300000   109.75000  ...   \n",
       "50%       3.000000     0.000000    34.500000     0.500000   139.00000  ...   \n",
       "75%       7.000000     1.000000    49.000000     0.800000   170.00000  ...   \n",
       "max      19.000000     1.000000    64.000000     1.000000   200.00000  ...   \n",
       "\n",
       "                pc    px_height     px_width          ram         sc_h  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     10.054000   627.121000  1239.774000  2138.998000    11.995000   \n",
       "std       6.095099   432.929699   439.670981  1088.092278     4.320607   \n",
       "min       0.000000     0.000000   501.000000   263.000000     5.000000   \n",
       "25%       5.000000   263.750000   831.750000  1237.250000     8.000000   \n",
       "50%      10.000000   564.500000  1250.000000  2153.500000    12.000000   \n",
       "75%      16.000000   903.000000  1637.750000  3065.500000    16.000000   \n",
       "max      20.000000  1907.000000  1998.000000  3989.000000    19.000000   \n",
       "\n",
       "              sc_w    talk_time      three_g  touch_screen         wifi  \n",
       "count  1000.000000  1000.000000  1000.000000    1000.00000  1000.000000  \n",
       "mean      5.316000    11.085000     0.756000       0.50000     0.507000  \n",
       "std       4.240062     5.497636     0.429708       0.50025     0.500201  \n",
       "min       0.000000     2.000000     0.000000       0.00000     0.000000  \n",
       "25%       2.000000     6.750000     1.000000       0.00000     0.000000  \n",
       "50%       5.000000    11.000000     1.000000       0.50000     1.000000  \n",
       "75%       8.000000    16.000000     1.000000       1.00000     1.000000  \n",
       "max      18.000000    20.000000     1.000000       1.00000     1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "perfect-australian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "blank-union",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
       "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'ram', 'talk_time',\n",
       "       'three_g', 'touch_screen', 'wifi', 'price_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "military-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['px_width','px_height', 'sc_h','sc_w'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "subtle-cylinder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>ram</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2549</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2631</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2603</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2769</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1411</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>668</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2032</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3057</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>869</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3919</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0               842     0          2.2         0   1       0           7   \n",
       "1              1021     1          0.5         1   0       1          53   \n",
       "2               563     1          0.5         1   2       1          41   \n",
       "3               615     1          2.5         0   0       0          10   \n",
       "4              1821     1          1.2         0  13       1          44   \n",
       "...             ...   ...          ...       ...  ..     ...         ...   \n",
       "1995            794     1          0.5         1   0       1           2   \n",
       "1996           1965     1          2.6         1   0       0          39   \n",
       "1997           1911     0          0.9         1   1       1          36   \n",
       "1998           1512     0          0.9         0   4       1          46   \n",
       "1999            510     1          2.0         1   5       1          45   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  pc   ram  talk_time  three_g  touch_screen  \\\n",
       "0       0.6        188        2   2  2549         19        0             0   \n",
       "1       0.7        136        3   6  2631          7        1             1   \n",
       "2       0.9        145        5   6  2603          9        1             1   \n",
       "3       0.8        131        6   9  2769         11        1             0   \n",
       "4       0.6        141        2  14  1411         15        1             1   \n",
       "...     ...        ...      ...  ..   ...        ...      ...           ...   \n",
       "1995    0.8        106        6  14   668         19        1             1   \n",
       "1996    0.2        187        4   3  2032         16        1             1   \n",
       "1997    0.7        108        8   3  3057          5        1             1   \n",
       "1998    0.1        145        5   5   869         19        1             1   \n",
       "1999    0.9        168        6  16  3919          2        1             1   \n",
       "\n",
       "      wifi  price_range  \n",
       "0        1            1  \n",
       "1        0            2  \n",
       "2        0            2  \n",
       "3        0            2  \n",
       "4        0            1  \n",
       "...    ...          ...  \n",
       "1995     0            0  \n",
       "1996     1            2  \n",
       "1997     0            3  \n",
       "1998     1            0  \n",
       "1999     1            3  \n",
       "\n",
       "[2000 rows x 17 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "plain-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Screen diagonal inches'] = np.sqrt(train['sc_h']**2 + train['sc_w']**2) * 0.393701\n",
    "train['Pixels screen'] = np.sqrt(train['px_height']**2 + train['px_width']**2)\n",
    "\n",
    "# we calculate pixels per inch (ppi)\n",
    "train['ppi'] = train['Pixels screen'] / train['Screen diagonal inches']\n",
    "train.drop(['px_height', 'px_width', 'sc_h', 'sc_w', 'Pixels screen'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deadly-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler_features = train.copy()\n",
    "\n",
    "scaler_features = pd.DataFrame(scaler.fit_transform(scaler_features), index = scaler_features.index, \n",
    "                               columns = scaler_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "former-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_dis = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\n",
    "scaler_features = pd.concat([scaler_features, train[col_names_dis]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "specified-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features.drop(['price_range'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "rocky-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = scaler_features.copy()\n",
    "y = train['price_range']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "convinced-practitioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1217\n",
      "[LightGBM] [Info] Number of data points in the train set: 1700, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.398129\n",
      "[LightGBM] [Info] Start training from score -1.379260\n",
      "[LightGBM] [Info] Start training from score -1.369958\n",
      "[LightGBM] [Info] Start training from score -1.398129\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "        \n",
    "params = {}\n",
    "params['learning_rate'] = 0.5\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['max_depth'] = 5\n",
    "params['num_class'] = 4\n",
    "\n",
    "params['objective'] = 'multiclass'\n",
    "params['metric'] ='multi_logloss'\n",
    "\n",
    "model = lgb.train(params, d_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "practical-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "designed-invite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "practical-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred, columns = ['0','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "unauthorized-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3\n",
       "0    1.0  0.0  0.0  0.0\n",
       "1    0.0  0.0  1.0  0.0\n",
       "2    0.0  1.0  0.0  0.0\n",
       "3    0.0  0.0  0.0  1.0\n",
       "4    0.0  1.0  0.0  0.0\n",
       "..   ...  ...  ...  ...\n",
       "295  1.0  0.0  0.0  0.0\n",
       "296  0.0  1.0  0.0  0.0\n",
       "297  1.0  0.0  0.0  0.0\n",
       "298  0.0  1.0  0.0  0.0\n",
       "299  0.0  0.0  0.0  1.0\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "capable-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricerange():\n",
    "    if y_pred['0'] == 1:\n",
    "        return 0\n",
    "    elif y_pred['1'] == 1:\n",
    "        return 1\n",
    "    elif y_pred['2'] == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "automotive-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(len(y_pred)):\n",
    "    for j in y_pred.columns:\n",
    "        if y_pred[j][i] == 1:\n",
    "            prediction.append(int(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "possible-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93        80\n",
      "           1       0.84      0.85      0.84        72\n",
      "           2       0.77      0.81      0.79        68\n",
      "           3       0.91      0.88      0.89        80\n",
      "\n",
      "    accuracy                           0.87       300\n",
      "   macro avg       0.86      0.86      0.86       300\n",
      "weighted avg       0.87      0.87      0.87       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "romance-webcam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3\n",
       "201  0.0  0.0  0.0  0.0\n",
       "236  0.0  0.0  0.0  0.0\n",
       "281  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[(y_pred['0']==0)&(y_pred['1']==0)&(y_pred['2']==0)&(y_pred['3']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "incorporated-blade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581     3\n",
       "76      0\n",
       "1916    1\n",
       "1414    2\n",
       "780     2\n",
       "       ..\n",
       "1130    3\n",
       "1294    0\n",
       "860     2\n",
       "1459    3\n",
       "1126    1\n",
       "Name: price_range, Length: 1700, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "incredible-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg = lgb.LGBMClassifier(**params)\n",
    "lg.fit(X_train,y_train)\n",
    "pred = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "social-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.8s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.3s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.3s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.0s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LGBMClassifier(),\n",
       "             param_grid={'learning_rate': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "                         'max_depth': array([1, 2, 3, 4, 5]),\n",
       "                         'n_estimators': array([ 50,  66,  83, 100, 116, 133, 150, 166, 183, 200])},\n",
       "             scoring='balanced_accuracy', verbose=2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'learning_rate': np.linspace(0.1,1,10),\n",
    "          'n_estimators': np.linspace(50,200,10).astype(int),\n",
    "          'max_depth': np.linspace(1,5,5).astype(int)}\n",
    "\n",
    "model = GridSearchCV(lgb.LGBMClassifier(),  params, scoring = 'balanced_accuracy', cv = 3, verbose = 2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "valuable-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.8, max_depth=1, n_estimators=116)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "appointed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "statutory-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        80\n",
      "           1       0.89      0.88      0.88        72\n",
      "           2       0.86      0.88      0.87        68\n",
      "           3       0.93      0.94      0.93        80\n",
      "\n",
      "    accuracy                           0.91       300\n",
      "   macro avg       0.90      0.90      0.90       300\n",
      "weighted avg       0.91      0.91      0.91       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "sapphire-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9049632352941177"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "hourly-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 961 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# XGBOOST\n",
    "\n",
    "model1 = GridSearchCV(XGBClassifier(), params, cv = 3, verbose = 2)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "params2 = {'n_estimators': np.linspace(50,200,10).astype(int),\n",
    "          'max_depth': np.linspace(1,5,5)}\n",
    "model2 = GridSearchCV(RandomForestClassifier(), params2, cv = 3, verbose = 2)\n",
    "\n",
    "# MLP\n",
    "\n",
    "params3 = {'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "           'learning_rate_init': np.linspace(0.001,0.005,5)}\n",
    "model3 = GridSearchCV(MLPClassifier(), params3, cv = 3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "latin-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[12:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[12:37:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:37:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:37:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:37:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:37:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:37:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:37:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:37:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:37:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:37:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:37:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:37:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:37:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:37:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:37:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:37:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.4s\n",
      "[12:37:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:37:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=1, n_estimators=200; total time=   0.2s\n",
      "[12:37:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[12:37:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[12:37:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=50; total time=   0.0s\n",
      "[12:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.2s\n",
      "[12:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=66; total time=   0.2s\n",
      "[12:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:37:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:37:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=2, n_estimators=83; total time=   0.1s\n",
      "[12:37:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:37:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:37:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:37:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:37:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.2s\n",
      "[12:37:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=133; total time=   0.2s\n",
      "[12:37:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:37:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.4s\n",
      "[12:37:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=150; total time=   0.4s\n",
      "[12:37:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.5s\n",
      "[12:37:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.5s\n",
      "[12:37:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:37:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.3s\n",
      "[12:37:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.3s\n",
      "[12:37:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=183; total time=   0.5s\n",
      "[12:37:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.6s\n",
      "[12:37:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:37:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=2, n_estimators=200; total time=   0.3s\n",
      "[12:37:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[12:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[12:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=66; total time=   0.1s\n",
      "[12:37:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:37:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:37:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:37:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:37:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:37:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:37:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:37:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:37:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:37:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.5s\n",
      "[12:37:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.5s\n",
      "[12:37:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=133; total time=   0.5s\n",
      "[12:37:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:37:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:37:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:37:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.6s\n",
      "[12:37:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.6s\n",
      "[12:37:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:37:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:37:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:37:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:37:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:37:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:37:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.8s\n",
      "[12:37:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:37:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.3s\n",
      "[12:37:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:37:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.3s\n",
      "[12:37:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:37:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:37:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:37:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:37:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=4, n_estimators=83; total time=   0.4s\n",
      "[12:37:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.5s\n",
      "[12:37:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:37:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:37:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.5s\n",
      "[12:37:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.6s\n",
      "[12:37:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:37:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:38:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:38:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=133; total time=   0.7s\n",
      "[12:38:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:38:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=150; total time=   0.7s\n",
      "[12:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.7s\n",
      "[12:38:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:38:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=166; total time=   0.7s\n",
      "[12:38:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.8s\n",
      "[12:38:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.6s\n",
      "[12:38:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=183; total time=   0.8s\n",
      "[12:38:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.8s\n",
      "[12:38:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.7s\n",
      "[12:38:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=200; total time=   0.8s\n",
      "[12:38:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:38:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:38:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:38:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:38:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:38:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=66; total time=   0.4s\n",
      "[12:38:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.5s\n",
      "[12:38:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.5s\n",
      "[12:38:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=83; total time=   0.4s\n",
      "[12:38:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:38:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:38:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:38:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:38:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:38:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:38:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.6s\n",
      "[12:38:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.6s\n",
      "[12:38:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=133; total time=   0.6s\n",
      "[12:38:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.7s\n",
      "[12:38:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[12:38:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[12:38:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.7s\n",
      "[12:38:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.7s\n",
      "[12:38:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=166; total time=   0.7s\n",
      "[12:38:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.9s\n",
      "[12:38:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.8s\n",
      "[12:38:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=183; total time=   0.8s\n",
      "[12:38:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.9s\n",
      "[12:38:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
      "[12:38:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.9s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:38:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:38:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:38:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:38:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:38:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:38:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:38:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:38:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:38:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:38:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:38:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:38:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:38:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:38:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:38:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:38:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:38:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:38:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:38:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:38:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:38:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:38:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:38:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:38:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:38:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:38:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:38:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:38:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:38:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:38:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:38:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:38:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:38:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:38:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:38:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:38:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:38:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:38:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:38:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:38:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:38:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:38:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:38:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:38:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:38:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:38:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:38:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.3s\n",
      "[12:38:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:38:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:38:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:38:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:38:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:38:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:38:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:38:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:38:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:38:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:38:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:38:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:38:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:38:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:38:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:38:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:38:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:39:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:39:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.6s\n",
      "[12:39:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=183; total time=   0.7s\n",
      "[12:39:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
      "[12:39:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.6s\n",
      "[12:39:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.6s\n",
      "[12:39:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:39:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:39:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:39:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:39:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:39:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:39:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:39:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.4s\n",
      "[12:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.4s\n",
      "[12:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=100; total time=   0.4s\n",
      "[12:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:39:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:39:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:39:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:39:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:39:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:39:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:39:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:39:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=150; total time=   0.6s\n",
      "[12:39:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:39:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:39:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:39:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.7s\n",
      "[12:39:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.7s\n",
      "[12:39:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=183; total time=   0.8s\n",
      "[12:39:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.7s\n",
      "[12:39:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.7s\n",
      "[12:39:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   0.7s\n",
      "[12:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:39:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:39:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:39:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:39:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:39:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:39:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:39:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:39:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:39:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:39:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:39:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:39:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:39:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:39:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:39:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[12:39:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[12:39:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[12:39:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[12:39:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[12:39:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=166; total time=   0.8s\n",
      "[12:39:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.8s\n",
      "[12:39:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.7s\n",
      "[12:39:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:39:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[12:39:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[12:39:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[12:39:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:39:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:39:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:39:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:39:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:39:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:39:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:39:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:39:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:39:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:39:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:39:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:39:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:39:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:39:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:39:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:39:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:39:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:39:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:39:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:39:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:39:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:39:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:39:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:39:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:39:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:39:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:39:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:39:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:39:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:39:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:39:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:39:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:39:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:39:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:39:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:39:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:39:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:39:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:39:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:39:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:39:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:39:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:39:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:39:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:39:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:39:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:39:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:39:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:39:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:40:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:40:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:40:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.4s\n",
      "[12:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.4s\n",
      "[12:40:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=116; total time=   0.4s\n",
      "[12:40:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:40:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:40:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:40:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:40:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:40:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:40:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:40:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:40:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:40:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:40:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:40:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:40:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:40:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:40:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:40:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:40:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:40:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:40:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:40:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:40:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:40:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:40:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:40:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:40:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:40:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:40:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:40:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:40:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:40:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:40:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:40:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.7s\n",
      "[12:40:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:40:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.6s\n",
      "[12:40:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.6s\n",
      "[12:40:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=183; total time=   0.6s\n",
      "[12:40:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[12:40:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[12:40:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[12:40:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:40:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:40:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:40:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:40:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:40:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:40:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:40:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:40:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:40:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:40:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:40:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:40:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:40:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:40:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:40:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:40:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.7s\n",
      "[12:40:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.7s\n",
      "[12:40:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[12:40:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:40:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:40:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:40:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:40:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:40:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.30000000000000004, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:40:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:40:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:40:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:40:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.0s\n",
      "[12:40:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:40:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:40:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:40:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:40:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:40:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:40:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:40:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:40:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:40:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:40:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:40:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:40:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:40:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:40:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:40:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:40:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:40:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.3s\n",
      "[12:40:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.3s\n",
      "[12:40:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=100; total time=   0.3s\n",
      "[12:40:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:40:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:40:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:40:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:40:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:40:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:40:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:40:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:40:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:40:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:40:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:40:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:40:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:40:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:41:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:41:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:41:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:41:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:41:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:41:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:41:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:41:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:41:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:41:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:41:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:41:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:41:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:41:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:41:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:41:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:41:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.4s\n",
      "[12:41:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:41:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:41:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:41:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:41:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:41:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:41:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:41:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=150; total time=   0.5s\n",
      "[12:41:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:41:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:41:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:41:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:41:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:41:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:41:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:41:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:41:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:41:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:41:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:41:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:41:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:41:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:41:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:41:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:41:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:41:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:41:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:41:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:41:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:41:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.5s\n",
      "[12:41:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:41:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:41:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:41:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:41:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:41:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:41:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:41:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:41:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:41:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:41:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[12:41:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:41:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:41:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:41:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:41:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:41:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:41:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:41:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:41:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:41:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:41:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.4, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:41:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:41:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:41:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:41:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:41:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:41:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:41:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:41:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:41:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:41:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:41:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[12:41:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[12:41:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[12:41:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.7s\n",
      "[12:41:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:41:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:41:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:41:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:41:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.4, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:41:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:41:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:41:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:41:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:41:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:41:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:41:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:41:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:41:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:41:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:41:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:41:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:41:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:41:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:41:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:41:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:41:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:41:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:41:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:41:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:41:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:41:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:41:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:41:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:41:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.4s\n",
      "[12:41:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.4s\n",
      "[12:41:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:42:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:42:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:42:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:42:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:42:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:42:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:42:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:42:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:42:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.3s\n",
      "[12:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:42:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:42:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:42:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:42:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:42:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:42:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:42:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:42:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:42:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:42:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:42:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:42:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:42:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:42:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:42:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:42:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=166; total time=   0.5s\n",
      "[12:42:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:42:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:42:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:42:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:42:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[12:42:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:42:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:42:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:42:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:42:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:42:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:42:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:42:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:42:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:42:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:42:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:42:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:42:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:42:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:42:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:42:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:42:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:42:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:42:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:42:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:42:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:42:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:42:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=183; total time=   0.6s\n",
      "[12:42:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.6s\n",
      "[12:42:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:42:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:42:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:42:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:42:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:42:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:42:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.5, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:42:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:42:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:42:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:42:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.5s\n",
      "[12:42:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:42:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:42:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:42:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:42:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:42:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:42:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:42:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[12:42:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.6s\n",
      "[12:42:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[12:42:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[12:42:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:42:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:42:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=183; total time=   0.6s\n",
      "[12:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[12:42:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.7s\n",
      "[12:42:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.5, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:42:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:42:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:42:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:42:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:42:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:42:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:42:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:42:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:42:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:42:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:42:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:42:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:42:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=183; total time=   0.3s\n",
      "[12:42:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:42:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:42:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:42:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:42:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:42:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:42:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:42:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:42:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:42:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:42:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:42:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:42:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.4s\n",
      "[12:43:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.4s\n",
      "[12:43:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=150; total time=   0.4s\n",
      "[12:43:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:43:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:43:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:43:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:43:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:43:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:43:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:43:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:43:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:43:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:43:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:43:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:43:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:43:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:43:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:43:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:43:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:43:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:43:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:43:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:43:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:43:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:43:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:43:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:43:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:43:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:43:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:43:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:43:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:43:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=183; total time=   0.6s\n",
      "[12:43:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.6s\n",
      "[12:43:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:43:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:43:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[12:43:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:43:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:43:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:43:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:43:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:43:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:43:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:43:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:43:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:43:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:43:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:43:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:43:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:43:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:43:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:43:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:43:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:43:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:43:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:43:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:43:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:43:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:43:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:43:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:43:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:43:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:43:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:43:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:43:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:43:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:43:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:43:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:43:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:43:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:43:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:43:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:43:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:43:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.6, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:43:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:43:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:43:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:43:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:43:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:43:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:43:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:43:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:43:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=133; total time=   0.4s\n",
      "[12:43:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:43:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:43:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:43:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:43:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:43:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:43:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:43:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:43:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:43:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.6, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:43:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:43:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:43:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:43:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:43:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:43:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:43:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:43:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:43:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:43:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:43:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:43:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:43:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:43:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:43:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:43:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:43:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:44:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:44:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:44:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:44:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:44:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:44:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:44:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:44:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:44:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:44:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:44:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=2, n_estimators=200; total time=   0.5s\n",
      "[12:44:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[12:44:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[12:44:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[12:44:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:44:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:44:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:44:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:44:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:44:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:44:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:44:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:44:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:44:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:44:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:44:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:44:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:44:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:44:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:44:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:44:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:44:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[12:44:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:44:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:44:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:44:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.5s\n",
      "[12:44:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:44:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:44:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:44:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:44:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:44:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:44:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:44:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:44:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:44:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=100; total time=   0.4s\n",
      "[12:44:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:44:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:44:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:44:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:44:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:44:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:44:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:44:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.5s\n",
      "[12:44:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:44:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:44:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:44:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:44:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:44:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:44:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:44:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:44:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:44:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:44:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:44:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=66; total time=   0.3s\n",
      "[12:44:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.4s\n",
      "[12:44:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:44:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=83; total time=   0.3s\n",
      "[12:44:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[12:44:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:44:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:44:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:44:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:44:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:44:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:44:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:44:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=133; total time=   0.5s\n",
      "[12:44:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:44:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:44:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:44:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.5s\n",
      "[12:44:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:44:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:44:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:44:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:44:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:44:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:44:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.6s\n",
      "[12:44:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.7000000000000001, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:44:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:44:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:44:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:44:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:44:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:44:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:44:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:44:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:44:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:44:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:44:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:44:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:44:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:44:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:44:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:44:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:44:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:44:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:44:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:44:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:44:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:44:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:44:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:44:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:44:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:44:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:44:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:44:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:44:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:44:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:44:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=116; total time=   0.2s\n",
      "[12:44:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:44:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:44:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:44:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:44:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:45:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:45:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:45:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:45:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:45:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:45:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:45:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:45:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:45:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:45:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:45:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:45:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:45:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:45:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:45:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:45:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:45:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.3s\n",
      "[12:45:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:45:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:45:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:45:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:45:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:45:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:45:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:45:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=116; total time=   0.4s\n",
      "[12:45:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:45:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:45:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:45:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:45:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:45:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:45:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:45:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:45:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:45:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:45:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:45:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:45:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:45:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:45:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:45:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:45:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:45:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:45:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:45:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:45:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:45:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:45:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:45:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=4, n_estimators=83; total time=   0.3s\n",
      "[12:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:45:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:45:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:45:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.4s\n",
      "[12:45:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:45:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:45:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:45:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=133; total time=   0.4s\n",
      "[12:45:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:45:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:45:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=150; total time=   0.6s\n",
      "[12:45:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.6s\n",
      "[12:45:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.5s\n",
      "[12:45:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:45:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:45:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:45:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:45:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:45:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:45:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:45:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:45:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:45:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:45:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:45:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:45:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:45:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:45:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:45:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.8, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:45:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:45:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:45:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:45:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:45:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:45:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:45:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:45:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:45:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:45:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:45:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:45:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=150; total time=   0.4s\n",
      "[12:45:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:45:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:45:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:45:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=183; total time=   0.5s\n",
      "[12:45:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:45:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:45:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.8, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:45:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:45:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:45:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:45:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:45:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:45:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:45:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:45:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:45:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:45:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:45:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:45:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:45:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:45:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.2s\n",
      "[12:45:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:45:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=50; total time=   0.1s\n",
      "[12:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:45:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:45:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=66; total time=   0.1s\n",
      "[12:45:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:45:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:45:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:45:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:45:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:45:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:45:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:45:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:45:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:45:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:45:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:45:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:45:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:45:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=166; total time=   0.4s\n",
      "[12:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=183; total time=   0.4s\n",
      "[12:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:46:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[12:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[12:46:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[12:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:46:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:46:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:46:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=3, n_estimators=200; total time=   0.5s\n",
      "[12:46:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:46:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:46:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=50; total time=   0.2s\n",
      "[12:46:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:46:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:46:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:46:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[12:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[12:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[12:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:46:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:46:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:46:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:46:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:46:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:46:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[12:46:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[12:46:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[12:46:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:46:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:46:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:46:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:46:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:46:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:46:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.4s\n",
      "[12:46:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.4s\n",
      "[12:46:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=4, n_estimators=200; total time=   0.4s\n",
      "[12:46:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[12:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:46:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:46:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:46:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:46:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=0.9, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:46:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:46:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:46:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:46:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.4s\n",
      "[12:46:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:46:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:46:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:46:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:46:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:46:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:46:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:46:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:46:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.4s\n",
      "[12:46:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[12:46:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[12:46:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:46:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:46:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:46:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:46:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:46:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.9, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:46:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:46:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:46:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=50; total time=   0.0s\n",
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=66; total time=   0.1s\n",
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:46:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=1, n_estimators=83; total time=   0.1s\n",
      "[12:46:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:46:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:46:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=100; total time=   0.1s\n",
      "[12:46:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:46:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.1s\n",
      "[12:46:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=116; total time=   0.2s\n",
      "[12:46:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:46:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:46:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=133; total time=   0.2s\n",
      "[12:46:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:46:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:46:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=150; total time=   0.2s\n",
      "[12:46:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:46:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:46:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=166; total time=   0.2s\n",
      "[12:46:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:46:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:46:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=183; total time=   0.2s\n",
      "[12:46:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:46:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=1, n_estimators=200; total time=   0.3s\n",
      "[12:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.2s\n",
      "[12:46:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.2s\n",
      "[12:46:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=66; total time=   0.2s\n",
      "[12:46:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:46:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:46:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=2, n_estimators=83; total time=   0.2s\n",
      "[12:46:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:46:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:46:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=100; total time=   0.2s\n",
      "[12:46:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:46:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:46:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=116; total time=   0.3s\n",
      "[12:46:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:46:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:46:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=133; total time=   0.3s\n",
      "[12:46:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:46:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:46:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=150; total time=   0.3s\n",
      "[12:46:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:46:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:46:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=166; total time=   0.3s\n",
      "[12:46:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.3s\n",
      "[12:46:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.3s\n",
      "[12:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=183; total time=   0.3s\n",
      "[12:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=2, n_estimators=200; total time=   0.4s\n",
      "[12:46:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:46:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[12:46:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=66; total time=   0.2s\n",
      "[12:46:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=3, n_estimators=83; total time=   0.2s\n",
      "[12:46:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[12:46:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[12:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=116; total time=   0.3s\n",
      "[12:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.3s\n",
      "[12:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=133; total time=   0.4s\n",
      "[12:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[12:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=166; total time=   0.4s\n",
      "[12:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=183; total time=   0.4s\n",
      "[12:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=3, n_estimators=200; total time=   0.4s\n",
      "[12:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[12:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[12:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=50; total time=   0.1s\n",
      "[12:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=66; total time=   0.2s\n",
      "[12:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=4, n_estimators=83; total time=   0.2s\n",
      "[12:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.3s\n",
      "[12:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[12:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=100; total time=   0.2s\n",
      "[12:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=116; total time=   0.3s\n",
      "[12:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=133; total time=   0.3s\n",
      "[12:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.4s\n",
      "[12:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=150; total time=   0.3s\n",
      "[12:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.4s\n",
      "[12:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.3s\n",
      "[12:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=166; total time=   0.3s\n",
      "[12:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.4s\n",
      "[12:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=183; total time=   0.5s\n",
      "[12:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.5s\n",
      "[12:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.4s\n",
      "[12:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=4, n_estimators=200; total time=   0.4s\n",
      "[12:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[12:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[12:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[12:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=66; total time=   0.2s\n",
      "[12:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....learning_rate=1.0, max_depth=5, n_estimators=83; total time=   0.2s\n",
      "[12:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[12:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[12:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[12:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=116; total time=   0.3s\n",
      "[12:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=133; total time=   0.3s\n",
      "[12:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=150; total time=   0.3s\n",
      "[12:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[12:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[12:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=166; total time=   0.3s\n",
      "[12:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=183; total time=   0.4s\n",
      "[12:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.5s\n",
      "[12:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=1.0, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[12:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] END .....................max_depth=1.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=1.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=116; total time=   0.0s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=133; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=166; total time=   0.3s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=166; total time=   0.3s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=166; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=1.0, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=2.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=133; total time=   0.2s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=133; total time=   0.2s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=166; total time=   0.3s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=166; total time=   0.3s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=166; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=183; total time=   0.1s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=2.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=3.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=100; total time=   0.0s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=116; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=133; total time=   0.3s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=133; total time=   0.3s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=133; total time=   0.3s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=166; total time=   0.1s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=166; total time=   0.1s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=166; total time=   0.1s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=3.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=50; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=4.0, n_estimators=83; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=100; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=4.0, n_estimators=100; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=116; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=116; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=116; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=133; total time=   0.3s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=150; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=150; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=150; total time=   0.1s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=166; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=166; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=166; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=4.0, n_estimators=200; total time=   0.3s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=66; total time=   0.1s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=66; total time=   0.1s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=66; total time=   0.0s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END .....................max_depth=5.0, n_estimators=83; total time=   0.0s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=116; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=133; total time=   0.1s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=150; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=166; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=166; total time=   0.4s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=166; total time=   0.4s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=183; total time=   0.3s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=183; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....................max_depth=5.0, n_estimators=200; total time=   0.2s\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.001; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.001; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.001; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.003; total time=   1.0s\n",
      "[CV] END ...learning_rate=constant, learning_rate_init=0.004; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.004; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.004; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.005; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=constant, learning_rate_init=0.005; total time=   1.2s\n",
      "[CV] END ...learning_rate=constant, learning_rate_init=0.005; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.001; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.001; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.001; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.002; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.002; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.002; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.003; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.004; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.004; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.004; total time=   1.0s\n",
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.005; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.005; total time=   1.0s\n",
      "[CV] END .learning_rate=invscaling, learning_rate_init=0.005; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.001; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.001; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.001; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.002; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.003; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.003; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.004; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.004; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.004; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.005; total time=   1.0s\n",
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.005; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=adaptive, learning_rate_init=0.005; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrgo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "             param_grid={'learning_rate': ['constant', 'invscaling',\n",
       "                                           'adaptive'],\n",
       "                         'learning_rate_init': array([0.001, 0.002, 0.003, 0.004, 0.005])},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "casual-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict(X_test)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "y_pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "caring-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        80\n",
      "           1       0.89      0.88      0.88        72\n",
      "           2       0.86      0.88      0.87        68\n",
      "           3       0.93      0.94      0.93        80\n",
      "\n",
      "    accuracy                           0.91       300\n",
      "   macro avg       0.90      0.90      0.90       300\n",
      "weighted avg       0.91      0.91      0.91       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "respective-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92        80\n",
      "           1       0.86      0.85      0.85        72\n",
      "           2       0.85      0.90      0.87        68\n",
      "           3       0.95      0.94      0.94        80\n",
      "\n",
      "    accuracy                           0.90       300\n",
      "   macro avg       0.90      0.90      0.90       300\n",
      "weighted avg       0.90      0.90      0.90       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "nearby-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92        80\n",
      "           1       0.82      0.74      0.77        72\n",
      "           2       0.70      0.71      0.70        68\n",
      "           3       0.85      0.85      0.85        80\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.81      0.81      0.81       300\n",
      "weighted avg       0.82      0.82      0.82       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "wooden-owner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94        80\n",
      "           1       0.89      0.88      0.88        72\n",
      "           2       0.89      0.91      0.90        68\n",
      "           3       0.96      0.96      0.96        80\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "sublime-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049632352941177\n",
      "0.8985702614379085\n",
      "0.8104983660130719\n",
      "0.9216911764705882\n"
     ]
    }
   ],
   "source": [
    "print(balanced_accuracy_score(y_test, pred))\n",
    "print(balanced_accuracy_score(y_test, y_pred1))\n",
    "print(balanced_accuracy_score(y_test, y_pred2))\n",
    "print(balanced_accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-stage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
